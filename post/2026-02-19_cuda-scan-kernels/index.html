<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deep dive into CUDA Scan Kernels: Hierarchical and Single-Pass Variants | Shreyansh Singh </title> <meta name="author" content="Shreyansh Singh"> <meta name="description" content="A guided tour of hierarchical and single-pass CUDA scan kernels with coarsening and warp-level optimizations."> <meta property="og:site_name" content="Shreyansh Singh"> <meta property="og:type" content="article"> <meta property="og:title" content="Shreyansh Singh | Deep dive into CUDA Scan Kernels: Hierarchical and Single-Pass Variants"> <meta property="og:url" content="https://shreyansh26.github.io/post/2026-02-19_cuda-scan-kernels/"> <meta property="og:description" content="A guided tour of hierarchical and single-pass CUDA scan kernels with coarsening and warp-level optimizations."> <meta property="og:image" content="https://shreyansh26.github.io/assets/img/posts_images/scan_cuda/hierarchical_scan.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:title" content="Deep dive into CUDA Scan Kernels: Hierarchical and Single-Pass Variants"> <meta name="twitter:description" content="A guided tour of hierarchical and single-pass CUDA scan kernels with coarsening and warp-level optimizations."> <meta name="twitter:image" content="https://shreyansh26.github.io/assets/img/posts_images/scan_cuda/hierarchical_scan.png"> <meta name="twitter:site" content="@shreyansh_26"> <meta name="twitter:creator" content="@shreyansh_26"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link defer href="/assets/css/toc-custom.css?018b6fb1db2a03192ea04ff844223cb2" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/profile_pictures/favicon.ico?7def459033b808c2a47593bed4cfb002"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shreyansh26.github.io/post/2026-02-19_cuda-scan-kernels/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shreyansh</span> Singh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/post/index.html">Posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/bookshelf/">Bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume/Resume_Shreyansh.pdf">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row toc-layout"> <div class="col-sm-3 toc-sidebar"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9 toc-content"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deep dive into CUDA Scan Kernels: Hierarchical and Single-Pass Variants</h1> <p class="post-meta"> Published on February 19, 2026 by Shreyansh Singh </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/cuda"> <i class="fa-solid fa-hashtag fa-sm"></i> cuda</a>   <a href="/blog/tag/gpu"> <i class="fa-solid fa-hashtag fa-sm"></i> gpu</a>   <a href="/blog/tag/scan"> <i class="fa-solid fa-hashtag fa-sm"></i> scan</a>   <a href="/blog/tag/prefix-sum"> <i class="fa-solid fa-hashtag fa-sm"></i> prefix-sum</a>   <a href="/blog/tag/mlsys"> <i class="fa-solid fa-hashtag fa-sm"></i> mlsys</a>   ·   <a href="/blog/category/cuda"> <i class="fa-solid fa-tag fa-sm"></i> CUDA</a>   <a href="/blog/category/mlsys"> <i class="fa-solid fa-tag fa-sm"></i> MLSys</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><em>The source code for this post is available on <a href="https://github.com/shreyansh26/scan.cu" rel="external nofollow noopener" target="_blank">GitHub</a>.</em></p> <hr> <h2 id="introduction">Introduction</h2> <p>A scan (prefix sum) is a deceptively small primitive: given an input array X, produce an output array Y where Y[i] = X[0] + X[1] + … + X[i]. On the GPU this is hard to do efficiently because each output depends on all previous elements, which sounds serial. The kernels in this repository explore multiple ways to restructure this computation so that thousands of threads can participate without breaking correctness.</p> <p>There are two broad families here:</p> <ol> <li> <strong>Hierarchical (multi-pass) scans</strong>: scan within blocks, scan the block totals, then redistribute those totals back into the output. This is the most standard GPU scan strategy and maps cleanly to CUDA’s execution model.</li> <li> <strong>Single-pass scans</strong>: attempt to compute the full array scan in a single kernel launch using inter-block coordination (domino propagation or decoupled lookbacks). These are more complex but avoid extra kernel launches.</li> </ol> <p>A CUB baseline in <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/cub_scan.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/cub_scan.cu</code></a> uses <code class="language-plaintext highlighter-rouge">cub::DeviceScan::InclusiveSum</code> as a reference for performance and correctness.</p> <h3 id="quick-cuda-primer-for-context">Quick CUDA primer (for context)</h3> <p>If you are new to CUDA, three concepts show up repeatedly in the kernels below:</p> <ul> <li> <strong>Warps</strong>: threads are executed in groups of 32. Many performance optimizations (like warp shuffles) are designed around this unit.</li> <li> <strong>Shared memory</strong>: fast, on-chip memory shared by threads in a block. Most scan algorithms use shared memory for their per-block scan stages.</li> <li> <strong>Synchronization</strong>: <code class="language-plaintext highlighter-rouge">__syncthreads()</code> synchronizes threads within a block. There is no built-in global synchronization across blocks inside a kernel, which is why single-pass scans must use explicit memory protocols to coordinate. <code class="language-plaintext highlighter-rouge">__threadfence()</code> orders a thread’s global memory writes so they are visible to other threads/blocks on the device before it continues.</li> </ul> <p>You’ll also see the term <strong>coalesced memory access</strong>. A global memory access is coalesced when consecutive threads in a warp access consecutive addresses, allowing the GPU to serve the warp with fewer memory transactions. This is a major performance factor, and it strongly influences how the kernels index into memory.</p> <hr> <h2 id="hierarchical-scan-algorithms">Hierarchical Scan Algorithms</h2> <h3 id="idea-of-hierarchical-scan">Idea of hierarchical scan</h3> <p>Hierarchical scan decomposes the full scan into three stages that are easy to parallelize:</p> <ol> <li> <strong>Per-block scan</strong>: each block scans a contiguous chunk of the input and writes the prefix results for that chunk into the output array. Each block also emits one number: the <strong>block total</strong> (sum of its entire chunk).</li> <li> <strong>Scan the block totals</strong>: scan the array of block totals to produce a prefix over blocks. Block 0 adds 0; block \(b&gt;0\) adds the scanned total of block \(b-1\) (i.e., the sum of everything before block \(b\)). If this array is long, it is scanned hierarchically using multiple levels.</li> <li> <strong>Redistribution (add carry‑in)</strong>: each block adds its carry‑in to every element of its local output, turning a block-local prefix into a correct global prefix.</li> </ol> <p>In the source, the “block totals” buffer is called <code class="language-plaintext highlighter-rouge">partialSums</code>: it is an auxiliary array with <strong>one entry per thread block</strong>.</p> <p>Concretely, after stage 1 each block has computed the right prefix order relative to the start of its own chunk, but every block except block 0 is missing a constant offset (the sum of all earlier chunks). Scanning the block totals computes exactly those offsets.</p> <p>Example with <code class="language-plaintext highlighter-rouge">BLOCK_SIZE = 4</code>:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input:        [1 2 3 4 | 5 6 7 8]
Local scans:  [1 3 6 10 | 5 11 18 26]
Block totals: [10, 26]
Scan totals:  [10, 36]
Add carry-in: [1 3 6 10 | (5+10) (11+10) (18+10) (26+10)]
            = [1 3 6 10 | 15 21 28 36]
</code></pre></div></div> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/scan_cuda/hierarchical_scan.png" alt="Hierarchical scan overview: per-block scan, scan of block totals (carry), and redistribution."> <figcaption>Hierarchical scan overview: per-block scan, scan of block totals (carry), and redistribution.</figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>This structure is consistent across:</p> <ul> <li><code class="language-plaintext highlighter-rouge">src/hierarchical_kogge_stone*.cu</code></li> <li><code class="language-plaintext highlighter-rouge">src/hierarchical_brent_kung*.cu</code></li> <li><code class="language-plaintext highlighter-rouge">src/hierarchical_warp_tiled*_optimized.cu</code></li> </ul> <p>A key idea for readers: the per-block scan only handles a local segment. The global correctness comes from the second and third stages, which propagate block totals across the array.</p> <h3 id="when-the-block-totals-scan-needs-multiple-levels">When the block-totals scan needs multiple levels</h3> <p>Stage 2 scans <strong>one value per block</strong>. If each block handles \(B =\) <code class="language-plaintext highlighter-rouge">BLOCK_SIZE</code> input elements, then the number of blocks is \(M = \lceil N / B \rceil\), so the block-totals array has length \(M\).</p> <p>A single CUDA block in these kernels scans at most \(B\) values (one value per thread in shared memory), so stage 2 is:</p> <ul> <li> <strong>one-block</strong> when \(M \le B\) (equivalently \(N \le B^2\); for \(B = 1024\), about one million elements),</li> <li> <strong>a small recursive hierarchy</strong> when \(M &gt; B\).</li> </ul> <p>Conceptually, you build a short “pyramid” of group totals:</p> <ul> <li> <strong>Level 0</strong>: per-block totals (length \(M_0 = M\))</li> <li> <strong>Level 1</strong>: totals of contiguous groups of \(B\) entries from level 0 (length \(M_1 = \lceil M_0 / B \rceil\))</li> <li>…</li> <li>stop at the first level \(L\) with \(M_L \le B\)</li> </ul> <p>Then you run the same up/down structure across levels:</p> <ol> <li> <strong>Up-sweep</strong>: scan each level in block-sized segments and write each segment’s total into the next level.</li> <li> <strong>Top scan</strong>: scan the final level in one block.</li> <li> <strong>Down-sweep</strong>: propagate prefixes back down by adding the scanned prefix of earlier segments (the carry‑in) into every element of the lower level.</li> </ol> <p>In <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/hierarchical_kogge_stone.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/hierarchical_kogge_stone.cu</code></a>, this logic is packaged as <code class="language-plaintext highlighter-rouge">ScanLevels</code>: level 0 is the block-totals buffer (<code class="language-plaintext highlighter-rouge">partialSums</code> in the code), and higher levels are temporary allocations that shrink by ~1024× each step:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="p">(</span><span class="n">curr_len</span> <span class="o">&gt;</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">next_len</span> <span class="o">=</span> <span class="n">cdiv</span><span class="p">(</span><span class="n">curr_len</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">);</span>
    <span class="n">T</span><span class="o">*</span> <span class="n">sums_d</span> <span class="o">=</span> <span class="nb">nullptr</span><span class="p">;</span>
    <span class="n">CHECK_CUDA_ERROR</span><span class="p">(</span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sums_d</span><span class="p">,</span> <span class="n">next_len</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">)));</span>
    <span class="n">levels</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">sums_d</span><span class="p">);</span>
    <span class="n">levels</span><span class="p">.</span><span class="n">lengths</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">next_len</span><span class="p">);</span>
    <span class="n">curr_len</span> <span class="o">=</span> <span class="n">next_len</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>The scan then follows the “up-sweep / top / down-sweep” pattern literally:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="kt">size_t</span> <span class="n">level</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">level</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">levels</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="o">++</span><span class="n">level</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">len</span> <span class="o">=</span> <span class="n">levels</span><span class="p">.</span><span class="n">lengths</span><span class="p">[</span><span class="n">level</span><span class="p">];</span>
    <span class="n">dim3</span> <span class="n">gridSize</span><span class="p">(</span><span class="n">cdiv</span><span class="p">(</span><span class="n">len</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">));</span>
    <span class="n">kogge_stone_segmented_scan_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridSize</span><span class="p">,</span> <span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
        <span class="n">levels</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">level</span><span class="p">],</span> <span class="n">levels</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">level</span><span class="p">],</span> <span class="n">levels</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">level</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">len</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">kogge_stone_scan_kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">dim3</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
    <span class="n">levels</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">back</span><span class="p">(),</span> <span class="n">levels</span><span class="p">.</span><span class="n">lengths</span><span class="p">.</span><span class="n">back</span><span class="p">());</span>

<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">level</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">levels</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">())</span> <span class="o">-</span> <span class="mi">2</span><span class="p">;</span> <span class="n">level</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="o">--</span><span class="n">level</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">len</span> <span class="o">=</span> <span class="n">levels</span><span class="p">.</span><span class="n">lengths</span><span class="p">[</span><span class="n">level</span><span class="p">];</span>
    <span class="n">dim3</span> <span class="n">gridSize</span><span class="p">(</span><span class="n">cdiv</span><span class="p">(</span><span class="n">len</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">));</span>
    <span class="n">redistribute_sum</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridSize</span><span class="p">,</span> <span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span>
        <span class="n">levels</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">level</span><span class="p">],</span> <span class="n">levels</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">level</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">len</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p>Without this multi-level pass, stage 2 would only produce correct prefixes <strong>within groups of \(B\) blocks</strong>, and the final redistribution would be wrong for long inputs.</p> <h3 id="inclusive-scan-padding-and-boundaries">Inclusive scan, padding, and boundaries</h3> <p>These kernels implement <strong>inclusive</strong> scan (each output includes its own input). That means the first output is just X[0]. For partial blocks at the end of the array, out-of-range elements are padded with the additive identity (0) so the tree logic stays correct. You’ll see code like:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">XY_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span> <span class="o">?</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">:</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">&gt;</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</code></pre></div></div> <p>This padding is a simple but important trick: it keeps the scan math valid without branching the tree structure.</p> <hr> <h3 id="kogge-stone-scan-simple">Kogge-Stone scan (simple)</h3> <p><strong>Kernel</strong>: <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/hierarchical_kogge_stone.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/hierarchical_kogge_stone.cu</code></a></p> <p>Kogge-Stone is the classic parallel scan. It uses a shared-memory array of size B (one element per thread), and performs log2(B) steps. In each step, every thread reads from a neighbor at distance <code class="language-plaintext highlighter-rouge">stride</code> and updates its own value. This produces an inclusive scan.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/scan_cuda/kogge_stone_scan.png" alt="Kogge-Stone scan (simple) within a block."> <figcaption>Kogge-Stone scan (simple) within a block.</figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Core pattern (in-place):</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">stride</span> <span class="o">&lt;</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> <span class="n">stride</span> <span class="o">*=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">__syncthreads</span><span class="p">();</span>
    <span class="n">T</span> <span class="n">temp</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">temp</span> <span class="o">=</span> <span class="n">XY_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">XY_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">stride</span><span class="p">];</span>
    <span class="p">}</span>
    <span class="n">__syncthreads</span><span class="p">();</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">stride</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">XY_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>Why two barriers per stride? Because updates are in-place: if thread i writes early, thread i+stride could see updated data incorrectly in the same stride. The temp + double barrier pattern avoids read-after-write hazards.</p> <p>This file is the best place to start if you want to understand the baseline hierarchical scan flow end-to-end.</p> <p><strong>Key characteristics:</strong></p> <ul> <li> <strong>Work</strong>: \(O(N \log N)\) additions</li> <li> <strong>Depth</strong>: \(\log_2(N)\) parallel steps</li> <li> <strong>Synchronization</strong>: Two <code class="language-plaintext highlighter-rouge">__syncthreads()</code> per iteration (read-modify-write pattern)</li> <li> <strong>Shared memory</strong>: \(B\) elements (where B is block size)</li> </ul> <hr> <h3 id="kogge-stone-scan-coarsened">Kogge-Stone scan (coarsened)</h3> <p><strong>Kernel</strong>: <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/hierarchical_kogge_stone_coarsening.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/hierarchical_kogge_stone_coarsening.cu</code></a></p> <p>Coarsening is a standard optimization: instead of one element per thread, each thread processes multiple elements. This reduces the number of blocks, which shrinks the block-totals buffer (<code class="language-plaintext highlighter-rouge">partialSums</code>) and thus reduces the amount of hierarchical work. It also increases the work per thread, which can improve instruction-level parallelism.</p> <h4 id="why-coalescing-matters-here">Why coalescing matters here</h4> <p>If each thread loaded a contiguous segment for itself, global memory access would be strided across threads and would not coalesce well. To preserve coalescing, the kernel loads/stores in the pattern:</p> <ul> <li> <strong>Coalesced layout</strong>: <code class="language-plaintext highlighter-rouge">data[c * B + t]</code> (consecutive threads read consecutive addresses)</li> </ul> <p>But for the scan itself, each thread wants a contiguous segment. So the kernel <strong>reinterprets</strong> the shared memory layout as:</p> <ul> <li> <strong>Thread-major layout</strong>: <code class="language-plaintext highlighter-rouge">data[t * C + c]</code> </li> </ul> <p>This is effectively a shared-memory transpose: coalesced global access on the way in and out, but contiguous per-thread access during the scan. It’s one of the most common CUDA tricks for marrying coalescing with local contiguity.</p> <p>If you want a concrete picture, imagine B = 8 and C = 2. A coalesced load makes threads read: thread 0 → X[0], thread 1 → X[1], … thread 7 → X[7], then again X[8..15] for c = 1. That is perfectly coalesced. But thread 0’s logical segment is X[0], X[1] (contiguous), which now sits in shared memory at positions data[0] and data[8]. The transpose reinterpretation is what makes that segment look contiguous again during the scan without sacrificing coalescing on the global load/store.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/scan_cuda/kogge_stone_coarsened_scan.png" alt="Kogge-Stone scan with coarsening and shared-memory transpose."> <figcaption>Kogge-Stone scan with coarsening and shared-memory transpose.</figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Scan flow:</p> <ol> <li>Each thread serially scans its C elements.</li> <li>Run a Kogge-Stone scan over the <strong>thread totals</strong> (last lane per thread).</li> <li>Redistribute each thread’s prefix to its remaining lanes.</li> </ol> <p>The redistribution step is mandatory because of the shared-memory layout:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">T</span> <span class="n">add</span> <span class="o">=</span> <span class="n">XY_s</span><span class="p">[(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">COARSENING_FACTOR</span> <span class="o">+</span> <span class="p">(</span><span class="n">COARSENING_FACTOR</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">COARSENING_FACTOR</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="o">++</span><span class="n">c</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">XY_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">COARSENING_FACTOR</span> <span class="o">+</span> <span class="n">c</span><span class="p">]</span> <span class="o">+=</span> <span class="n">add</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>In contrast, some single-pass variants that keep the coarsened segment in registers can apply the redistribution implicitly at write-out time (see <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/single_pass_scan_naive.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/single_pass_scan_naive.cu</code></a>). The explicit redistribution loop here is the price paid for the coalesced/shared-transpose layout.</p> <hr> <h3 id="kogge-stone-scan-double-buffering">Kogge-Stone scan (double buffering)</h3> <p><strong>Kernel</strong>: <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/hierarchical_kogge_stone_double_buffering.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/hierarchical_kogge_stone_double_buffering.cu</code></a></p> <p>Double buffering uses two shared-memory arrays. Each stride writes into the output buffer, then swaps input and output. That reduces synchronization to one barrier per stride:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out_XY_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&gt;=</span> <span class="n">stride</span><span class="p">)</span>
    <span class="o">?</span> <span class="n">in_XY_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">in_XY_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">-</span> <span class="n">stride</span><span class="p">]</span>
    <span class="o">:</span> <span class="n">in_XY_s</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="n">__syncthreads</span><span class="p">();</span>
<span class="n">T</span><span class="o">*</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">in_XY_s</span><span class="p">;</span>
<span class="n">in_XY_s</span> <span class="o">=</span> <span class="n">out_XY_s</span><span class="p">;</span>
<span class="n">out_XY_s</span> <span class="o">=</span> <span class="n">temp</span><span class="p">;</span>
</code></pre></div></div> <p>This often helps Kogge-Stone because the in-place version needs two barriers per stride, and Kogge-Stone has many strides (log2(B)). The extra shared-memory traffic is usually offset by fewer synchronizations.</p> <hr> <h3 id="brent-kung-scan-simple">Brent-Kung scan (simple)</h3> <p><strong>Kernel</strong>: <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/hierarchical_brent_kung.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/hierarchical_brent_kung.cu</code></a></p> <p>Brent-Kung trades fewer total operations for a more complex indexing pattern. It scans 2B elements per block by assigning two elements per thread and building a balanced tree:</p> <ul> <li> <strong>Upsweep</strong>: reduce to a block total.</li> <li> <strong>Downsweep</strong>: distribute partial sums.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/scan_cuda/brent_kung_scan.png" alt="Brent-Kung scan (simple) tree structure."> <figcaption>Brent-Kung scan (simple) tree structure.</figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Unlike Kogge-Stone, Brent-Kung does not update all elements each stride; it updates only a subset. This is why its in-place version is efficient: it needs only one barrier per stride and does not require the temp + double barrier pattern.</p> <p>Brent-Kung is often discussed as an algorithm with fewer operations but more complex indexing. In practice, the actual performance depends on shared-memory traffic and synchronization, which this codebase makes easy to study side-by-side.</p> <p><strong>Key characteristics:</strong></p> <ul> <li> <strong>Work</strong>: \(O(N)\) additions (work-efficient)</li> <li> <strong>Depth</strong>: \(2\log_2(N)\) parallel steps</li> <li> <strong>Synchronization</strong>: One <code class="language-plaintext highlighter-rouge">__syncthreads()</code> per iteration</li> <li> <strong>Shared memory</strong>: \(2B\) elements</li> </ul> <hr> <h3 id="brent-kung-scan-coarsened">Brent-Kung scan (coarsened)</h3> <p><strong>Kernel</strong>: <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/hierarchical_brent_kung_coarsening.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/hierarchical_brent_kung_coarsening.cu</code></a></p> <p>The coarsened Brent-Kung kernel mirrors the Kogge-Stone coarsening idea, but with one extra detail: <strong>it uses a 2B shared array for the thread totals.</strong></p> <p>Why? The Brent-Kung scan kernel in this repository is written for an array of length <code class="language-plaintext highlighter-rouge">2 * blockDim</code>, with two elements per thread. To reuse that exact kernel, the coarsened version pads the totals array:</p> <ul> <li>totals[0..B-1] = per-thread totals</li> <li>totals[B..2B-1] = 0</li> </ul> <p>This preserves the expected tree shape and makes the existing Brent-Kung scan code correct without rewriting it.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/scan_cuda/brent_kung_coarsened_scan.png" alt="Brent-Kung scan with coarsening and padded totals array."> <figcaption>Brent-Kung scan with coarsening and padded totals array.</figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>After the totals scan, each thread adds the scanned total of all previous threads to its local \(C\) elements to produce the correct block-wide prefix order. This is the same “redistribution” idea as in coarsened Kogge-Stone, but the padded 2B array is a specific quirk of the Brent-Kung implementation here.</p> <hr> <h3 id="brent-kung-scan-double-buffering">Brent-Kung scan (double buffering)</h3> <p><strong>Kernel</strong>: <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/hierarchical_brent_kung_double_buffering.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/hierarchical_brent_kung_double_buffering.cu</code></a></p> <p>Brent-Kung double buffering is included for completeness, but it is usually not beneficial. The in-place Brent-Kung already avoids read-after-write hazards and uses one barrier per stride. The double-buffered version:</p> <ul> <li>Copies the entire 2B array each stride,</li> <li>Uses extra barriers,</li> <li>Doubles shared memory usage.</li> </ul> <p>So the overhead outweighs the benefit for Brent-Kung in this codebase.</p> <hr> <h3 id="optimized-hierarchical-scan-warp-primitives--register-tiling">Optimized hierarchical scan (warp primitives + register tiling)</h3> <p><strong>Kernels</strong>:</p> <ul> <li><a href="https://github.com/shreyansh26/scan.cu/blob/main/src/hierarchical_warp_tiled_optimized.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/hierarchical_warp_tiled_optimized.cu</code></a></li> <li><a href="https://github.com/shreyansh26/scan.cu/blob/main/src/hierarchical_warp_tiled_coarsening_optimized.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/hierarchical_warp_tiled_coarsening_optimized.cu</code></a></li> </ul> <p>These optimized kernels combine multiple techniques to reduce synchronization and shared-memory traffic.</p> <h4 id="1-warp-level-inclusive-scan">1) Warp-level inclusive scan</h4> <p>Instead of a block-wide shared-memory tree, each warp scans its own totals with warp shuffle primitives:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__device__</span> <span class="n">__forceinline__</span> <span class="n">T</span> <span class="nf">warp_inclusive_scan</span><span class="p">(</span><span class="n">T</span> <span class="n">val</span><span class="p">,</span> <span class="kt">int</span> <span class="n">lane</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">mask</span> <span class="o">=</span> <span class="mh">0xffffffff</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&lt;</span> <span class="n">warpSize</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">T</span> <span class="n">up</span> <span class="o">=</span> <span class="n">__shfl_up_sync</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">offset</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">&gt;=</span> <span class="n">offset</span><span class="p">)</span> <span class="p">{</span> <span class="n">val</span> <span class="o">+=</span> <span class="n">up</span><span class="p">;</span> <span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">val</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>This is fast because warp shuffles are register-only and do not require <code class="language-plaintext highlighter-rouge">__syncthreads()</code>. Conceptually, after the loop, lane i contains the sum of lanes 0..i, which is exactly what a prefix scan needs.</p> <h4 id="2-register-tiling">2) Register tiling</h4> <p>Each thread scans a small contiguous tile in registers to reduce shared-memory traffic:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span> <span class="n">regs</span><span class="p">[</span><span class="n">TILE_FACTOR</span><span class="p">];</span>
<span class="cp">#pragma unroll
</span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">TILE_FACTOR</span><span class="p">;</span> <span class="o">++</span><span class="n">c</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">regs</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">TILE_FACTOR</span> <span class="o">+</span> <span class="n">c</span><span class="p">];</span>
<span class="p">}</span>
<span class="cp">#pragma unroll
</span><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">c</span> <span class="o">&lt;</span> <span class="n">TILE_FACTOR</span><span class="p">;</span> <span class="o">++</span><span class="n">c</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">regs</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">+=</span> <span class="n">regs</span><span class="p">[</span><span class="n">c</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div> <p>This gives each thread a local prefix and a thread total (<code class="language-plaintext highlighter-rouge">regs[TILE_FACTOR-1]</code>) without extra synchronization.</p> <h4 id="3-shared-memory-transpose-for-coalescing">3) Shared-memory transpose for coalescing</h4> <p>Data is loaded as <code class="language-plaintext highlighter-rouge">data[c * B + t]</code> for coalesced global reads, then read back as <code class="language-plaintext highlighter-rouge">data[t * TILE_FACTOR + c]</code> for the register scan. Stores use the coalesced layout again. This preserves global memory efficiency while keeping per-thread data contiguous.</p> <h4 id="4-two-level-warp-totals-scan">4) Two-level warp-totals scan</h4> <p>Each warp writes its total into <code class="language-plaintext highlighter-rouge">warp_totals_s[warp]</code>. Warp 0 then scans these warp totals using the same warp primitive. This works because the maximum number of warps per block is 32 (1024 threads / 32), so warp 0 can cover all warp totals using its 32 lanes.</p> <p>You’ll see <code class="language-plaintext highlighter-rouge">warp_totals_s</code> indexed by both <code class="language-plaintext highlighter-rouge">warp</code> and <code class="language-plaintext highlighter-rouge">lane</code> in the code. This is safe because in the write phase the last lane of each warp writes its total to index <code class="language-plaintext highlighter-rouge">warp</code>, and in the scan phase only warp 0 participates, where lane id (0..warp_count-1) maps directly to warp id. Since <code class="language-plaintext highlighter-rouge">warp_count &lt;= 32</code>, warp 0 has exactly enough lanes.</p> <h4 id="5-fewer-hierarchical-levels">5) Fewer hierarchical levels</h4> <p>Because each block processes more elements, the block-totals buffer (<code class="language-plaintext highlighter-rouge">partialSums</code>) is smaller and the multi-level scan has fewer levels. That reduces total kernel launches and memory traffic.</p> <hr> <h2 id="single-pass-scan-algorithms">Single Pass Scan Algorithms</h2> <p>Single-pass scans try to avoid the multi-launch hierarchy. The main challenge is that blocks cannot synchronize with each other directly, so global ordering must be achieved by careful memory protocols.</p> <h3 id="naive-single-pass-scan-domino-propagation">Naive single-pass scan (domino propagation)</h3> <p><strong>Kernels</strong>:</p> <ul> <li> <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/single_pass_scan_naive.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/single_pass_scan_naive.cu</code></a> (register-tile output)</li> <li> <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/single_pass_scan_naive_alternate.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/single_pass_scan_naive_alternate.cu</code></a> (shared-memory tile output)</li> </ul> <p>Both kernels do the same high-level steps:</p> <ol> <li>Each block scans its local data (with coarsening).</li> <li>Blocks participate in a <strong>domino chain</strong>: block i waits for block i-1 to publish a prefix, then publishes its own prefix for block i+1.</li> </ol> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/scan_cuda/single_pass_scan_naive.png" alt="Single-pass naive scan with domino propagation."> <figcaption>Single-pass naive scan with domino propagation.</figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The domino chain needs two pieces of global state:</p> <ul> <li> <strong>Published prefixes</strong>: a per-block slot where block \(i\) publishes the prefix up to the end of its tile, so block \(i+1\) can read it.</li> <li> <strong>Readiness markers</strong>: a per-block marker so the successor knows the published prefix is valid and globally visible.</li> </ul> <p>In the code these are <code class="language-plaintext highlighter-rouge">scan_value</code> (the published prefix values) and <code class="language-plaintext highlighter-rouge">flags</code> (the readiness markers). The <code class="language-plaintext highlighter-rouge">epoch</code> value is a generation counter: instead of clearing <code class="language-plaintext highlighter-rouge">flags</code> between invocations, the kernel treats <code class="language-plaintext highlighter-rouge">flags[k] == epoch</code> as “ready for this run”.</p> <p>One small indexing convenience shows up in the snippet below: the arrays are effectively shifted by one (<code class="language-plaintext highlighter-rouge">bid + 1</code>) so slot 0 can represent the empty prefix (0). Block <code class="language-plaintext highlighter-rouge">bid</code> publishes into slot <code class="language-plaintext highlighter-rouge">bid+1</code> and waits on slot <code class="language-plaintext highlighter-rouge">bid</code>.</p> <p>The publish sequence requires a <strong>global memory fence</strong>:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">scan_value</span><span class="p">[</span><span class="n">bid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">previous_sum</span> <span class="o">+</span> <span class="n">block_sum</span><span class="p">;</span>
<span class="n">__threadfence</span><span class="p">();</span>
<span class="n">atomicExch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">flags</span><span class="p">[</span><span class="n">bid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">epoch</span><span class="p">);</span>
</code></pre></div></div> <p>Why <code class="language-plaintext highlighter-rouge">__threadfence()</code>? Because <code class="language-plaintext highlighter-rouge">__syncthreads()</code> only synchronizes threads inside a block. We need to ensure that block i’s write to <code class="language-plaintext highlighter-rouge">scan_value[i+1]</code> is globally visible before block i+1 observes <code class="language-plaintext highlighter-rouge">flags[i+1]</code> and proceeds. Without the fence, the successor block could read stale data.</p> <p><strong>Naive ordering hazard</strong>: these kernels use <code class="language-plaintext highlighter-rouge">blockIdx.x</code> as the logical block id. If CUDA schedules blocks out of order (which is allowed), the domino chain can deadlock. Concretely, a later block can become resident and spin waiting for a predecessor that was never scheduled; if all resident blocks are waiting, no block makes progress. This is why the next variant exists.</p> <hr> <h3 id="dynamic-block-indexing-scan">Dynamic block indexing scan</h3> <p><strong>Kernel</strong>: <a href="https://github.com/shreyansh26/scan.cu/blob/main/src/single_pass_scan_dynamic_block_index.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/single_pass_scan_dynamic_block_index.cu</code></a></p> <p>To make the domino chain follow <strong>actual execution order</strong> (instead of launch order), blocks take a ticket from a global counter when they start running. That ticket becomes the block’s logical id in the chain, so a block never waits on a predecessor that hasn’t been scheduled yet.</p> <p>In code, thread 0 does:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">bid_s</span> <span class="o">=</span> <span class="n">atomicAdd</span><span class="p">(</span><span class="n">blockCounter</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p>Because tickets are handed out in arrival order, the predecessor of ticket \(k\) (ticket \(k-1\)) must already be resident (it had to run to take ticket \(k-1\)), so the wait cannot be on a non-resident block. The rest of the logic (published prefixes, readiness flags, and <code class="language-plaintext highlighter-rouge">__threadfence()</code>) remains the same.</p> <p>This approach is still a strict chain: every block still waits for its predecessor, but the ordering is now safe.</p> <hr> <h3 id="decoupled-lookbacks-single-pass">Decoupled lookbacks (single-pass)</h3> <p><strong>Kernels</strong>:</p> <ul> <li><a href="https://github.com/shreyansh26/scan.cu/blob/main/src/single_pass_scan_decoupled_lookbacks.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/single_pass_scan_decoupled_lookbacks.cu</code></a></li> <li><a href="https://github.com/shreyansh26/scan.cu/blob/main/src/single_pass_scan_decoupled_lookbacks_warp_window.cu" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">src/single_pass_scan_decoupled_lookbacks_warp_window.cu</code></a></li> </ul> <p>Decoupled lookback removes the strict block-serialization of the domino chain. Terminology: I’ll call each block’s contiguous chunk of the input a <strong>tile</strong>. The algorithm maintains a global per-tile state array (called <code class="language-plaintext highlighter-rouge">tile_state</code> in the code) where each tile publishes information that later tiles can reuse.</p> <p>Instead of “wait only for your immediate predecessor”, each tile:</p> <ol> <li>Publishes its local tile sum as a <strong>partial</strong> value in the tile-state array.</li> <li>Looks back over preceding tiles until it finds an <strong>inclusive</strong> tile, accumulating partial sums along the way.</li> <li>Publishes its own <strong>inclusive</strong> value (prefix + tile sum).</li> </ol> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/scan_cuda/single_pass_scan_decoupled_lookbacks.png" alt="Single-pass scan with decoupled lookbacks."> <figcaption>Single-pass scan with decoupled lookbacks.</figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h4 id="tile-state-packing">Tile state packing</h4> <p>Each tile state stores <strong>(status, value)</strong>. Status is one of:</p> <ul> <li> <strong>Invalid</strong> (nothing published yet) — <code class="language-plaintext highlighter-rouge">TILE_INVALID</code> in the code</li> <li> <strong>Partial</strong> (tile sum published; no carry from predecessors yet) — <code class="language-plaintext highlighter-rouge">TILE_PARTIAL</code> in the code</li> <li> <strong>Inclusive</strong> (full prefix up to the end of the tile published) — <code class="language-plaintext highlighter-rouge">TILE_INCLUSIVE</code> in the code</li> </ul> <p>The code packs <code class="language-plaintext highlighter-rouge">(status, value)</code> into a single 64-bit word so updates can be atomic:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span> <span class="nf">pack_state</span><span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">status</span><span class="p">,</span> <span class="n">T</span> <span class="n">value</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">return</span> <span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span><span class="o">&gt;</span><span class="p">(</span><span class="n">status</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="mi">32</span><span class="p">)</span>
           <span class="o">|</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">unsigned</span> <span class="kt">long</span> <span class="kt">long</span><span class="o">&gt;</span><span class="p">(</span><span class="n">__float_as_uint</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">(</span><span class="n">value</span><span class="p">)));</span>
<span class="p">}</span>
</code></pre></div></div> <p>This uses <code class="language-plaintext highlighter-rouge">__float_as_uint</code> and <code class="language-plaintext highlighter-rouge">__uint_as_float</code> to preserve the exact 32-bit bit pattern. The implementation assumes 32-bit values (<code class="language-plaintext highlighter-rouge">sizeof(T) == sizeof(unsigned int)</code>), which is enforced by a static_assert. If you want a type-safe version for non-float data, you would use a different packing strategy.</p> <p>The lookback needs an atomic snapshot read of this 64-bit word. The code uses the CUDA idiom <code class="language-plaintext highlighter-rouge">atomicAdd(&amp;tile_state[idx], 0ULL)</code> (atomic add of zero) as an atomic load, which ensures you see the most recent tile update while other blocks are publishing.</p> <h4 id="serial-lookback">Serial lookback</h4> <p>Thread 0 walks backward until it sees an inclusive tile:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">while</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">packed</span> <span class="o">=</span> <span class="n">atomicAdd</span><span class="p">(</span><span class="o">&amp;</span><span class="n">tile_state</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="mi">0ULL</span><span class="p">);</span>
    <span class="n">unpack_state</span><span class="p">(</span><span class="n">packed</span><span class="p">,</span> <span class="n">status</span><span class="p">,</span> <span class="n">value</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">status</span> <span class="o">!=</span> <span class="n">TILE_INVALID</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">running</span> <span class="o">+=</span> <span class="n">value</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">status</span> <span class="o">==</span> <span class="n">TILE_INCLUSIVE</span><span class="p">)</span> <span class="p">{</span> <span class="k">break</span><span class="p">;</span> <span class="p">}</span>
        <span class="n">idx</span> <span class="o">-=</span> <span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>This lets blocks make progress even if predecessors have only published partial sums.</p> <h4 id="warp-window-lookback">Warp-window lookback</h4> <p>The warp-window variant accelerates lookback by reading <strong>32 tiles per iteration</strong> using a warp. Each lane reads one tile, the warp performs a prefix scan over that 32-tile window, and if any lane sees an INCLUSIVE tile the warp can stop immediately with the correct prefix.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span> <span class="n">prefix</span> <span class="o">=</span> <span class="n">value</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&lt;</span> <span class="n">WARP_SIZE</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&lt;&lt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">T</span> <span class="n">shifted</span> <span class="o">=</span> <span class="n">__shfl_up_sync</span><span class="p">(</span><span class="n">full_mask</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">offset</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">&gt;=</span> <span class="n">offset</span><span class="p">)</span> <span class="p">{</span> <span class="n">prefix</span> <span class="o">+=</span> <span class="n">shifted</span><span class="p">;</span> <span class="p">}</span>
<span class="p">}</span>

<span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">inclusive_mask</span> <span class="o">=</span> <span class="n">__ballot_sync</span><span class="p">(</span><span class="n">full_mask</span><span class="p">,</span> <span class="n">status</span> <span class="o">==</span> <span class="n">TILE_INCLUSIVE</span><span class="p">);</span>
<span class="k">if</span> <span class="p">(</span><span class="n">inclusive_mask</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">first</span> <span class="o">=</span> <span class="n">__ffs</span><span class="p">(</span><span class="n">inclusive_mask</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">T</span> <span class="n">inclusive_prefix</span> <span class="o">=</span> <span class="n">__shfl_sync</span><span class="p">(</span><span class="n">full_mask</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">first</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">lane</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span> <span class="n">running</span> <span class="o">+=</span> <span class="n">inclusive_prefix</span><span class="p">;</span> <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>This reduces global memory polling by a factor of ~32 compared to the serial lookback.</p> <p>The logic is subtle but important: the warp-level prefix scan computes cumulative sums over a 32-tile window. If any lane sees an INCLUSIVE tile, the prefix at that lane already includes all partials between the current block and that inclusive tile, so it is the correct prefix for the block. If no inclusive tile exists in the window, the warp adds the sum of the entire window and moves the lookback back by 32 tiles—no double counting because windows do not overlap.</p> <h4 id="how-this-differs-from-dynamic-block-indexing">How this differs from dynamic block indexing</h4> <ul> <li>Dynamic block indexing still enforces a strict chain (block i waits for block i-1).</li> <li>Decoupled lookback lets blocks make partial progress even if predecessors are not finished, which improves parallelism when many blocks are resident.</li> <li> <strong>State footprint</strong>: lookback uses one packed per-tile state array (named <code class="language-plaintext highlighter-rouge">tile_state</code> in the code). The dynamic-index domino uses a published-prefix array + readiness flags + a global ticket counter (named <code class="language-plaintext highlighter-rouge">scan_value</code>, <code class="language-plaintext highlighter-rouge">flags</code>, <code class="language-plaintext highlighter-rouge">blockCounter</code>).</li> </ul> <hr> <h2 id="performance-overview-from-the-provided-benchmarks">Performance Overview (from the provided benchmarks)</h2> <p>The repository includes benchmark results in <a href="https://github.com/shreyansh26/scan.cu/blob/main/bench/timing.txt" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">bench/timing.txt</code></a>, generated by running <a href="https://github.com/shreyansh26/scan.cu/blob/main/bench.sh" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">bench.sh</code></a> over all kernels and a set of input sizes. These numbers are <strong>wall-clock kernel times</strong> reported by each binary. Since hardware and build settings are not embedded in the file, treat these results as a <strong>relative comparison</strong> for the current environment, not as universal performance claims.</p> <p><strong>Notes on reading the numbers</strong>:</p> <ul> <li>All times are in <strong>milliseconds</strong> and represent a single kernel’s timing output for the given N.</li> <li>Small-N results are dominated by launch/synchronization overheads; large-N results better reflect algorithmic scaling and memory behavior.</li> </ul> <h3 id="latency-plot-power-of-two-sizes">Latency plot (power-of-two sizes)</h3> <p>The plot below visualizes the <strong>top 3 kernels (by average power-of-two latency) plus CUB</strong> across power-of-two input sizes. It is generated from <a href="https://github.com/shreyansh26/scan.cu/blob/main/bench/timing.txt" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">bench/timing.txt</code></a> and saved at <a href="https://github.com/shreyansh26/scan.cu/blob/main/bench/latency_pow2_top3.png" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">bench/latency_pow2_top3.png</code></a>.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/scan_cuda/latency_pow2_top3.png" alt="Latency vs N for top 3 kernels + CUB (power-of-two sizes from bench/timing.txt)."> <figcaption>Latency vs N for top 3 kernels + CUB (power-of-two sizes from bench/timing.txt).</figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="smalln-snapshot-poweroftwo-sizes">Small‑N snapshot (power‑of‑two sizes)</h3> <p>For very small inputs, fixed overheads (kernel launch, synchronization, and setup) dominate. The absolute differences are tiny, but it’s still useful to see which kernels stay competitive when N is small.</p> <table> <thead> <tr> <th>Kernel (selected)</th> <th>N = 512 (ms)</th> <th>N = 1,024 (ms)</th> <th>N = 2,048 (ms)</th> <th>N = 4,096 (ms)</th> <th>N = 8,192 (ms)</th> </tr> </thead> <tbody> <tr> <td>CUB DeviceScan</td> <td>0.013136</td> <td>0.012864</td> <td>0.0082496</td> <td>0.0140896</td> <td>0.014944</td> </tr> <tr> <td>Hierarchical warp-tiled optimized</td> <td>0.0075968</td> <td>0.0080544</td> <td>0.00784</td> <td>0.011216</td> <td>0.0105504</td> </tr> <tr> <td>Single-pass decoupled lookback (warp window)</td> <td>0.0097248</td> <td>0.0087072</td> <td>0.009504</td> <td>0.0097696</td> <td>0.011408</td> </tr> <tr> <td>Hierarchical warp-tiled coarsened optimized</td> <td>0.0109376</td> <td>0.0132864</td> <td>0.0123424</td> <td>0.0112896</td> <td>0.0109824</td> </tr> </tbody> </table> <p><strong>Observations (small N)</strong>:</p> <ul> <li>Differences are within a few microseconds, so <strong>launch and synchronization overheads dominate</strong>.</li> <li>The warp-tiled optimized variant is consistently strong, suggesting its low sync count helps even at small N.</li> <li>The coarsened optimized variant carries extra shared-memory traffic and setup, which can be less favorable at tiny sizes.</li> </ul> <h3 id="largen-snapshot-representative-sizes">Large‑N snapshot (representative sizes)</h3> <p>Below is a snapshot of representative larger sizes from <a href="https://github.com/shreyansh26/scan.cu/blob/main/bench/timing.txt" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">bench/timing.txt</code></a>. (Lower is better.)</p> <table> <thead> <tr> <th>Kernel (selected)</th> <th>N = 100,000 (ms)</th> <th>N = 1,000,000 (ms)</th> <th>N = 4,194,303 (ms)</th> </tr> </thead> <tbody> <tr> <td>CUB DeviceScan</td> <td>0.0139072</td> <td>0.0121504</td> <td>0.0190912</td> </tr> <tr> <td>Hierarchical Kogge-Stone</td> <td>0.0101056</td> <td>0.0189024</td> <td>0.0597216</td> </tr> <tr> <td>Hierarchical Kogge-Stone (double buffer)</td> <td>0.0109184</td> <td>0.0179616</td> <td>0.0564128</td> </tr> <tr> <td>Hierarchical Brent-Kung</td> <td>0.0131008</td> <td>0.0216928</td> <td>0.0584768</td> </tr> <tr> <td>Hierarchical warp-tiled optimized</td> <td>0.0110592</td> <td>0.0128512</td> <td>0.02824</td> </tr> <tr> <td>Hierarchical warp-tiled coarsened optimized</td> <td>0.0141440</td> <td>0.0156544</td> <td>0.0278208</td> </tr> <tr> <td>Single-pass decoupled lookback</td> <td>0.0128544</td> <td>0.0213984</td> <td>0.0563424</td> </tr> <tr> <td>Single-pass decoupled lookback (warp window)</td> <td>0.01008</td> <td>0.014992</td> <td>0.0386688</td> </tr> <tr> <td>Single-pass dynamic block index</td> <td>0.029392</td> <td>0.230454</td> <td>0.946202</td> </tr> <tr> <td>Single-pass naive</td> <td>0.0302944</td> <td>0.221734</td> <td>0.910448</td> </tr> </tbody> </table> <p><br></p> <h3 id="takeaways">Takeaways</h3> <ul> <li> <strong>Warp-tiled hierarchical scans are consistently strong at large N.</strong> They combine coalesced access with warp primitives and fewer synchronization points, so they stay competitive as the array grows.</li> <li> <strong>CUB is a strong baseline</strong>, often among the fastest (as expected).</li> <li> <strong>Decoupled lookback with warp-window usually beats the serial lookback</strong> because the warp cooperatively scans 32 tiles at a time, reducing the number of global polls.</li> <li> <strong>Naive and dynamic-block-index single-pass scans degrade at large N.</strong> The domino chain introduces strong serialization, which dominates as the number of blocks grows.</li> <li> <strong>Brent-Kung double buffering does not help</strong> in these measurements, matching the reasoning in the notes: the in-place Brent-Kung already avoids the hazards that double buffering tries to fix.</li> </ul> <p>If you want to reproduce or extend these results, <a href="https://github.com/shreyansh26/scan.cu/blob/main/bench.sh" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">bench.sh</code></a> runs all binaries under <code class="language-plaintext highlighter-rouge">bin/</code> and prints the same table format used in <a href="https://github.com/shreyansh26/scan.cu/blob/main/bench/timing.txt" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">bench/timing.txt</code></a>.</p> <hr> <h2 id="conclusion">Conclusion</h2> <ul> <li> <strong>Hierarchical scans</strong> provide a clear, scalable baseline with predictable synchronization.</li> <li> <strong>Coarsening and double buffering</strong> highlight memory and synchronization tradeoffs.</li> <li> <strong>Warp-tiled optimizations</strong> show how warp primitives and register tiling reduce shared-memory pressure and synchronization overhead.</li> <li> <strong>Single-pass scans</strong> demonstrate how to coordinate blocks without global barriers, from simple dominos to sophisticated decoupled lookbacks.</li> </ul> <p>If you are new to GPU scans, start with the simple hierarchical Kogge-Stone and Brent-Kung versions and study how the block-totals buffer enables global correctness. Then move to the coarsened and warp-tiled kernels to see how memory coalescing and warp primitives change the design. Finally, explore single-pass scans to understand how inter-block coordination can be done without extra kernel launches.</p> <hr> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26" rel="external nofollow noopener" target="_blank">Twitter</a>, <a href="https://github.com/shreyansh26" rel="external nofollow noopener" target="_blank">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/" rel="external nofollow noopener" target="_blank">LinkedIn</a>.</p> </div> </article> <div id="giscus_thread" style="max-width: 1440px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shreyansh26/shreyansh26.github.io","data-repo-id":"R_kgDOMn767Q","data-category":"General","data-category-id":"DIC_kwDOMn767c4CiLUv","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Shreyansh Singh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?317646887210841a1b23a5eedc9fba39"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZEZ7673Y7G"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZEZ7673Y7G");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-posts",title:"Posts",description:"",section:"Navigation",handler:()=>{window.location.href="/post/index.html"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-bookshelf",title:"Bookshelf",description:"A list of books I&#39;ve read and some of which I&#39;ve found interesting :)",section:"Navigation",handler:()=>{window.location.href="/bookshelf/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/assets/pdf/resume/Resume_Shreyansh.pdf"}},{id:"post-deep-dive-into-cuda-scan-kernels-hierarchical-and-single-pass-variants",title:"Deep dive into CUDA Scan Kernels: Hierarchical and Single-Pass Variants",description:"A guided tour of hierarchical and single-pass CUDA scan kernels with coarsening and warp-level optimizations.",section:"Posts",handler:()=>{window.location.href="/post/2026-02-19_cuda-scan-kernels/"}},{id:"post-paper-summary-14-physics-of-language-models-part-3-1-knowledge-storage-and-extraction",title:"Paper Summary #14 - Physics of Language Models: Part 3.1, Knowledge Storage and...",description:"My notes from the Physics of Language Models series of papers.",section:"Posts",handler:()=>{window.location.href="/post/2026-01-17_physics-of-lms-3-1-knowledge-storage-and-extraction/"}},{id:"post-understanding-multi-head-latent-attention-mla",title:"Understanding Multi-Head Latent Attention (MLA)",description:"A mathematical and code deep-dive on one of the key innovations from Deepseek - Multihead Latent Attention (MLA)",section:"Posts",handler:()=>{window.location.href="/post/2025-11-08_multihead-latent-attention/"}},{id:"post-deriving-the-gradient-for-the-backward-pass-of-layer-normalization",title:"Deriving the Gradient for the Backward Pass of Layer Normalization",description:"Understanding the math behind Layer Normalization and deriving the gradients for the backward pass.",section:"Posts",handler:()=>{window.location.href="/post/2025-06-04_layernorm-gradients/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-compute-and-instruction-throughput",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Compute and Instruction Throughput",description:"My notes from the talk on maximizing compute and instruction throughput at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-04-04_gtc25-maximize-compute-instruction-throughput/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-2",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"Second part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-2/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-1",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"First part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/"}},{id:"post-faster-cross-encoder-inference-unleashing-torch-compile-for-speed",title:"Faster Cross-Encoder Inference: Unleashing torch.compile for speed",description:"A quick writeup on accelerating a Jina Cross-Encoder using torch.compile",section:"Posts",handler:()=>{window.location.href="/post/2025-03-02_cross-encoder-inference-torch-compile/"}},{id:"post-paper-summary-13-physics-of-language-models-part-2-1-grade-school-math-and-the-hidden-reasoning-process",title:"Paper Summary #13 - Physics of Language Models: Part 2.1, Grade-School Math and...",description:"My notes from the Physics of Language Models series of papers.",section:"Posts",handler:()=>{window.location.href="/post/2024-09-21_physics-of-lms-2-1-grade-school-math-and-the-hidden-reasoning-process/"}},{id:"post-paper-summary-12-image-recaptioning-in-dall-e-3",title:"Paper Summary #12 - Image Recaptioning in DALL-E 3",description:"The image recaptioning technique used in DALL-E 3 was extended to videos in Sora.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_dalle3_image_recaptioner/"}},{id:"post-paper-summary-11-sora",title:"Paper Summary #11 - Sora",description:"OpenAI announced a ground-breaking text-to-video diffusion model capable of generating high-definition videos up to 60 seconds long.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_sora_openai/"}},{id:"post-paper-summary-10-gemini-1-5-pro",title:"Paper Summary #10 - Gemini 1.5 Pro",description:"Google DeepMind announced a multimodal LLM with support of up to 10M context length.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_gemini_pro_google/"}},{id:"post-solving-substitution-ciphers-using-markov-chain-monte-carlo-mcmc",title:"Solving Substitution Ciphers using Markov Chain Monte Carlo (MCMC)",description:"Deciphering substitution ciphers can be framed as a Markov chain problem and a simple Monte Carlo sampling approach can help solve them very efficiently",section:"Posts",handler:()=>{window.location.href="/post/2023-07-22_solving_substitution_cipher_using_mcmc/"}},{id:"post-paper-summary-9-sophia-a-scalable-stochastic-second-order-optimizer-for-language-model-pre-training",title:"Paper Summary #9 - Sophia: A Scalable Stochastic Second-order Optimizer for Language Model...",description:"Understanding Sophia - A new fast, scalable second-order optimizer which beats Adam on LLM pretraining.",section:"Posts",handler:()=>{window.location.href="/post/2023-05-28_sophia_scalable_second_order_optimizer_llms/"}},{id:"post-paper-summary-8-flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness",title:"Paper Summary #8 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",description:"Understanding FlashAttention which is the most efficient exact attention implementation out there, which optimizes for both memory requirements and wall-clock time.",section:"Posts",handler:()=>{window.location.href="/post/2023-03-26_flash-attention/"}},{id:"post-paper-summary-7-efficient-transformers-a-survey",title:"Paper Summary #7 - Efficient Transformers: A Survey",description:"A survey paper of improvements over the original Transformer architecture in terms of memory-efficiency.",section:"Posts",handler:()=>{window.location.href="/post/2022-10-10_efficient_transformers_survey/"}},{id:"post-deploying-machine-learning-models-using-gcp-39-s-google-ai-platform-a-detailed-tutorial",title:"Deploying Machine Learning models using GCP&#39;s Google AI Platform - A Detailed Tutorial...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using GCP, specifically the Google AI Platform and use Streamlit to access the model through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-03-06_model_deployment_using_gcp_google_ai_platform/"}},{id:"post-deploying-machine-learning-models-using-aws-lambda-and-github-actions-a-detailed-tutorial",title:"Deploying Machine Learning models using AWS Lambda and Github Actions - A Detailed...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using AWS Lambda, Github Actions, API Gateway and use Streamlit to access the model API through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-01-23_model_deployment_using_aws_lambda/"}},{id:"post-ppml-series-3-federated-learning-for-mobile-keyboard-prediction",title:"PPML Series #3 - Federated Learning for Mobile Keyboard Prediction",description:"Understanding how your mobile keyboard (Gboard, specifically) performs the next word prediction task and performs model training and updates",section:"Posts",handler:()=>{window.location.href="/post/2021-12-27_federated_learning_mobile_keyboard/"}},{id:"post-ppml-series-2-federated-optimization-algorithms-fedsgd-and-fedavg",title:"PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg",description:"A mathematical deep dive on a Federated Optimization algorithm - FedAvg and comparing it with a standard approach - FedSGD.",section:"Posts",handler:()=>{window.location.href="/post/2021-12-18_federated_optimization_fedavg/"}},{id:"post-ppml-series-1-an-introduction-to-federated-learning",title:"PPML Series #1 - An introduction to Federated Learning",description:"A short general introduction to Federated Learning (FL) for folks interested in privacy-preserving machine learning (PPML).",section:"Posts",handler:()=>{window.location.href="/post/2021-12-11_intro_to_federated_learning/"}},{id:"post-paper-summary-6-language-models-are-unsupervised-multitask-learners",title:"Paper Summary #6 - Language Models are Unsupervised Multitask Learners",description:"The GPT2 model which aimed to perform complex NLP tasks while relying only on a language model trained in a completely unsupervised fashion.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/"}},{id:"post-paper-summary-5-xlnet-generalized-autoregressive-pretraining-for-language-understanding",title:"Paper Summary #5 - XLNet: Generalized Autoregressive Pretraining for Language Understanding",description:"XLNet tries to overcome the limitations of BERT by having a autoregressive component while also capturing the bidirectional context.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/"}},{id:"post-paper-summary-4-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding",title:"Paper Summary #4 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding...",description:"The ground breaking paper that introduced the famous BERT model. This started the inflow of a large number of BERT-based language understanding models.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/"}},{id:"post-paper-summary-3-improving-language-understanding-by-generative-pre-training",title:"Paper Summary #3 - Improving Language Understanding by Generative Pre-Training",description:"The first paper in the GPT set of models. This is OpenAI&#39;s GPT-1.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-02_language_understanding_generative_pretraining/"}},{id:"post-paper-summary-2-deep-contextualized-word-representations-elmo",title:"Paper Summary #2 - Deep contextualized word representations (ELMo)",description:"The second post in the paper notes series. This time we take a look at ELMo.",section:"Posts",handler:()=>{window.location.href="/post/2021-04-25_deep_contextualized_word_representations_elmo/"}},{id:"post-paper-summary-1-attention-is-all-you-need",title:"Paper Summary #1 - Attention Is All You Need",description:"The first of the paper summary series. This is where I briefly summarise the important papers that I read for my job or just for fun :P",section:"Posts",handler:()=>{window.location.href="/post/2021-04-18_attention_is_all_you_need/"}},{id:"post-deep-learning-in-the-browser-exploring-tf-js-webdnn-and-onnx-js",title:"Deep Learning in the Browser - Exploring TF.js, WebDNN and ONNX.js",description:"A quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2021-01-25_deep_learning_in_the_browser/"}},{id:"post-quick-tutorial-to-deploy-your-ml-models-using-fastapi-and-docker",title:"Quick tutorial to deploy your ML models using FastAPI and Docker",description:"Just a quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2020-11-30_fast_api_docker_ml_deploy/"}},{id:"post-androids-encryption-crypto-pwn2win-ctf-2020",title:"Androids Encryption (Crypto) - Pwn2Win CTF 2020",description:"My writeup for Androids Encryption challenge in the Pwn2Win CTF 2020",section:"Posts",handler:()=>{window.location.href="/post/2020-06-01_androids_encryption-pwn2win-2020/"}},{id:"post-malwaretech-39-s-vm1-reversing-challenge",title:"MalwareTech&#39;s VM1 Reversing Challenge",description:"My writeup for the VM1 reversing challenge posted by MalwareTech on his website.",section:"Posts",handler:()=>{window.location.href="/post/2020-01-04_malwaretech-vm1-challenge/"}},{id:"post-hxp-36c3-ctf-writeups",title:"hxp 36C3 CTF Writeups",description:"The writeups for the challenges I solved in the first MAJOR CTF that I participated in after a long time.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-30_hxp-36c3-ctf/"}},{id:"post-watevrctf-2019-writeups-mainly-rev-and-pwn",title:"watevrCTF 2019 Writeups (Mainly Rev and Pwn)",description:"My writeups for the challenges I solved in the CTF. I mainly focused on Rev and Pwn categories.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-15_watevr-ctf-2019-writeups/"}},{id:"post-tuctf-2019-pwn-amp-rev-challenges",title:"TUCTF 2019 - Pwn &amp; Rev Challenges",description:"My writeups for some of the PWN challenges of TUCTF 2019.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-02_tuctf-pwn-2019/"}},{id:"post-ritsec-ctf-2019",title:"RITSEC CTF 2019",description:"My writeups for RITSEC CTF 2019. A bit late, but I hope this helps someone!",section:"Posts",handler:()=>{window.location.href="/post/2019-11-19_ritsec-ctf-2019/"}},{id:"post-codefest-39-19-ctf-writeups",title:"Codefest&#39;19 CTF Writeups",description:"The Capture the Flag event for Codefest&#39;19 was hosted from 8 pm IST, 23rd August 2019 to 12 noon IST, 24th August 2019 on Hackerrank.",section:"Posts",handler:()=>{window.location.href="/post/2019-08-25_codefest19-ctf-writeups/"}},{id:"post-angstromctf-writeups",title:"AngstromCTF Writeups",description:"These are the writeups to the problems I solved during the AngstromCTF.",section:"Posts",handler:()=>{window.location.href="/post/2018-03-23_angstromctf-writeups/"}},{id:"post-neverlan-ctf-2018-writeups",title:"NeverLAN CTF 2018 Writeups",description:"These are the writeups of the problems I solved over the weekend for the NeverLAN CTF 2018.",section:"Posts",handler:()=>{window.location.href="/post/2018-02-27_neverlan-ctf-2018-writeups/"}},{id:"news-paper-accepted-at-the-workshop-on-multilingual-surface-realisation-acl",title:"Paper accepted at the Workshop on Multilingual Surface Realisation, ACL",description:"",section:"News",handler:()=>{window.location.href="/news/srst-acl/"}},{id:"news-won-a-student-scholarship-to-attend-blackhat-asia-2019-singapore-100-students-were-selected-from-82-countries",title:"Won a student scholarship to attend BlackHat Asia 2019, Singapore. 100 students were...",description:"",section:"News"},{id:"news-started-to-work-as-a-ta-for-the-artificial-intelligence-course-offered-to-sophomores-of-the-cse-department-of-iit-bhu-varanasi",title:"Started to work as a TA for the Artificial Intelligence course offered to...",description:"",section:"News"},{id:"news-got-my-first-job-and-started-my-career-in-the-field-of-ai-as-a-research-data-scientist-at-mastercard-ai-garage",title:"Got my first job and started my career in the field of AI...",description:"",section:"News"},{id:"news-silver-medal-shopee-price-match-guarantee-competition",title:"Silver medal - Shopee - Price Match Guarantee Competition",description:"",section:"News",handler:()=>{window.location.href="/news/silver-kaggle/"}},{id:"news-paper-accepted-at-the-30th-international-conference-on-artificial-neural-networks-icann-2021",title:"Paper accepted at the 30th International Conference on Artificial Neural Networks (ICANN 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/icann/"}},{id:"news-paper-accepted-at-the-28th-international-conference-on-neural-information-processing-iconip-2021",title:"Paper accepted at the 28th International Conference on Neural Information Processing (ICONIP 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/iconip/"}},{id:"news-joined-level-ai-as-a-machine-learning-engineer-in-nlp",title:"Joined Level AI as a Machine Learning Engineer in NLP.",description:"",section:"News"},{id:"news-promoted-to-senior-ml-engineer-at-level-ai",title:"Promoted to Senior ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-lead-ml-engineer-at-level-ai",title:"Promoted to Lead ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-principal-ml-engineer-at-level-ai",title:"Promoted to Principal ML Engineer at Level AI!",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%68%72%65%79%61%6E%73%68.%70%65%74%74%73%77%6F%6F%64@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=x8LmoJIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shreyansh26","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/shreyansh26","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/shreyansh_26","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@shreyansh26","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/shreyanshs","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>