<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Faster Cross-Encoder Inference: Unleashing torch.compile for speed | Shreyansh Singh </title> <meta name="author" content="Shreyansh Singh"> <meta name="description" content="A quick writeup on accelerating a Jina Cross-Encoder using torch.compile"> <meta property="og:site_name" content="Shreyansh Singh"> <meta property="og:type" content="article"> <meta property="og:title" content="Shreyansh Singh | Faster Cross-Encoder Inference: Unleashing torch.compile for speed"> <meta property="og:url" content="https://shreyansh26.github.io/post/2025-03-02_cross-encoder-inference-torch-compile/"> <meta property="og:description" content="A quick writeup on accelerating a Jina Cross-Encoder using torch.compile"> <meta property="og:image" content="https://shreyansh26.github.io/assets/img/posts_images/jina_torch_compile/image.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:title" content="Faster Cross-Encoder Inference: Unleashing torch.compile for speed"> <meta name="twitter:description" content="A quick writeup on accelerating a Jina Cross-Encoder using torch.compile"> <meta name="twitter:image" content="https://shreyansh26.github.io/assets/img/posts_images/jina_torch_compile/image.png"> <meta name="twitter:site" content="@shreyansh_26"> <meta name="twitter:creator" content="@shreyansh_26"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link defer href="/assets/css/toc-custom.css?018b6fb1db2a03192ea04ff844223cb2" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/profile_pictures/favicon.ico?7def459033b808c2a47593bed4cfb002"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shreyansh26.github.io/post/2025-03-02_cross-encoder-inference-torch-compile/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shreyansh</span> Singh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/post/index.html">Posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/bookshelf/">Bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume/Resume_Shreyansh.pdf">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row toc-layout"> <div class="col-sm-3 toc-sidebar"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9 toc-content"> <div class="post"> <header class="post-header"> <h1 class="post-title">Faster Cross-Encoder Inference: Unleashing torch.compile for speed</h1> <p class="post-meta"> Published on March 02, 2025 by Shreyansh Singh </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/inference-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> inference-optimization</a>   <a href="/blog/tag/efficiency"> <i class="fa-solid fa-hashtag fa-sm"></i> efficiency</a>   <a href="/blog/tag/mlsys"> <i class="fa-solid fa-hashtag fa-sm"></i> mlsys</a>   ·   <a href="/blog/category/mlsys"> <i class="fa-solid fa-tag fa-sm"></i> MLSys</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <hr> <p><strong>Code</strong> - <a href="https://github.com/shreyansh26/Accelerating-Cross-Encoder-Inference" rel="external nofollow noopener" target="_blank">Github repo</a></p> <hr> <p>When deploying large ML models in production, optimization becomes crucial for maintaining both performance and cost-effectiveness. In this post, I’ll share my experience optimizing the inference of a cross-encoder (reranker) model using torch.compile and a custom batching strategy. We’ll explore how combining torch.compile with careful input handling can significantly improve inference speed.</p> <h2 id="the-setup-cross-encoder-neural-reranker-model">The Setup: Cross-Encoder (Neural Reranker) Model</h2> <p>For this experiment, I used the Jina reranker model (<code class="language-plaintext highlighter-rouge">jinaai/jina-reranker-v2-base-multilingual</code>), which is designed for scoring the similarity between text pairs. Such type of models are used in a lot of applications like information retrieval, semantic search, recommender systems, etc. The model takes pairs of text as input and outputs similarity scores. Here’s what makes this use case interesting:</p> <ol> <li>Variable input lengths (here we assume each text contains 2-15 sentences)</li> <li>Batch processing</li> </ol> <p>While running inference at scale, even the smallest of optimizations can make a huge difference.</p> <blockquote> <p><strong>Note</strong> - The optimizations and the techniques described in this post are not silver bullets for model inference optimization. Models may have different architectures and inference algorithms which can completely change how they can be optimized. However, the general principles described in this post would definitely hold.</p> </blockquote> <h2 id="understanding-torchcompile-and-the-inductor-backend">Understanding torch.compile and the Inductor Backend</h2> <p>PyTorch 2.0 (and onwards) comes with <code class="language-plaintext highlighter-rouge">torch.compile</code>. Although there are a <ins><a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="external nofollow noopener" target="_blank">bunch</a></ins> <ins><a href="https://pytorch.org/docs/stable/torch.compiler.html" rel="external nofollow noopener" target="_blank">of resources</a></ins> to understand how it works, in short, torch.compile JIT (just in time) compiles your model and makes your Pytorch code run faster by using optimizations like operation fusion, graph capture, custom triton kernels, etc.</p> <p>There are various choices of backends for torch.compile. I used the <code class="language-plaintext highlighter-rouge">inductor</code> backend in my experiments as it is also the most advanced Pytorch-native backend at the moment. Let’s understand how it works:</p> <h3 id="how-inductor-works">How Inductor Works</h3> <p>At its core, Inductor optimizes your PyTorch model through several key steps:</p> <ol> <li> <strong>Graph Capture</strong>: (TorchDynamo) When you first run your compiled model, Inductor captures the computational graph of your operations.</li> <li> <strong>Operation Fusion</strong>: (TorchDynamo) Multiple operations are combined where possible to reduce memory transfers.</li> <li> <strong>Hardware-Specific Optimization</strong>: (TorchInductor) The backend generates optimized kernels specifically for your GPU.</li> </ol> <p>Here’s how we set up our compiled model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_compile</span> <span class="o">=</span> <span class="nc">DynamicCrossEncoder</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">jinaai/jina-reranker-v2-base-multilingual</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">config_args</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">use_flash_attn</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">model_compile</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">model_compile</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">,</span> 
    <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">max-autotune</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">dynamic</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</code></pre></div></div> <p>The key parameters we’re using:</p> <ul> <li><code class="language-plaintext highlighter-rouge">backend="inductor"</code></li> <li> <code class="language-plaintext highlighter-rouge">mode="max-autotune"</code>: Enables aggressive optimization</li> <li> <code class="language-plaintext highlighter-rouge">dynamic=True</code>: Handles our variable input sizes</li> </ul> <p>If you’re curious as to why we set <code class="language-plaintext highlighter-rouge">use_flash_attn = False</code>, I discuss it in a <a href="#but-why-set-use_flash_attn--false">later section</a> after describing the optimizations and results.</p> <h2 id="smart-batching-with-length-buckets">Smart Batching with Length Buckets</h2> <p>Having static shapes is ideal for torch.compile. If there are a variations in the sizes of the variables, then TorchDynamo will have to trace all such variations. Keeping the number of size variations minimum while still giving enough flexibility will be our goal.</p> <p>One way to do it is, depending on the lengths of our sentences in the dataset, we can decide to keep a static sequence length for the model by specifying the <code class="language-plaintext highlighter-rouge">max_length</code> parameter while initializing the cross encoder. This length could be the maximum sequence length or a high enough length that covers most sequences (the ones longer would be truncated), The main issue with this approach is that for sequence lengths much smaller than the fixed length (which could be a significant portion of the dataset), we would be wasting a lot of compute on the padding tokens.</p> <p>In our experiment, we tackle this by creating sequence-length buckets for padding. Instead of padding all sequences to the maximum length in the batch, we pad to the nearest multiple of 16. Obviously this is not perfect, but in my experience of using cross encoders, I find that a max-length of 512 is enough for most practical use cases where a reranker works effectively. In case we do need longer sequence lengths, I would recommend increasing the bucket size from 16 to 32 or even higher based on the maximum length we need.</p> <p>Here’s our implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">BUCKETS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">528</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">smart_batching_collate_text_only</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[[</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">field</span><span class="p">]</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)]</span>
    <span class="n">tokenized</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tokenizer</span><span class="p">(</span>
        <span class="o">*</span><span class="n">texts</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="sh">"</span><span class="s">longest_first</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">max_length</span>
    <span class="p">)</span>
    <span class="n">tokenized</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tokenized</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>

    <span class="c1"># Pad to nearest bucket
</span>    <span class="n">cur_length</span> <span class="o">=</span> <span class="n">tokenized</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">].</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bucket_length</span> <span class="o">=</span> <span class="nf">next</span><span class="p">((</span><span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">BUCKETS</span> <span class="k">if</span> <span class="n">b</span> <span class="o">&gt;=</span> <span class="n">cur_length</span><span class="p">),</span> <span class="n">cur_length</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bucket_length</span> <span class="o">&gt;</span> <span class="n">cur_length</span><span class="p">:</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">bucket_length</span> <span class="o">-</span> <span class="n">cur_length</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">tokenized</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">pad_value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">tokenized</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">diff</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="n">pad_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenized</span>
</code></pre></div></div> <p>This bucketing approach helps in two ways:</p> <ol> <li>Reduces wasted computation on padding tokens</li> <li>Helps the compiled model optimize for specific input sizes</li> </ol> <h2 id="input-sorting-for-better-efficiency">Input Sorting for Better Efficiency</h2> <p>To further improve performance, we implemented input sorting. This groups similarly-sized inputs together, making our bucket-based padding more effective:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">on_sorted_inputs</span><span class="p">:</span>
    <span class="c1"># Sort by max length of each pair
</span>    <span class="n">lengths</span> <span class="o">=</span> <span class="p">[(</span><span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">i</span><span class="p">)</span> 
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">sentence_pairs</span><span class="p">)]</span>
    <span class="n">sorted_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>
    <span class="n">sentence_pairs_sorted</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence_pairs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sorted_indices</span><span class="p">]</span>
</code></pre></div></div> <h2 id="but-why-set-use_flash_attn--false">But why set <code class="language-plaintext highlighter-rouge">use_flash_attn = False</code>?</h2> <p>While Flash Attention is generally faster than vanilla attention implementations, there are several technical reasons why I disabled it when using torch.compile for this particular optimization:</p> <h3 id="1-variable-sequence-lengths-complicate-tracing">1. Variable sequence lengths complicate tracing</h3> <p>Flash Attention operates through highly optimized CUDA kernels that are already compiled for performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In FlashSelfAttention, from mha.py - showing Flash Attention's compiled nature
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/mha.py
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">qkv</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">cu_seqlens</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">max_seqlen</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1"># ...
</span>    <span class="k">if</span> <span class="n">unpadded</span><span class="p">:</span>
        <span class="c1"># Using pre-compiled CUDA kernel
</span>        <span class="k">return</span> <span class="nf">flash_attn_varlen_qkvpacked_func</span><span class="p">(</span>
            <span class="n">qkv</span><span class="p">,</span>
            <span class="n">cu_seqlens</span><span class="p">,</span>
            <span class="n">max_seqlen</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">drop</span><span class="p">.</span><span class="n">p</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="n">softmax_scale</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">softmax_scale</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
            <span class="c1"># ...
</span>        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Using pre-compiled CUDA kernel
</span>        <span class="k">return</span> <span class="nf">flash_attn_qkvpacked_func</span><span class="p">(</span>
            <span class="n">qkv</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">drop</span><span class="p">.</span><span class="n">p</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="n">softmax_scale</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">softmax_scale</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
            <span class="c1"># ...
</span>        <span class="p">)</span>
</code></pre></div></div> <p>The goal of our bucketing strategy was to have a consistent and a small number of tensor shapes for efficient compilation. However, when using <code class="language-plaintext highlighter-rouge">flash_attn_varlen_qkvpacked_func</code> the unpadding mechanism in the original Flash Attention implementation leads to dynamic tensor shapes that are difficult to trace:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From xlm_padding.py, and called in modeling_xlm_roberta.py
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/xlm_padding.py
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/modeling_xlm_roberta.py
</span><span class="k">def</span> <span class="nf">unpad_input</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Convert padded sequences to packed format for efficiency
    </span><span class="sh">"""</span>
    <span class="n">seqlens_in_batch</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">nonzero</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">as_tuple</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
    <span class="n">max_seqlen_in_batch</span> <span class="o">=</span> <span class="n">seqlens_in_batch</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">cu_seqlens</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">seqlens_in_batch</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="nf">return </span><span class="p">(</span>
        <span class="nf">index_first_axis</span><span class="p">(</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="sh">"</span><span class="s">b s ... -&gt; (b s) ...</span><span class="sh">"</span><span class="p">),</span> <span class="n">indices</span><span class="p">),</span>
        <span class="n">indices</span><span class="p">,</span>
        <span class="n">cu_seqlens</span><span class="p">,</span>
        <span class="n">max_seqlen_in_batch</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <p>This operation creates tensors with sizes dependent on the input data, which conflicts with our bucketing strategy where we want to pad to the nearest multiple of 16. This dynamic sizing makes it challenging for torch.compile to effectively trace and optimize the model.</p> <h3 id="2-attention-mask-handling-limitations">2. Attention mask handling limitations</h3> <p>The alternative in the code was to use <code class="language-plaintext highlighter-rouge">flash_attn_qkvpacked_func</code> which doesn’t offer the flexibility we needed for custom attention masking as it expects qkv matrices together and internally handles causal or non-causal masking.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In FlashSelfAttention, from mha.py
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/mha.py
</span><span class="k">return</span> <span class="nf">flash_attn_qkvpacked_func</span><span class="p">(</span>
    <span class="n">qkv</span><span class="p">,</span>
    <span class="n">self</span><span class="p">.</span><span class="n">drop</span><span class="p">.</span><span class="n">p</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">softmax_scale</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">softmax_scale</span><span class="p">,</span>
    <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
    <span class="n">alibi_slopes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">window_size</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">window_size</span><span class="p">,</span>
    <span class="n">deterministic</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">deterministic</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>While there is a regular <code class="language-plaintext highlighter-rouge">flash_attn_func</code> that might have worked, integrating our attention mask to mask padding tokens was not straightforward.</p> <h2 id="the-hybrid-approach">The Hybrid Approach</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In SelfAttention, from mha.py
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/mha.py
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">qkv</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Implements the multihead softmax attention.
        Arguments
        ---------
            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)
            causal: if passed, will override self.causal
            key_padding_mask: boolean mask to apply to the attention weights. True means to keep,
                False means to mask out. (B, S)
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seqlen</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qkv</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">causal</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">causal</span> <span class="k">if</span> <span class="n">causal</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">causal</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">unbind</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">softmax_scale</span> <span class="ow">or</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bthd,bshd-&gt;bhts</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">softmax_scale</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">),</span> <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="p">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">padding_mask</span><span class="p">.</span><span class="nf">masked_fill_</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="sh">"</span><span class="s">b s -&gt; b 1 1 s</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">causal</span><span class="p">:</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">triu</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">),</span> <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="p">.</span><span class="n">device</span><span class="p">),</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">causal_mask</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">v</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">attention_drop</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bhts,bshd-&gt;bthd</span><span class="sh">"</span><span class="p">,</span> <span class="n">attention_drop</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>The standard PyTorch attention implementation (without Flash Attention) allowed torch.compile to see through the entire computation graph and apply optimizations like operation fusion and kernel generation tailored to our specific inputs.</p> <p>By disabling Flash Attention but keeping our bucketing and sorting strategies, we created a middle ground that allowed torch.compile to shine. This approach:</p> <ol> <li>Gives torch.compile more visibility into the computation graph</li> <li>Maintains consistent tensor shapes through our bucketing strategy</li> <li>Allows handling of attention mask quite simply</li> </ol> <p>The results showed this hybrid approach outperformed the baseline (Flash Attention) implementation. Even without input sorting, the torch.compile version was faster or about the same as the baseline (Flash Attention) + input sorting version. </p> <h2 id="benchmarking">Benchmarking</h2> <p>Our benchmarking system provides reliable measurements through proper warm-up and synchronization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">benchmark</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_scores</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_runs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">on_sorted_inputs</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>  
    <span class="n">sentence_pairs_warmup</span> <span class="o">=</span> <span class="nf">load_and_sample_sentences</span><span class="p">(</span><span class="n">num_pairs</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">base_seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">sentence_pairs</span> <span class="o">=</span> <span class="nf">load_and_sample_sentences</span><span class="p">(</span><span class="n">num_pairs</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">base_seed</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
        <span class="c1"># Warmup
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Warming up...</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">sentence_pairs_warmup</span> <span class="o">=</span> <span class="nf">load_and_sample_sentences</span><span class="p">(</span><span class="n">num_pairs</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">base_seed</span><span class="o">=</span><span class="n">seed</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">_</span> <span class="o">=</span> <span class="nf">inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentence_pairs_warmup</span><span class="p">)</span>

        <span class="c1"># Multiple benchmark runs
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Benchmarking...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
            <span class="n">sentence_pairs</span> <span class="o">=</span> <span class="nf">load_and_sample_sentences</span><span class="p">(</span><span class="n">num_pairs</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">base_seed</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">seed</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">on_sorted_inputs</span><span class="p">:</span>
                <span class="c1"># Apply sorting if enabled
</span>                <span class="n">lengths</span> <span class="o">=</span> <span class="p">[(</span><span class="nf">max</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]))),</span> <span class="n">i</span><span class="p">)</span> 
                          <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">sentence_pairs</span><span class="p">)]</span>
                <span class="n">sorted_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>
                <span class="n">sentence_pairs_sorted</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence_pairs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sorted_indices</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sentence_pairs_sorted</span> <span class="o">=</span> <span class="n">sentence_pairs</span>
                <span class="n">sorted_indices</span> <span class="o">=</span> <span class="bp">None</span>

            <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
            
            <span class="n">scores</span> <span class="o">=</span> <span class="nf">inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentence_pairs_sorted</span><span class="p">)</span>
            
            <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
            <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
</code></pre></div></div> <h2 id="results">Results</h2> <p>Here are our key findings:</p> <table> <thead> <tr> <th style="text-align: left">Configuration</th> <th style="text-align: left">Mean Time (s)</th> <th style="text-align: left">Std Dev (s)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Baseline (Without Flash Attention) + Unsorted Inputs</td> <td style="text-align: left">0.3566</td> <td style="text-align: left">0.0101</td> </tr> <tr> <td style="text-align: left">Baseline (Without Flash Attention) + Sorted Inputs</td> <td style="text-align: left">0.3245</td> <td style="text-align: left">0.0623</td> </tr> <tr> <td style="text-align: left">Baseline (Flash Attention) + Unsorted Inputs</td> <td style="text-align: left">0.2961</td> <td style="text-align: left">0.0089</td> </tr> <tr> <td style="text-align: left">Baseline (Flash Attention) + Sorted Inputs</td> <td style="text-align: left">0.2658</td> <td style="text-align: left">0.0119</td> </tr> <tr> <td style="text-align: left">torch.compile + Unsorted Inputs</td> <td style="text-align: left">0.2595</td> <td style="text-align: left">0.0077</td> </tr> <tr> <td style="text-align: left"><strong>torch.compile + Sorted Inputs</strong></td> <td style="text-align: left"><strong>0.2089</strong></td> <td style="text-align: left"><strong>0.0196</strong></td> </tr> </tbody> </table> <p><br></p> <h3 id="key-observations">Key observations:</h3> <ol> <li>torch.compile provides upto ~1.3x speedup over the base model</li> <li>Input sorting improves performance by upto 1.25x</li> <li>The combination of torch.compile and sorted inputs gives us the best performance</li> </ol> <h2 id="best-practices-and-learnings">Best Practices and Learnings</h2> <p>Through this optimization process, we discovered several important practices:</p> <ol> <li> <p><strong>Proper Warm-up</strong>: Always run warm-up iterations before benchmarking to ensure the compiled model has optimized its execution path and seen all variations of sizes so that there are no recompilations during the actual benchmarking.</p> </li> <li> <p><strong>Accurate Timing</strong>: Use proper CUDA synchronization for accurate measurements:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="c1"># ... inference ...
</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
</code></pre></div> </div> </li> </ol> <h2 id="conclusion">Conclusion</h2> <p>By combining torch.compile with smart batching and input sorting, we achieved a significant speedup in our neural reranker inference. The key takeaway is that optimization often requires a multi-faceted approach - compiler optimizations alone might not give you the best results, but when combined with domain-specific optimizations like bucket-based padding and input sorting, the improvements can be substantial.</p> <p>For those looking to optimize their own models, I recommend:</p> <ol> <li>Start with torch.compile as it’s relatively easy to implement</li> <li>Add bucket-based padding if you have variable-length inputs</li> <li>Consider input sorting if your batch sizes are large enough to benefit from it</li> <li>Always measure and profile your specific use case, as the benefits of each optimization can vary depending on your model and data</li> </ol> <p>The complete code for this optimization project is available in the snippets above. Feel free to adapt these techniques for your own use case!</p> <hr> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26" rel="external nofollow noopener" target="_blank">Twitter</a>, <a href="https://github.com/shreyansh26" rel="external nofollow noopener" target="_blank">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/" rel="external nofollow noopener" target="_blank">LinkedIn</a>.</p> </div> </article> <div id="giscus_thread" style="max-width: 1440px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shreyansh26/shreyansh26.github.io","data-repo-id":"R_kgDOMn767Q","data-category":"General","data-category-id":"DIC_kwDOMn767c4CiLUv","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Shreyansh Singh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?317646887210841a1b23a5eedc9fba39"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZEZ7673Y7G"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZEZ7673Y7G");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-posts",title:"Posts",description:"",section:"Navigation",handler:()=>{window.location.href="/post/index.html"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-bookshelf",title:"Bookshelf",description:"A list of books I&#39;ve read and some of which I&#39;ve found interesting :)",section:"Navigation",handler:()=>{window.location.href="/bookshelf/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/assets/pdf/resume/Resume_Shreyansh.pdf"}},{id:"post-paper-summary-14-physics-of-language-models-part-3-1-knowledge-storage-and-extraction",title:"Paper Summary #14 - Physics of Language Models: Part 3.1, Knowledge Storage and...",description:"My notes from the Physics of Language Models series of papers.",section:"Posts",handler:()=>{window.location.href="/post/2026-01-17_physics-of-lms-3-1-knowledge-storage-and-extraction/"}},{id:"post-understanding-multi-head-latent-attention-mla",title:"Understanding Multi-Head Latent Attention (MLA)",description:"A mathematical and code deep-dive on one of the key innovations from Deepseek - Multihead Latent Attention (MLA)",section:"Posts",handler:()=>{window.location.href="/post/2025-11-08_multihead-latent-attention/"}},{id:"post-deriving-the-gradient-for-the-backward-pass-of-layer-normalization",title:"Deriving the Gradient for the Backward Pass of Layer Normalization",description:"Understanding the math behind Layer Normalization and deriving the gradients for the backward pass.",section:"Posts",handler:()=>{window.location.href="/post/2025-06-04_layernorm-gradients/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-compute-and-instruction-throughput",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Compute and Instruction Throughput",description:"My notes from the talk on maximizing compute and instruction throughput at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-04-04_gtc25-maximize-compute-instruction-throughput/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-2",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"Second part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-2/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-1",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"First part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/"}},{id:"post-faster-cross-encoder-inference-unleashing-torch-compile-for-speed",title:"Faster Cross-Encoder Inference: Unleashing torch.compile for speed",description:"A quick writeup on accelerating a Jina Cross-Encoder using torch.compile",section:"Posts",handler:()=>{window.location.href="/post/2025-03-02_cross-encoder-inference-torch-compile/"}},{id:"post-paper-summary-13-physics-of-language-models-part-2-1-grade-school-math-and-the-hidden-reasoning-process",title:"Paper Summary #13 - Physics of Language Models: Part 2.1, Grade-School Math and...",description:"My notes from the Physics of Language Models series of papers.",section:"Posts",handler:()=>{window.location.href="/post/2024-09-21_physics-of-lms-2-1-grade-school-math-and-the-hidden-reasoning-process/"}},{id:"post-paper-summary-12-image-recaptioning-in-dall-e-3",title:"Paper Summary #12 - Image Recaptioning in DALL-E 3",description:"The image recaptioning technique used in DALL-E 3 was extended to videos in Sora.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_dalle3_image_recaptioner/"}},{id:"post-paper-summary-11-sora",title:"Paper Summary #11 - Sora",description:"OpenAI announced a ground-breaking text-to-video diffusion model capable of generating high-definition videos up to 60 seconds long.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_sora_openai/"}},{id:"post-paper-summary-10-gemini-1-5-pro",title:"Paper Summary #10 - Gemini 1.5 Pro",description:"Google DeepMind announced a multimodal LLM with support of up to 10M context length.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_gemini_pro_google/"}},{id:"post-solving-substitution-ciphers-using-markov-chain-monte-carlo-mcmc",title:"Solving Substitution Ciphers using Markov Chain Monte Carlo (MCMC)",description:"Deciphering substitution ciphers can be framed as a Markov chain problem and a simple Monte Carlo sampling approach can help solve them very efficiently",section:"Posts",handler:()=>{window.location.href="/post/2023-07-22_solving_substitution_cipher_using_mcmc/"}},{id:"post-paper-summary-9-sophia-a-scalable-stochastic-second-order-optimizer-for-language-model-pre-training",title:"Paper Summary #9 - Sophia: A Scalable Stochastic Second-order Optimizer for Language Model...",description:"Understanding Sophia - A new fast, scalable second-order optimizer which beats Adam on LLM pretraining.",section:"Posts",handler:()=>{window.location.href="/post/2023-05-28_sophia_scalable_second_order_optimizer_llms/"}},{id:"post-paper-summary-8-flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness",title:"Paper Summary #8 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",description:"Understanding FlashAttention which is the most efficient exact attention implementation out there, which optimizes for both memory requirements and wall-clock time.",section:"Posts",handler:()=>{window.location.href="/post/2023-03-26_flash-attention/"}},{id:"post-paper-summary-7-efficient-transformers-a-survey",title:"Paper Summary #7 - Efficient Transformers: A Survey",description:"A survey paper of improvements over the original Transformer architecture in terms of memory-efficiency.",section:"Posts",handler:()=>{window.location.href="/post/2022-10-10_efficient_transformers_survey/"}},{id:"post-deploying-machine-learning-models-using-gcp-39-s-google-ai-platform-a-detailed-tutorial",title:"Deploying Machine Learning models using GCP&#39;s Google AI Platform - A Detailed Tutorial...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using GCP, specifically the Google AI Platform and use Streamlit to access the model through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-03-06_model_deployment_using_gcp_google_ai_platform/"}},{id:"post-deploying-machine-learning-models-using-aws-lambda-and-github-actions-a-detailed-tutorial",title:"Deploying Machine Learning models using AWS Lambda and Github Actions - A Detailed...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using AWS Lambda, Github Actions, API Gateway and use Streamlit to access the model API through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-01-23_model_deployment_using_aws_lambda/"}},{id:"post-ppml-series-3-federated-learning-for-mobile-keyboard-prediction",title:"PPML Series #3 - Federated Learning for Mobile Keyboard Prediction",description:"Understanding how your mobile keyboard (Gboard, specifically) performs the next word prediction task and performs model training and updates",section:"Posts",handler:()=>{window.location.href="/post/2021-12-27_federated_learning_mobile_keyboard/"}},{id:"post-ppml-series-2-federated-optimization-algorithms-fedsgd-and-fedavg",title:"PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg",description:"A mathematical deep dive on a Federated Optimization algorithm - FedAvg and comparing it with a standard approach - FedSGD.",section:"Posts",handler:()=>{window.location.href="/post/2021-12-18_federated_optimization_fedavg/"}},{id:"post-ppml-series-1-an-introduction-to-federated-learning",title:"PPML Series #1 - An introduction to Federated Learning",description:"A short general introduction to Federated Learning (FL) for folks interested in privacy-preserving machine learning (PPML).",section:"Posts",handler:()=>{window.location.href="/post/2021-12-11_intro_to_federated_learning/"}},{id:"post-paper-summary-6-language-models-are-unsupervised-multitask-learners",title:"Paper Summary #6 - Language Models are Unsupervised Multitask Learners",description:"The GPT2 model which aimed to perform complex NLP tasks while relying only on a language model trained in a completely unsupervised fashion.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/"}},{id:"post-paper-summary-5-xlnet-generalized-autoregressive-pretraining-for-language-understanding",title:"Paper Summary #5 - XLNet: Generalized Autoregressive Pretraining for Language Understanding",description:"XLNet tries to overcome the limitations of BERT by having a autoregressive component while also capturing the bidirectional context.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/"}},{id:"post-paper-summary-4-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding",title:"Paper Summary #4 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding...",description:"The ground breaking paper that introduced the famous BERT model. This started the inflow of a large number of BERT-based language understanding models.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/"}},{id:"post-paper-summary-3-improving-language-understanding-by-generative-pre-training",title:"Paper Summary #3 - Improving Language Understanding by Generative Pre-Training",description:"The first paper in the GPT set of models. This is OpenAI&#39;s GPT-1.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-02_language_understanding_generative_pretraining/"}},{id:"post-paper-summary-2-deep-contextualized-word-representations-elmo",title:"Paper Summary #2 - Deep contextualized word representations (ELMo)",description:"The second post in the paper notes series. This time we take a look at ELMo.",section:"Posts",handler:()=>{window.location.href="/post/2021-04-25_deep_contextualized_word_representations_elmo/"}},{id:"post-paper-summary-1-attention-is-all-you-need",title:"Paper Summary #1 - Attention Is All You Need",description:"The first of the paper summary series. This is where I briefly summarise the important papers that I read for my job or just for fun :P",section:"Posts",handler:()=>{window.location.href="/post/2021-04-18_attention_is_all_you_need/"}},{id:"post-deep-learning-in-the-browser-exploring-tf-js-webdnn-and-onnx-js",title:"Deep Learning in the Browser - Exploring TF.js, WebDNN and ONNX.js",description:"A quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2021-01-25_deep_learning_in_the_browser/"}},{id:"post-quick-tutorial-to-deploy-your-ml-models-using-fastapi-and-docker",title:"Quick tutorial to deploy your ML models using FastAPI and Docker",description:"Just a quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2020-11-30_fast_api_docker_ml_deploy/"}},{id:"post-androids-encryption-crypto-pwn2win-ctf-2020",title:"Androids Encryption (Crypto) - Pwn2Win CTF 2020",description:"My writeup for Androids Encryption challenge in the Pwn2Win CTF 2020",section:"Posts",handler:()=>{window.location.href="/post/2020-06-01_androids_encryption-pwn2win-2020/"}},{id:"post-malwaretech-39-s-vm1-reversing-challenge",title:"MalwareTech&#39;s VM1 Reversing Challenge",description:"My writeup for the VM1 reversing challenge posted by MalwareTech on his website.",section:"Posts",handler:()=>{window.location.href="/post/2020-01-04_malwaretech-vm1-challenge/"}},{id:"post-hxp-36c3-ctf-writeups",title:"hxp 36C3 CTF Writeups",description:"The writeups for the challenges I solved in the first MAJOR CTF that I participated in after a long time.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-30_hxp-36c3-ctf/"}},{id:"post-watevrctf-2019-writeups-mainly-rev-and-pwn",title:"watevrCTF 2019 Writeups (Mainly Rev and Pwn)",description:"My writeups for the challenges I solved in the CTF. I mainly focused on Rev and Pwn categories.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-15_watevr-ctf-2019-writeups/"}},{id:"post-tuctf-2019-pwn-amp-rev-challenges",title:"TUCTF 2019 - Pwn &amp; Rev Challenges",description:"My writeups for some of the PWN challenges of TUCTF 2019.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-02_tuctf-pwn-2019/"}},{id:"post-ritsec-ctf-2019",title:"RITSEC CTF 2019",description:"My writeups for RITSEC CTF 2019. A bit late, but I hope this helps someone!",section:"Posts",handler:()=>{window.location.href="/post/2019-11-19_ritsec-ctf-2019/"}},{id:"post-codefest-39-19-ctf-writeups",title:"Codefest&#39;19 CTF Writeups",description:"The Capture the Flag event for Codefest&#39;19 was hosted from 8 pm IST, 23rd August 2019 to 12 noon IST, 24th August 2019 on Hackerrank.",section:"Posts",handler:()=>{window.location.href="/post/2019-08-25_codefest19-ctf-writeups/"}},{id:"post-angstromctf-writeups",title:"AngstromCTF Writeups",description:"These are the writeups to the problems I solved during the AngstromCTF.",section:"Posts",handler:()=>{window.location.href="/post/2018-03-23_angstromctf-writeups/"}},{id:"post-neverlan-ctf-2018-writeups",title:"NeverLAN CTF 2018 Writeups",description:"These are the writeups of the problems I solved over the weekend for the NeverLAN CTF 2018.",section:"Posts",handler:()=>{window.location.href="/post/2018-02-27_neverlan-ctf-2018-writeups/"}},{id:"news-paper-accepted-at-the-workshop-on-multilingual-surface-realisation-acl",title:"Paper accepted at the Workshop on Multilingual Surface Realisation, ACL",description:"",section:"News",handler:()=>{window.location.href="/news/srst-acl/"}},{id:"news-won-a-student-scholarship-to-attend-blackhat-asia-2019-singapore-100-students-were-selected-from-82-countries",title:"Won a student scholarship to attend BlackHat Asia 2019, Singapore. 100 students were...",description:"",section:"News"},{id:"news-started-to-work-as-a-ta-for-the-artificial-intelligence-course-offered-to-sophomores-of-the-cse-department-of-iit-bhu-varanasi",title:"Started to work as a TA for the Artificial Intelligence course offered to...",description:"",section:"News"},{id:"news-got-my-first-job-and-started-my-career-in-the-field-of-ai-as-a-research-data-scientist-at-mastercard-ai-garage",title:"Got my first job and started my career in the field of AI...",description:"",section:"News"},{id:"news-silver-medal-shopee-price-match-guarantee-competition",title:"Silver medal - Shopee - Price Match Guarantee Competition",description:"",section:"News",handler:()=>{window.location.href="/news/silver-kaggle/"}},{id:"news-paper-accepted-at-the-30th-international-conference-on-artificial-neural-networks-icann-2021",title:"Paper accepted at the 30th International Conference on Artificial Neural Networks (ICANN 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/icann/"}},{id:"news-paper-accepted-at-the-28th-international-conference-on-neural-information-processing-iconip-2021",title:"Paper accepted at the 28th International Conference on Neural Information Processing (ICONIP 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/iconip/"}},{id:"news-joined-level-ai-as-a-machine-learning-engineer-in-nlp",title:"Joined Level AI as a Machine Learning Engineer in NLP.",description:"",section:"News"},{id:"news-promoted-to-senior-ml-engineer-at-level-ai",title:"Promoted to Senior ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-lead-ml-engineer-at-level-ai",title:"Promoted to Lead ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-principal-ml-engineer-at-level-ai",title:"Promoted to Principal ML Engineer at Level AI!",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%68%72%65%79%61%6E%73%68.%70%65%74%74%73%77%6F%6F%64@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=x8LmoJIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shreyansh26","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/shreyansh26","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/shreyansh_26","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@shreyansh26","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/shreyanshs","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>