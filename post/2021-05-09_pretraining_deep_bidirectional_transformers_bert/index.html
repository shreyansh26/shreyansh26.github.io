<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Summary #4 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding | Shreyansh Singh </title> <meta name="author" content="Shreyansh Singh"> <meta name="description" content="The ground breaking paper that introduced the famous BERT model. This started the inflow of a large number of BERT-based language understanding models."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link defer href="/assets/css/toc-custom.css?018b6fb1db2a03192ea04ff844223cb2" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/profile_pictures/favicon.ico?7def459033b808c2a47593bed4cfb002"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shreyansh</span> Singh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/post/index.html">Posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/bookshelf/">Bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume/Resume_Shreyansh.pdf">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Summary #4 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h1> <p class="post-meta"> Published on May 09, 2021 by Shreyansh Singh </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   <a href="/blog/tag/paper-summaries"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-summaries</a>   ·   <a href="/blog/category/llms"> <i class="fa-solid fa-tag fa-sm"></i> LLMs</a>   <a href="/blog/category/nlp"> <i class="fa-solid fa-tag fa-sm"></i> NLP</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/featured.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr> <p><strong>Paper</strong>: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding<br> <strong>Link</strong>: <a href="https://www.aclweb.org/anthology/N19-1423.pdf" rel="external nofollow noopener" target="_blank">https://bit.ly/3bdTUra</a> <br> <strong>Authors</strong>: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova<br> <strong>Code</strong>: <a href="https://github.com/google-research/bert" rel="external nofollow noopener" target="_blank">https://bit.ly/3vRXlM7</a></p> <hr> <h2 id="what">What?</h2> <p>The paper proposes BERT which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pre-train deep bidirectional representations from unlabeled text. It performs a joint conditioning on both left and right context in all the layers. The pre-trained BERT model can be fine-tuned with one additional layer to create the final task-specific models i.e., without substantial task-specific architecture modifications. BERT achieves SOTA results on eleven NLP tasks such as natural language inference, question answering textual similarity, text classification, etc.</p> <h2 id="why">Why?</h2> <p>The existing strategies for the pre-trained language representations are mostly based on unidirectional language models and hence are not very effective in capturing the entire context for sentence-level tasks. These are also harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to capture context from both directions. BERT aims to generate deep bidirectional representations by using maked language models.</p> <h2 id="how">How?</h2> <p>Two main steps in the BERT framework are - pre-training and fine-tuning. Pre-training involves training the model on unlabeled data over different pretraining tasks. During fine-tuning, all the BERT parameters are fine-tuned using the labelled data from the downstream tasks. The fine-tuned model is different for each task, however, they share the same pre-trained parameters.</p> <h3 id="model-architecture">Model Architecture</h3> <p>The underlying architecture of BERT is a multi-layer Transformer encoder, which is inherently bidirectional in nature. Two models are proposed in the paper.</p> <ul> <li>BERT<sub>BASE</sub> - 12 Transformer blocks, 12 self-attention heads, 768 is the hidden size</li> <li>BERT<sub>LARGE</sub> - 24 transformer blocks, 16 self-attention heads, 1024 is the hidden size</li> </ul> <p><i>The model size of BERT<sub>BASE</sub> and Open AI’s GPT was chosen to be the same.</i></p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/model.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="input-output-representations">Input-Output Representations</h3> <p>BERT uses WordPiece embeddings with a 30,000 token vocabulary. The first token of every sequence is ([CLS]). The final hidden state corresponding to the [CLS] token is used as the aggregate sequence representation. <br> To deal with sentence pairs, BERT uses a special token [SEP] to separate the two sentences. A learned embedding is added to every token indicating whether it is the first or the second sentence. The input embedding for each token is obtained by adding the corresponding token embedding (WordPiece embedding), segment embedding (first / second sentence) and position embedding (as in Transformers).</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/inputembeds.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="bert-pre-training">BERT pre-training</h3> <p>BERT is pre-trained using two unsupervised tasks.</p> <h4 id="masked-lm">Masked LM</h4> <p>The bidirectional model is more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and right-to-left model.<br> In order to train a deep bidirectional representation, some percentage (15% in the paper) of the input tokens are masked at random, and those masked tokens are predicted using an output softmax over the vocabulary. This is called a masked LM. The masking is performed by replacing the token with a [MASK] token. Now since the [MASK] token does not appear during fine-tuning, the [MASK] token is used 80% of the time. For 10% of the selected tokens (from the 15%) a random token is used to replace it and the token is kept unchanged for the rest 10%. The token is then predicted using cross-entropy loss.</p> <h4 id="next-sentence-prediction-nsp">Next Sentence Prediction (NSP)</h4> <p>To understand the relationship between two sentences (which is not captured by language modelling), a binarized NSP task is formulated. Here, when choosing the sentences A and B (refer to the model pre-training figure above) for each pre-training example, 50% of the time B is the actual next sentence and the rest 50% of the time, a random sentence from the corpus is used. The vector C (without fine-tuning) is used for NSP. This is helpful for tasks like Question Answering and Natural Language Inference.</p> <h4 id="pre-training-data">Pre-training data</h4> <p>It is useful for BERT to use a document-level corpus rather than a shuffled sentence-level corpus. BERT 9as in the paper) uses the BookCorpus (800M words) and English Wikipedia (2500M words).</p> <h3 id="fine-tuning-bert">Fine-tuning BERT</h3> <p>Instead of independently encoding text (sentence) pairs and then applying bidirectional cross attention, BERT uses the Transformer model architecture’s self-attention mechanism. Encoding the concatenated text (sentence) pair with self-attention effectively incorporates bidirectional cross attention between the two sentences.</p> <p>The fine-tuning is performed for all the parameters and the task-specific inputs and outputs of the downstream task are plugged for fine-tuning.</p> <ul> <li>A and B are the sentence pairs in case of paraphrasing</li> <li>A and B are hypothesis-premise pairs in the entailment task</li> <li>A and B are question-passage pairs in question answering</li> <li>A and B are the text and Φ in text classification or sequence tagging task</li> </ul> <p>At the output, for the token-level tasks (sequence tagging, question answering), the token representations are fed into the output layer. For the sentence-level tasks, the representation of the [CLS] token is fed to the output layer for classification.</p> <hr> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <hr> <h2 id="results">Results</h2> <p><strong>GLUE</strong> - The General Language Understanding Evaluation benchmark is a collection of a number of diverse NLP tasks. The 8 datasets the paper evaluates on, are shown below. For these tasks, the [CLS] representation (hidden vector associated with it) is used. The classification layer (a single layer is used) and its weights are the only new parameters introduced. Standard log softmax loss is used. The model used a batch size of 32 and was fine-tuned for 3 epochs. The learning rate was chosen from a list based on performance on the validation set. BERT<sub>LARGE</sub> was unstable on small datasets so random restarts were done with data shuffling and classification layer initialization. It was found that BERT<sub>LARGE</sub> significantly outperforms BERT<sub>BASE</sub> (and all other models) across all tasks, especially those with very little training data.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/glue.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>SQuAD v1.1</strong> - A collection of 100k question-answer pairs. Given a question and a passage, the task is to predict the answer span in the text. The question and the passage are represented using A and B embedding respectively. A start vector S and end vector E is introduced in the output. The probability of token <em>i</em> being the start of the answer is given as</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/start.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>and similarly for the end token. The score of a candidate span from position <em>i</em> to position <em>j</em> is decided to be -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/etend.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>This objective is maximised to get the answer range. Batch size of 32, learning rate of 5e-5 was used and the model was fine-tuned for 3 epochs. Also, for enhanced performance, a prior fine-tuning on the Trivia-QA dataset was done before the fine-tuning on SQuAD.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/squad1.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>SQuAD v2.0</strong> - This task allows for the possibility of no short answer existing in the passage for the question, to present a more realistic situation. So, in this case, for the questions which don’t have an answer, the start and end is set to be the [CLS] token. So, now there is also a s<sub>null</sub> = S•C + E•C as the no-answer span score. For a non-null answer, a s<sub>i,j</sub> = S•T<sub>i</sub> + E•T<sub>j</sub> is defined. A non-null answer is predicted when s<sub>i,j</sub> &gt; s<sub>null</sub> + τ. τ is decided on the basis of the performance of the model on the validation set. TriviaQA data was not used for this model. The model was fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/squad2.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>SWAG</strong> - The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. Given a sentence, the task is to choose the most correct continuation of the sentence among four choices. Scoring is performed for the four sentence pairs, the given sentence A and the possible continuation B. Here a vector is introduced whose dot product with the [CLS] token representation C denotes the score for each of the four choices and a softmax layer is used to get the probability distribution. The model was fine-tuned for 3 epochs with a learning rate of 2e-5 and a batch size of 16.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/swag.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Key points from the analysis/ablation studies section -</p> <ul> <li>Two additional modes of pre-training were performed. <ul> <li> <strong>No NSP</strong> - The model is pre-trained with mask LM but not with the NSP task.</li> <li> <strong>LTR and No NSP</strong> - Instead of a masked LM, a standard left-to-right LM is used and the NSP task is again not performed.</li> </ul> </li> <li>The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.</li> <li>An LTR model performs poorly on token predictions and hence doesn’t perform well on SQuAD.</li> <li>For strengthening the LTR models, a randomly initialized BiLSTM model is added on the top. This improves the results on SQuAD but does not perform well on the GLUE tasks.</li> <li>Separately training LTR (left-to-right) and RTL (right-to-left) models and concatenating them for the token representations is an approach similar to ELMo. But the authors mention that this is twice as expensive as a single bidirectional model. Also, this is unintuitive for tasks like Question Answering since the RTL model would not be able to condition the answer on the question. Furthermore, it is less powerful than a deep bidirectional model, since it can use both left and right context at every layer.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/ablation1.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>BERT<sub>BASE</sub> contains 110M parameters and BERT<sub>LARGE</sub> contains 340M parameters.</li> <li>Larger models lead to a strict accuracy improvement across all four datasets, even for MRPC (paraphrasing) which only has 3,600 labelled training examples.</li> <li>BERT claims to be the first model to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.</li> <li>When the model is fine-tuned directly on the downstream task and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/ablation2.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>The feature-based model, in which fixed features are obtained from the model, has some advantages. Firstly, not all tasks can be modelled using a Transformer encoder and require task-specific model architecture to be added.</li> <li>Secondly, pre-computing the expensive representations and using them for multiple experiments with cheaper models is a computational benefit.</li> <li>The authors compare the feature-based approach for the BERT inference and the normal BERT for the NER task. In the inference part of the feature-based approach, the activations from one or more layers are taken <em>without</em> any fine-tuning of the BERT parameters for the NER task. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.</li> <li>Although this does not perform better than the fine-tuned approach, the best performing method used the concatenation of the last four hidden layers’ representation of the pre-trained Transformer as the token representation is only 0.3 F1 behind the fine-tuning approach. So, the authors conclude that BERT is effective for both fine-tuning and feature-based approaches.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/bert/ablation3.PNG" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26" rel="external nofollow noopener" target="_blank">Twitter</a>, <a href="https://github.com/shreyansh26" rel="external nofollow noopener" target="_blank">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/" rel="external nofollow noopener" target="_blank">LinkedIn</a>.</p> </div> </article> <div id="giscus_thread" style="max-width: 1200px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shreyansh26/shreyansh26.github.io","data-repo-id":"R_kgDOMn767Q","data-category":"General","data-category-id":"DIC_kwDOMn767c4CiLUv","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Shreyansh Singh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?317646887210841a1b23a5eedc9fba39"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZEZ7673Y7G"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZEZ7673Y7G");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-posts",title:"Posts",description:"",section:"Navigation",handler:()=>{window.location.href="/post/index.html"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-bookshelf",title:"Bookshelf",description:"A list of books I&#39;ve read and some of which I&#39;ve found interesting :)",section:"Navigation",handler:()=>{window.location.href="/bookshelf/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/assets/pdf/resume/Resume_Shreyansh.pdf"}},{id:"post-deriving-the-gradient-for-the-backward-pass-of-layer-normalization",title:"Deriving the Gradient for the Backward Pass of Layer Normalization",description:"Understanding the math behind Layer Normalization and deriving the gradients for the backward pass.",section:"Posts",handler:()=>{window.location.href="/post/2025-06-04_layernorm-gradients/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-compute-and-instruction-throughput",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Compute and Instruction Throughput",description:"My notes from the talk on maximizing compute and instruction throughput at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-04-04_gtc25-maximize-compute-instruction-throughput/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-2",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"Second part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-2/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-1",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"First part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/"}},{id:"post-faster-cross-encoder-inference-unleashing-torch-compile-for-speed",title:"Faster Cross-Encoder Inference: Unleashing torch.compile for speed",description:"A quick writeup on accelerating a Jina Cross-Encoder using torch.compile",section:"Posts",handler:()=>{window.location.href="/post/2025-03-02_cross-encoder-inference-torch-compile/"}},{id:"post-paper-summary-13-physics-of-language-models-part-2-1-grade-school-math-and-the-hidden-reasoning-process",title:"Paper Summary #13 - Physics of Language Models: Part 2.1, Grade-School Math and...",description:"My notes from the Physics of Language Models series of papers.",section:"Posts",handler:()=>{window.location.href="/post/2024-09-21_physics-of-lms-2-1-grade-school-math-and-the-hidden-reasoning-process/"}},{id:"post-paper-summary-12-image-recaptioning-in-dall-e-3",title:"Paper Summary #12 - Image Recaptioning in DALL-E 3",description:"The image recaptioning technique used in DALL-E 3 was extended to videos in Sora.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_dalle3_image_recaptioner/"}},{id:"post-paper-summary-11-sora",title:"Paper Summary #11 - Sora",description:"OpenAI announced a ground-breaking text-to-video diffusion model capable of generating high-definition videos up to 60 seconds long.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_sora_openai/"}},{id:"post-paper-summary-10-gemini-1-5-pro",title:"Paper Summary #10 - Gemini 1.5 Pro",description:"Google DeepMind announced a multimodal LLM with support of up to 10M context length.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_gemini_pro_google/"}},{id:"post-solving-substitution-ciphers-using-markov-chain-monte-carlo-mcmc",title:"Solving Substitution Ciphers using Markov Chain Monte Carlo (MCMC)",description:"Deciphering substitution ciphers can be framed as a Markov chain problem and a simple Monte Carlo sampling approach can help solve them very efficiently",section:"Posts",handler:()=>{window.location.href="/post/2023-07-22_solving_substitution_cipher_using_mcmc/"}},{id:"post-paper-summary-9-sophia-a-scalable-stochastic-second-order-optimizer-for-language-model-pre-training",title:"Paper Summary #9 - Sophia: A Scalable Stochastic Second-order Optimizer for Language Model...",description:"Understanding Sophia - A new fast, scalable second-order optimizer which beats Adam on LLM pretraining.",section:"Posts",handler:()=>{window.location.href="/post/2023-05-28_sophia_scalable_second_order_optimizer_llms/"}},{id:"post-paper-summary-8-flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness",title:"Paper Summary #8 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",description:"Understanding FlashAttention which is the most efficient exact attention implementation out there, which optimizes for both memory requirements and wall-clock time.",section:"Posts",handler:()=>{window.location.href="/post/2023-03-26_flash-attention/"}},{id:"post-paper-summary-7-efficient-transformers-a-survey",title:"Paper Summary #7 - Efficient Transformers: A Survey",description:"A survey paper of improvements over the original Transformer architecture in terms of memory-efficiency.",section:"Posts",handler:()=>{window.location.href="/post/2022-10-10_efficient_transformers_survey/"}},{id:"post-deploying-machine-learning-models-using-gcp-39-s-google-ai-platform-a-detailed-tutorial",title:"Deploying Machine Learning models using GCP&#39;s Google AI Platform - A Detailed Tutorial...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using GCP, specifically the Google AI Platform and use Streamlit to access the model through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-03-06_model_deployment_using_gcp_google_ai_platform/"}},{id:"post-deploying-machine-learning-models-using-aws-lambda-and-github-actions-a-detailed-tutorial",title:"Deploying Machine Learning models using AWS Lambda and Github Actions - A Detailed...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using AWS Lambda, Github Actions, API Gateway and use Streamlit to access the model API through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-01-23_model_deployment_using_aws_lambda/"}},{id:"post-ppml-series-3-federated-learning-for-mobile-keyboard-prediction",title:"PPML Series #3 - Federated Learning for Mobile Keyboard Prediction",description:"Understanding how your mobile keyboard (Gboard, specifically) performs the next word prediction task and performs model training and updates",section:"Posts",handler:()=>{window.location.href="/post/2021-12-27_federated_learning_mobile_keyboard/"}},{id:"post-ppml-series-2-federated-optimization-algorithms-fedsgd-and-fedavg",title:"PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg",description:"A mathematical deep dive on a Federated Optimization algorithm - FedAvg and comparing it with a standard approach - FedSGD.",section:"Posts",handler:()=>{window.location.href="/post/2021-12-18_federated_optimization_fedavg/"}},{id:"post-ppml-series-1-an-introduction-to-federated-learning",title:"PPML Series #1 - An introduction to Federated Learning",description:"A short general introduction to Federated Learning (FL) for folks interested in privacy-preserving machine learning (PPML).",section:"Posts",handler:()=>{window.location.href="/post/2021-12-11_intro_to_federated_learning/"}},{id:"post-paper-summary-6-language-models-are-unsupervised-multitask-learners",title:"Paper Summary #6 - Language Models are Unsupervised Multitask Learners",description:"The GPT2 model which aimed to perform complex NLP tasks while relying only on a language model trained in a completely unsupervised fashion.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/"}},{id:"post-paper-summary-5-xlnet-generalized-autoregressive-pretraining-for-language-understanding",title:"Paper Summary #5 - XLNet: Generalized Autoregressive Pretraining for Language Understanding",description:"XLNet tries to overcome the limitations of BERT by having a autoregressive component while also capturing the bidirectional context.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/"}},{id:"post-paper-summary-4-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding",title:"Paper Summary #4 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding...",description:"The ground breaking paper that introduced the famous BERT model. This started the inflow of a large number of BERT-based language understanding models.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/"}},{id:"post-paper-summary-3-improving-language-understanding-by-generative-pre-training",title:"Paper Summary #3 - Improving Language Understanding by Generative Pre-Training",description:"The first paper in the GPT set of models. This is OpenAI&#39;s GPT-1.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-02_language_understanding_generative_pretraining/"}},{id:"post-paper-summary-2-deep-contextualized-word-representations-elmo",title:"Paper Summary #2 - Deep contextualized word representations (ELMo)",description:"The second post in the paper notes series. This time we take a look at ELMo.",section:"Posts",handler:()=>{window.location.href="/post/2021-04-25_deep_contextualized_word_representations_elmo/"}},{id:"post-paper-summary-1-attention-is-all-you-need",title:"Paper Summary #1 - Attention Is All You Need",description:"The first of the paper summary series. This is where I briefly summarise the important papers that I read for my job or just for fun :P",section:"Posts",handler:()=>{window.location.href="/post/2021-04-18_attention_is_all_you_need/"}},{id:"post-deep-learning-in-the-browser-exploring-tf-js-webdnn-and-onnx-js",title:"Deep Learning in the Browser - Exploring TF.js, WebDNN and ONNX.js",description:"A quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2021-01-25_deep_learning_in_the_browser/"}},{id:"post-quick-tutorial-to-deploy-your-ml-models-using-fastapi-and-docker",title:"Quick tutorial to deploy your ML models using FastAPI and Docker",description:"Just a quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2020-11-30_fast_api_docker_ml_deploy/"}},{id:"post-androids-encryption-crypto-pwn2win-ctf-2020",title:"Androids Encryption (Crypto) - Pwn2Win CTF 2020",description:"My writeup for Androids Encryption challenge in the Pwn2Win CTF 2020",section:"Posts",handler:()=>{window.location.href="/post/2020-06-01_androids_encryption-pwn2win-2020/"}},{id:"post-malwaretech-39-s-vm1-reversing-challenge",title:"MalwareTech&#39;s VM1 Reversing Challenge",description:"My writeup for the VM1 reversing challenge posted by MalwareTech on his website.",section:"Posts",handler:()=>{window.location.href="/post/2020-01-04_malwaretech-vm1-challenge/"}},{id:"post-hxp-36c3-ctf-writeups",title:"hxp 36C3 CTF Writeups",description:"The writeups for the challenges I solved in the first MAJOR CTF that I participated in after a long time.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-30_hxp-36c3-ctf/"}},{id:"post-watevrctf-2019-writeups-mainly-rev-and-pwn",title:"watevrCTF 2019 Writeups (Mainly Rev and Pwn)",description:"My writeups for the challenges I solved in the CTF. I mainly focused on Rev and Pwn categories.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-15_watevr-ctf-2019-writeups/"}},{id:"post-tuctf-2019-pwn-amp-rev-challenges",title:"TUCTF 2019 - Pwn &amp; Rev Challenges",description:"My writeups for some of the PWN challenges of TUCTF 2019.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-02_tuctf-pwn-2019/"}},{id:"post-ritsec-ctf-2019",title:"RITSEC CTF 2019",description:"My writeups for RITSEC CTF 2019. A bit late, but I hope this helps someone!",section:"Posts",handler:()=>{window.location.href="/post/2019-11-19_ritsec-ctf-2019/"}},{id:"post-codefest-39-19-ctf-writeups",title:"Codefest&#39;19 CTF Writeups",description:"The Capture the Flag event for Codefest&#39;19 was hosted from 8 pm IST, 23rd August 2019 to 12 noon IST, 24th August 2019 on Hackerrank.",section:"Posts",handler:()=>{window.location.href="/post/2019-08-25_codefest19-ctf-writeups/"}},{id:"post-angstromctf-writeups",title:"AngstromCTF Writeups",description:"These are the writeups to the problems I solved during the AngstromCTF.",section:"Posts",handler:()=>{window.location.href="/post/2018-03-23_angstromctf-writeups/"}},{id:"post-neverlan-ctf-2018-writeups",title:"NeverLAN CTF 2018 Writeups",description:"These are the writeups of the problems I solved over the weekend for the NeverLAN CTF 2018.",section:"Posts",handler:()=>{window.location.href="/post/2018-02-27_neverlan-ctf-2018-writeups/"}},{id:"news-paper-accepted-at-the-workshop-on-multilingual-surface-realisation-acl",title:"Paper accepted at the Workshop on Multilingual Surface Realisation, ACL",description:"",section:"News",handler:()=>{window.location.href="/news/srst-acl/"}},{id:"news-won-a-student-scholarship-to-attend-blackhat-asia-2019-singapore-100-students-were-selected-from-82-countries",title:"Won a student scholarship to attend BlackHat Asia 2019, Singapore. 100 students were...",description:"",section:"News"},{id:"news-started-to-work-as-a-ta-for-the-artificial-intelligence-course-offered-to-sophomores-of-the-cse-department-of-iit-bhu-varanasi",title:"Started to work as a TA for the Artificial Intelligence course offered to...",description:"",section:"News"},{id:"news-got-my-first-job-and-started-my-career-in-the-field-of-ai-as-a-research-data-scientist-at-mastercard-ai-garage",title:"Got my first job and started my career in the field of AI...",description:"",section:"News"},{id:"news-silver-medal-shopee-price-match-guarantee-competition",title:"Silver medal - Shopee - Price Match Guarantee Competition",description:"",section:"News",handler:()=>{window.location.href="/news/silver-kaggle/"}},{id:"news-paper-accepted-at-the-30th-international-conference-on-artificial-neural-networks-icann-2021",title:"Paper accepted at the 30th International Conference on Artificial Neural Networks (ICANN 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/icann/"}},{id:"news-paper-accepted-at-the-28th-international-conference-on-neural-information-processing-iconip-2021",title:"Paper accepted at the 28th International Conference on Neural Information Processing (ICONIP 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/iconip/"}},{id:"news-joined-level-ai-as-a-machine-learning-engineer-in-nlp",title:"Joined Level AI as a Machine Learning Engineer in NLP.",description:"",section:"News"},{id:"news-promoted-to-senior-ml-engineer-at-level-ai",title:"Promoted to Senior ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-lead-ml-engineer-at-level-ai",title:"Promoted to Lead ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-principal-ml-engineer-at-level-ai",title:"Promoted to Principal ML Engineer at Level AI!",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%68%72%65%79%61%6E%73%68.%70%65%74%74%73%77%6F%6F%64@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=x8LmoJIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shreyansh26","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/shreyansh26","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/shreyansh_26","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@shreyansh26","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/shreyanshs","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>