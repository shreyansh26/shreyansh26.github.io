<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Paper Summary #13 - Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process | Shreyansh Singh </title> <meta name="author" content="Shreyansh Singh"> <meta name="description" content="My notes from the Physics of Language Models series of papers."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link defer href="/assets/css/toc-custom.css?018b6fb1db2a03192ea04ff844223cb2" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/profile_pictures/favicon.ico?7def459033b808c2a47593bed4cfb002"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shreyansh26.github.io/post/2024-09-21_physics-of-lms-2-1-grade-school-math-and-the-hidden-reasoning-process/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shreyansh</span> Singh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/post/index.html">Posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/bookshelf/">Bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume/Resume_Shreyansh.pdf">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Paper Summary #13 - Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process</h1> <p class="post-meta"> Published on September 21, 2024 by Shreyansh Singh </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> transformer</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/paper-summaries"> <i class="fa-solid fa-hashtag fa-sm"></i> paper-summaries</a>   ·   <a href="/blog/category/llms"> <i class="fa-solid fa-tag fa-sm"></i> LLMs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/featured.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2407.20311" rel="external nofollow noopener" target="_blank">Arxiv Link</a><br> <strong>Video</strong>: <a href="https://www.youtube.com/watch?v=bpp6Dz8N2zY" rel="external nofollow noopener" target="_blank">YouTube Link</a> <br> <strong>Annotated Paper</strong>: <a href="https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/LLMs/Physics%20of%20Large%20Language%20Models/Part%202.1%2C%20Grade-School%20Math%20and%20the%20Hidden%20Reasoning%20Process.pdf" rel="external nofollow noopener" target="_blank">Github repo</a></p> <hr> <h2 id="key-questions">Key Questions</h2> <ol> <li>Can language models truly develop reasoning skills, or do they simply memorize templates?</li> <li>What is the model’s hidden (mental) reasoning process?</li> <li>Do models solve math questions using skills similar to or different from humans?</li> <li>Do models trained solely on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems?</li> <li>What mental process causes models to make reasoning mistakes?</li> <li>How large or deep must a model be to effectively solve GSM8K-level math questions?</li> </ol> <h2 id="idea">Idea</h2> <ul> <li>To study reasoning and intelligence, a pre-trained model can be fine-tuned on existing math problem datasets like GSM8K.</li> <li>However, there are concerns that GSM8K or similar data may be leaked in the pre-train data for LLMs.</li> <li>Hence when answering mathematical questions, it is hard to tell whether these models are actually performing reasoning (i.e. questions 1-3 above) or has just memorized problem templates during training.</li> <li>Using existing math datasets like GSM8K for pre-training is insufficient due to the small size of these datasets.</li> <li>Also, the idea of using GPT4 to augment similar problems like GSM8K wasn’t chosen because of the potential bias of the augmented data towards a small number of solution templates.</li> <li>Hence the authors created their own pre-training dataset of math problems, with a much larger and diverse set of grade-school math problems to test a GPT2-like language model.</li> </ul> <h2 id="dataset-generation">Dataset Generation</h2> <ul> <li>This section in the paper is super cool but also quiet dense.</li> <li>It is a great source of learning on how to create synthetic datasets. Won’t go into much detail in the notes, the paper is the best source to understand the complexity.</li> <li>A standard grade-school math problem in GSM8K looks like this - <ul> <li> <blockquote> <p>Betty is saving money for a new wallet which costs 100. Betty has only half of the money she needs. Her parents decided to give her 15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?</p> </blockquote> </li> </ul> </li> <li>This problem has multiple parameters whose values are connected with various equalities, such as “Betty’s current money = 0.5 × cost of the wallet” and “money given by grandparents = 2 × money given by parents.”</li> <li>Motivated by this, the authors build a GSM8K-like math dataset through a synthetic generation pipeline that captures the dependencies of parameters.</li> <li>The main focus was on the “logical reasoning” aspect of the problems which involves understanding the dependency of parameters in the problem statement, such as direct, instance, and implicit dependencies.</li> <li> <strong>Dependency Types</strong>: <ul> <li> <strong>Direct Dependency</strong>: Parameters directly depend on others (e.g., \(A = 5 \times (X + Y)\)).</li> <li> <strong>Instance Dependency</strong>: Parameters depend on instances of categories (e.g., total number of chairs = chairs per classroom × number of classrooms).</li> <li> <strong>Implicit Dependency</strong>: Parameters involve abstract concepts that need to be inferred (e.g., identifying fruits from a list of items).</li> </ul> </li> <li> <strong>Problem Structure</strong>: Problems are generated using a hierarchical categorization of items, where each category can include multiple layers and items. This structure adds complexity by requiring models to learn concepts implicitly.</li> <li> <strong>Dependency Graph</strong>: A directed acyclic graph (DAG) is used to represent dependencies among parameters, including both direct and implicit dependencies. The graph guides the generation of math problems by linking parameters in specific ways.</li> <li> <strong>Problem Generation</strong>: The math problems are articulated by translating the dependency graphs into English sentences. The order of these sentences is randomized to increase the difficulty, and a question is posed to test the model’s understanding.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/structure_graph.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>Additional care was taken to reduce the difficulty of the problem statements <ul> <li>Ensuring clarity in expression, to reduce the difficulty arising from common-sense</li> <li>For arithmetic, all integers and arithmetic were mod23</li> </ul> </li> <li>A sample problem (corresponding to the figure above) looks like this - <ul> <li> <blockquote> <p>The number of each Riverview High’s Film Studio equals 5 times as much as the sum of each Film Studio’s Backpack and each Dance Studio’s School Daypack. The number of each Film Studio’s School Daypack equals 12 more than the sum of each Film Studio’s Messenger Backpack and each Central High’s Film Studio. The number of each Central High’s Film Studio equals the sum of each Dance Studio’s School Daypack and each Film Studio’s Messenger Backpack. The number of each Riverview High’s Dance Studio equals the sum of each Film Studio’s Backpack, each Film Studio’s Messenger Backpack, each Film Studio’s School Daypack and each Central High’s Backpack. The number of each Dance Studio’s School Daypack equals 17. The number of each Film Studio’s Messenger Backpack equals 13. How many Backpack does Central High have?</p> </blockquote> </li> </ul> </li> </ul> <h3 id="solution-construction">Solution Construction</h3> <ul> <li>The solution generation is done in the Chain of Thought reasoning format.</li> <li>The methodology is given below - <ul> <li>Let <strong>solution</strong> be a sequence of sentences describing the <strong>necessary</strong> steps towards solving the given problem, where the sentences follow any topological order - known as CoT</li> <li>For each parameter <strong>necessary</strong> towards answering the final question, it is assigned a random letter among the 52 choices (a..z or A..Z), and a sentence is used to describe its computation.</li> <li>Sample solution corresponding to the above problem -</li> <li> <blockquote> <p>Define Dance Studio’s School Daypack as p; so p = 17. Define Film Studio’s Messenger Backpack as W; so W = 13. Define Central High’s Film Studio as B; so B = p + W = 17 + 13 = 7. Define Film Studio’s School Daypack as g; R = W + B = 13 + 7 = 20; so g = 12 + R = 12 + 20 = 9. Define Film Studio’s Backpack as w; so w = g + W = 9 + 13 = 22. Define Central High’s Backpack as c; so c = B * w = 7 * 22 = 16. <i>Answer: 16</i>.</p> </blockquote> </li> </ul> </li> <li>Important points during solution construction - <ul> <li>The solution only contain parameters necessary towards calculating the final query parameter</li> <li>The solution follows the correct logical order: i.e. all the parameters used in the calculation must have appeared and been computed beforehand.</li> <li>Computations are broken into binary ops: \(g = 12 + 13 + 7\) is broken into g = 12+R and R = 13+7 in the above solution. The number of semicolons “;” equals the number of operations. This reduces the arithmetic complexity of the solution.</li> </ul> </li> </ul> <h3 id="difficulty-control">Difficulty Control</h3> <ul> <li>Two parameters are used to control the difficulty of the problem - <ul> <li>\(\textrm{ip}\) is the number of instance parameters</li> <li>\(\textrm{op}\) is the number of solution operations</li> </ul> </li> <li>The data difficulty is an increasing function over them</li> <li>The authors call the dataset iGSM</li> <li>We use \(\textrm{iGSM}^{\textrm{op} \leq op,\textrm{ip} \leq ip}\) to denote the data generated with constraint \(\textrm{op} \leq op\) and \(\textrm{ip} \leq ip\), and use \(\textrm{iGSM}^{\textrm{op}=op,\textrm{ip} \leq ip}\) to denote those restricting to \(\textrm{op}=op\).</li> </ul> <h3 id="train-and-test-datasets">Train and Test Datasets</h3> <ul> <li>\(\textrm{iGSM-med}\) uses \(\textrm{ip} \leq 20\) <ul> <li>Train data is essentially \(\textrm{iGSM}^{\textrm{op} \leq 15,\textrm{ip} \leq 20}\) (referred as \(\textrm{iGSM-med}^{\textrm{op} \leq 15}\))</li> <li>Evaluation is done on both \(\textrm{iGSM-med}^{\textrm{op} \leq 15}\) and out-of-distribution (OOD) on \(\textrm{iGSM-med}^{\textrm{op} \leq op}\) where \(op \in \{20, 21, 23, 23\}\)</li> <li>Another set of evaluation is also done on \(\textrm{iGSM-med}^{\textrm{op} = op,\textrm{reask}}\). Here \(\textrm{reask}\) denotes first generating a problem from \(\textrm{iGSM-med}^{\textrm{op} = op}\) and then resampling a query parameter.</li> <li>Due to the topological nature of the data/solution generation process, \(\textrm{reask}\) greatly changes the data distribution and the number of operations needed. It provides an excellent OOD sample for evaluation.</li> </ul> </li> <li>\(\textrm{iGSM-hard}\) uses \(\textrm{ip} \leq 28\) <ul> <li>Train data is essentially \(\textrm{iGSM}^{\textrm{op} \leq 21,\textrm{ip} \leq 28}\) (referred as \(\textrm{iGSM-hard}^{\textrm{op} \leq 21}\))</li> <li>Evaluation is done on both \(\textrm{iGSM-hard}^{\textrm{op} \leq 21}\) and out-of-distribution (OOD) on \(\textrm{iGSM-hard}^{\textrm{op} \leq op}\) where \(op \in \{28, 29, 30, 31, 32\}\)</li> <li>Another set of evaluation is also done on \(\textrm{iGSM-hard}^{\textrm{op} = op,\textrm{reask}}\).</li> </ul> </li> <li>Key points about the dataset <ul> <li>Ignoring unused parameters, numerics, sentence orderings, English words, a-z and A-Z letter choices, \(\textrm{iGSM}^{\textrm{op} = 15}\) still has at least 7 billion solution templates, and \(\textrm{iGSM-hard}^{\textrm{op} = 21}\) has at least 90 trillion solution templates.</li> <li>The OOD evaluation is guaranteed to not have data contamination as training is done only on \(\textrm{op} \leq 21\) but evaluation is done on \(\textrm{op} \geq 28\)</li> <li>Training is done on data whose hash value of solution template is \(&lt;17\) (mod 23), and but tested with those \(\geq 17\). This ensures no template-level overlap between training and testing.</li> </ul> </li> </ul> <h2 id="model">Model</h2> <ul> <li>A GPT2 model but with its positional embedding replaced with rotary embeddings (RoPE)</li> <li>It is still called GPT2 in the paper</li> <li>The authors mostly stick to the 12-layer, 12-head, 768-dim GPT2 (a.k.a. GPT2-small, 124M) for experiments but also explore larger models for some experiments</li> <li>The context length is 768 / 1024 for pretraining on \(\textrm{iGSM-med}\) / \(\textrm{iGSM-hard}\) and 2048 for evaluation.</li> </ul> <h2 id="key-result---models-behavior-process">Key Result - Model’s Behavior process</h2> <ul> <li> <strong>TL;DR</strong> - <ul> <li>The authors demonstrate that the GPT2 model, pretrained on the synthetic dataset, not only achieves 99% accuracy in solving math problems from the same distribution but also out-of-distribution generalizes, such as to those of longer reasoning lengths than any seen during training.</li> <li>Note that the model has never seen any training example of the same length as in test time.</li> <li>This signifies that the model can truly learn some reasoning skill instead of memorizing solution templates.</li> <li>Crucially, the model can learn to generate shortest solutions, almost always avoiding unnecessary computations.</li> <li>This suggests that the model formulates a plan before it generates, in order to avoid computing any quantities that are not needed towards solving the underlying math problem.</li> </ul> </li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/test_acc_3.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_4.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="key-result---models-mental-process">Key Result - Model’s Mental Process</h2> <ul> <li> <strong>TL;DR</strong> <ul> <li>The authors examine the model’s internal states through probing, introducing six probing tasks to elucidate how the model solves math problems.</li> <li>For instance, they discover that the model (mentally!) preprocesses the full set of necessary parameters before it starts any generation. Likewise, humans also do this preprocess although we write this down on scratch pads.</li> <li>The model also learns unnecessary, yet important skills after pretraining, such as all-pair dependency.</li> <li>Before any question is asked, it already (mentally!) computes with good accuracy which parameters depend on which, even though some are not needed for solving the math problem.</li> <li>Note that computing all-pair dependency is a skill not needed to fit all the solutions in the training data. This is the first evidence that a language model can learn useful skills beyond those necessary to fit its pretraining data.</li> </ul> </li> <li>The authors use linear probing to study the following tasks which align with human problem-solving strategies - <ul> <li>\(\textrm{nece(A)}\): If the parameter \(A\) is necessary for computing the answer.</li> <li>\(\textrm{dep(A, B)}\): if parameter \(A\) (recursively) depends on parameter B given the problem statement.</li> <li>\(\textrm{known(A)}\): if parameter \(A\) has already been computed.</li> <li>\(\textrm{value(A)}\): the value of parameter \(A\) (a number between 0-22, or 23 if \(\textrm{known(A) = false}\)).</li> <li>\(\textrm{can_next(A)}\): if \(A\) can be computed in the next solution sentence (namely, its predecessors have all been calculated). Note that \(A\) might not be necessary to answer the question.</li> <li>\(\textrm{nece_next(A)}\): if parameter A satisfies both \(\textrm{can_next(A)}\) and \(\textrm{nece(A)}\).</li> </ul> </li> <li>For a model to generate the shortest solutions, it must identify \(\textrm{nece(A)}\) for all \(A\)’s in its mental process.</li> <li>Whether \(\textrm{nece(A)}\) is \(\textrm{true}\), directly corresponds to whether there is a solution sentence to compute \(A\).</li> <li>Other similar probing tasks and what they imply are shown in the figure below -</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_5.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="variable-probing-a--nearly-linear-probing-method">V(ariable)-Probing: A Nearly-Linear Probing Method</h3> <ul> <li>As illustrated in the figure above, the authors conduct probing <ul> <li>At the end of the problem description for the \(\textrm{dep}\) task</li> <li>At the end of the question description for the \(\textrm{nece}\) task</li> <li>For all other tasks, they are probed at the end of every solution sentence</li> </ul> </li> <li>In Linear Probing a trainable linear classifier is introduced over the hidden states and a lightweight finetuning is performed for the task.</li> <li>V-Probing however has certain differences.</li> <li> <strong>Motivation</strong>: V-Probing is introduced to handle more complex properties in math problems that involve one or two conditional variables (A and B) described in plain English.</li> <li> <strong>Probing Setup</strong>: The math problems are truncated to the probing position, and special tokens <code class="language-plaintext highlighter-rouge">[START]</code> and <code class="language-plaintext highlighter-rouge">[END]</code> are placed around the descriptions of A (or A, B). The probing is done from the <code class="language-plaintext highlighter-rouge">[END]</code> token position to check if the property is linearly encoded in the model’s last layer.</li> <li> <strong>Enhancement Over Linear Probing</strong>: Unlike standard linear probing, V-Probing introduces a small, trainable rank-8 linear update to the input embedding layer to account for input changes.</li> <li> <strong>Training Process</strong>: The pretrained language model is frozen, and both the linear classifier and the rank-8 update are fine-tuned to probe for the desired property, making the process more adaptable to complex input structures.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_13.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="probing-results">Probing Results</h3> <ul> <li>The probing accuracies are high for all the tasks, compared to majority guess and random-model probing - except for the very hard OOD cases (i.e., for large op where the model’s generation accuracies fall down to 80% anyways.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_7.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li> <strong>Model solves math problems like humans</strong> <ul> <li>When generating solutions, the model not only remembers which parameters have been computed and which have not (\(\textrm{value, known}\)) but <strong>also knows which parameters can be computed next</strong> (\(\textrm{can_next, nece_next}\)). These abilities <strong>ensure that the model can solve the given math problem step by step</strong>, similar to human problem-solving skill.</li> <li>By the end of the problem description, the model already knows the full list of necessary parameters (\(\textrm{nece}\)). This indicates that the model has learned to plan ahead, identifying necessary parameters before starting to generate the solution. This aligns with human behavior, except that the model plans mentally while humans typically write this down.</li> </ul> </li> <li> <strong>Model learns beyond human reasoning skills</strong> <ul> <li>The model learns \(\textrm{dep(A, B)}\) and \(\textrm{can_next(A)}\), even for parameters A not necessary for answering the question, as shown in the figure above.</li> <li>This differs from human problem-solving, where we typically use backward reasoning from the question to identify necessary parameters, often overlooking unnecessary ones.</li> <li>In contrast, language models can pre-compute the all-pair dependency graph \(\textrm{dep(A, B)}\) mentally even before a question is asked. This “level-2” reasoning is very different from human behavior or mental processes.</li> <li>This enables the model to sort relationships among the things it hears, a skill that can be useful for future tasks (via instruction fine-tuning).</li> <li>This may be the first evidence of a language model acquiring skills beyond those needed for learning its pretrain data.</li> </ul> </li> </ul> <h2 id="key-results---explain-models-mistakes">Key Results - Explain Model’s Mistakes</h2> <ul> <li>The authors tried to answer the following questions - <ul> <li>When does the model answer correctly but include unnecessary parameters?</li> <li>What causes incorrect answers?</li> </ul> </li> <li>The aim is to determine if such erroneous behaviour of the model aligns with errors in the model’s mental process (via probing)</li> <li>Earlier, it was shown that the model rarely produces solutions longer than necessary, so the authors looked at the OOD \(\textrm{reask}\) data for evaluation.</li> <li>On this data, pretrained models produce an average of ~0.5 unnecessary parameters per solution even for \(\textrm{op} = 32\) (figure 4).</li> <li>The authors examined if these unnecessary parameters \(A\) were incorrectly predicted as \(\textrm{nece = true}\) in the probing task.</li> <li>The following figure reveals that this is often indeed the case, thus language models produce solutions with unnecessary steps due to errors in their mental planning phase.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_8.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>In the second part of the figure, the author’s findings show that the model’s errors mainly stem from incorrectly predicting \(\textrm{nece_next(A)}\) or \(\textrm{can_next(A)}\) as true in its internal states when such \(A\)’s are not ready for computation.</li> <li>Essentially, <ul> <li>Many reasoning mistakes made by the language model are systematic, stemming from errors in its mental process, not merely random from the generation process.</li> <li>Some of the model’s mistakes can be discovered by probing its inner states even before the model opens its mouth (i.e., before it says the first solution step).</li> </ul> </li> </ul> <h2 id="key-results---depth-vs-reasoning-length">Key Results - Depth vs. Reasoning Length</h2> <ul> <li>The authors find that - <strong>Language model depth is crucial for mathematical reasoning.</strong> </li> <li>They experimented with model of various depths and various hidden sizes.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_9.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>From the figure above, it can be seen that a 4-layer transformer, even with 1920 hidden dimensions, underperforms on the math datasets in the paper.</li> <li>Conversely, deeper but smaller models, such as a 20-layer 576-dim, perform very well.</li> <li>There is a clear correlation between the model depth and performance.</li> </ul> <h3 id="but-how-does-model-depth-influence-math-problem-solving-skills">But how does model depth influence math problem solving skills?</h3> <ul> <li>Using the \(\textrm{nece}\) probing task, the authors focus on the necessary parameters at distance \(t\) from the query parameter, for \(t \in \{1,2,...,8\}\).</li> <li>Though these parameters all have \(\textrm{nece = true}\), but the model can be probed to see how correct they are at predicting \(\textrm{nece(A)}\) at different hidden layers.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_10.png" alt=""> <figcaption></figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>The above figure shows a correlation between the model’s layer hierarchy, reasoning accuracy and mental reasoning depth.</li> <li>Shallower layers excel at predicting nece(A) for parameters \(A\) closer to the query, whereas deeper layers are more accurate and can predict \(\textrm{nece(A)}\) for parameters further from the query.</li> <li>This suggests that the **model employs layer-by-layer reasoning during the planning phase to recursively identify all parameters the query depends on. **</li> <li>So, for larger \(t\), the model may require and benefit from deeper models (assuming all other hyperparameters remain constant).</li> <li>To note - <ul> <li>If the “backward thinking process” is added as CoT to the data, then deep mental thinking is no longer required, reducing the language model’s depth requirement. However, in practice, many such “thinking processes” may not be included in standard math solutions or languages in general.</li> <li>The above claim does not imply that “a \(t\)-step mental thinking requires a depth-\(t\) transformer”. It is plausible for a single transformer layer (containing many sub-layers) to implement \(t &gt; 1\) mental thinking steps, though possibly with reduced accuracy as \(t\) increases. There is no exact correlation as it depends on the data distribution.</li> </ul> </li> </ul> <hr> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26" rel="external nofollow noopener" target="_blank">Twitter</a>, <a href="https://github.com/shreyansh26" rel="external nofollow noopener" target="_blank">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/" rel="external nofollow noopener" target="_blank">LinkedIn</a>.</p> </div> </article> <div id="giscus_thread" style="max-width: 1200px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shreyansh26/shreyansh26.github.io","data-repo-id":"R_kgDOMn767Q","data-category":"General","data-category-id":"DIC_kwDOMn767c4CiLUv","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Shreyansh Singh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?317646887210841a1b23a5eedc9fba39"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZEZ7673Y7G"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZEZ7673Y7G");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-posts",title:"Posts",description:"",section:"Navigation",handler:()=>{window.location.href="/post/index.html"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-bookshelf",title:"Bookshelf",description:"A list of books I&#39;ve read and some of which I&#39;ve found interesting :)",section:"Navigation",handler:()=>{window.location.href="/bookshelf/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/assets/pdf/resume/Resume_Shreyansh.pdf"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-compute-and-instruction-throughput",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Compute and Instruction Throughput",description:"My notes from the talk on maximizing compute and instruction throughput at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-04-04_gtc25-maximize-compute-instruction-throughput/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-2",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"Second part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-2/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-1",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"First part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/"}},{id:"post-faster-cross-encoder-inference-unleashing-torch-compile-for-speed",title:"Faster Cross-Encoder Inference: Unleashing torch.compile for speed",description:"A quick writeup on accelerating a Jina Cross-Encoder using torch.compile",section:"Posts",handler:()=>{window.location.href="/post/2025-03-02_cross-encoder-inference-torch-compile/"}},{id:"post-paper-summary-13-physics-of-language-models-part-2-1-grade-school-math-and-the-hidden-reasoning-process",title:"Paper Summary #13 - Physics of Language Models: Part 2.1, Grade-School Math and...",description:"My notes from the Physics of Language Models series of papers.",section:"Posts",handler:()=>{window.location.href="/post/2024-09-21_physics-of-lms-2-1-grade-school-math-and-the-hidden-reasoning-process/"}},{id:"post-paper-summary-12-image-recaptioning-in-dall-e-3",title:"Paper Summary #12 - Image Recaptioning in DALL-E 3",description:"The image recaptioning technique used in DALL-E 3 was extended to videos in Sora.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_dalle3_image_recaptioner/"}},{id:"post-paper-summary-11-sora",title:"Paper Summary #11 - Sora",description:"OpenAI announced a ground-breaking text-to-video diffusion model capable of generating high-definition videos up to 60 seconds long.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_sora_openai/"}},{id:"post-paper-summary-10-gemini-1-5-pro",title:"Paper Summary #10 - Gemini 1.5 Pro",description:"Google DeepMind announced a multimodal LLM with support of up to 10M context length.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_gemini_pro_google/"}},{id:"post-solving-substitution-ciphers-using-markov-chain-monte-carlo-mcmc",title:"Solving Substitution Ciphers using Markov Chain Monte Carlo (MCMC)",description:"Deciphering substitution ciphers can be framed as a Markov chain problem and a simple Monte Carlo sampling approach can help solve them very efficiently",section:"Posts",handler:()=>{window.location.href="/post/2023-07-22_solving_substitution_cipher_using_mcmc/"}},{id:"post-paper-summary-9-sophia-a-scalable-stochastic-second-order-optimizer-for-language-model-pre-training",title:"Paper Summary #9 - Sophia: A Scalable Stochastic Second-order Optimizer for Language Model...",description:"Understanding Sophia - A new fast, scalable second-order optimizer which beats Adam on LLM pretraining.",section:"Posts",handler:()=>{window.location.href="/post/2023-05-28_sophia_scalable_second_order_optimizer_llms/"}},{id:"post-paper-summary-8-flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness",title:"Paper Summary #8 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",description:"Understanding FlashAttention which is the most efficient exact attention implementation out there, which optimizes for both memory requirements and wall-clock time.",section:"Posts",handler:()=>{window.location.href="/post/2023-03-26_flash-attention/"}},{id:"post-paper-summary-7-efficient-transformers-a-survey",title:"Paper Summary #7 - Efficient Transformers: A Survey",description:"A survey paper of improvements over the original Transformer architecture in terms of memory-efficiency.",section:"Posts",handler:()=>{window.location.href="/post/2022-10-10_efficient_transformers_survey/"}},{id:"post-deploying-machine-learning-models-using-gcp-39-s-google-ai-platform-a-detailed-tutorial",title:"Deploying Machine Learning models using GCP&#39;s Google AI Platform - A Detailed Tutorial...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using GCP, specifically the Google AI Platform and use Streamlit to access the model through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-03-06_model_deployment_using_gcp_google_ai_platform/"}},{id:"post-deploying-machine-learning-models-using-aws-lambda-and-github-actions-a-detailed-tutorial",title:"Deploying Machine Learning models using AWS Lambda and Github Actions - A Detailed...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using AWS Lambda, Github Actions, API Gateway and use Streamlit to access the model API through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-01-23_model_deployment_using_aws_lambda/"}},{id:"post-ppml-series-3-federated-learning-for-mobile-keyboard-prediction",title:"PPML Series #3 - Federated Learning for Mobile Keyboard Prediction",description:"Understanding how your mobile keyboard (Gboard, specifically) performs the next word prediction task and performs model training and updates",section:"Posts",handler:()=>{window.location.href="/post/2021-12-27_federated_learning_mobile_keyboard/"}},{id:"post-ppml-series-2-federated-optimization-algorithms-fedsgd-and-fedavg",title:"PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg",description:"A mathematical deep dive on a Federated Optimization algorithm - FedAvg and comparing it with a standard approach - FedSGD.",section:"Posts",handler:()=>{window.location.href="/post/2021-12-18_federated_optimization_fedavg/"}},{id:"post-ppml-series-1-an-introduction-to-federated-learning",title:"PPML Series #1 - An introduction to Federated Learning",description:"A short general introduction to Federated Learning (FL) for folks interested in privacy-preserving machine learning (PPML).",section:"Posts",handler:()=>{window.location.href="/post/2021-12-11_intro_to_federated_learning/"}},{id:"post-paper-summary-6-language-models-are-unsupervised-multitask-learners",title:"Paper Summary #6 - Language Models are Unsupervised Multitask Learners",description:"The GPT2 model which aimed to perform complex NLP tasks while relying only on a language model trained in a completely unsupervised fashion.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/"}},{id:"post-paper-summary-5-xlnet-generalized-autoregressive-pretraining-for-language-understanding",title:"Paper Summary #5 - XLNet: Generalized Autoregressive Pretraining for Language Understanding",description:"XLNet tries to overcome the limitations of BERT by having a autoregressive component while also capturing the bidirectional context.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/"}},{id:"post-paper-summary-4-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding",title:"Paper Summary #4 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding...",description:"The ground breaking paper that introduced the famous BERT model. This started the inflow of a large number of BERT-based language understanding models.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/"}},{id:"post-paper-summary-3-improving-language-understanding-by-generative-pre-training",title:"Paper Summary #3 - Improving Language Understanding by Generative Pre-Training",description:"The first paper in the GPT set of models. This is OpenAI&#39;s GPT-1.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-02_language_understanding_generative_pretraining/"}},{id:"post-paper-summary-2-deep-contextualized-word-representations-elmo",title:"Paper Summary #2 - Deep contextualized word representations (ELMo)",description:"The second post in the paper notes series. This time we take a look at ELMo.",section:"Posts",handler:()=>{window.location.href="/post/2021-04-25_deep_contextualized_word_representations_elmo/"}},{id:"post-paper-summary-1-attention-is-all-you-need",title:"Paper Summary #1 - Attention Is All You Need",description:"The first of the paper summary series. This is where I briefly summarise the important papers that I read for my job or just for fun :P",section:"Posts",handler:()=>{window.location.href="/post/2021-04-18_attention_is_all_you_need/"}},{id:"post-deep-learning-in-the-browser-exploring-tf-js-webdnn-and-onnx-js",title:"Deep Learning in the Browser - Exploring TF.js, WebDNN and ONNX.js",description:"A quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2021-01-25_deep_learning_in_the_browser/"}},{id:"post-quick-tutorial-to-deploy-your-ml-models-using-fastapi-and-docker",title:"Quick tutorial to deploy your ML models using FastAPI and Docker",description:"Just a quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2020-11-30_fast_api_docker_ml_deploy/"}},{id:"post-androids-encryption-crypto-pwn2win-ctf-2020",title:"Androids Encryption (Crypto) - Pwn2Win CTF 2020",description:"My writeup for Androids Encryption challenge in the Pwn2Win CTF 2020",section:"Posts",handler:()=>{window.location.href="/post/2020-06-01_androids_encryption-pwn2win-2020/"}},{id:"post-malwaretech-39-s-vm1-reversing-challenge",title:"MalwareTech&#39;s VM1 Reversing Challenge",description:"My writeup for the VM1 reversing challenge posted by MalwareTech on his website.",section:"Posts",handler:()=>{window.location.href="/post/2020-01-04_malwaretech-vm1-challenge/"}},{id:"post-hxp-36c3-ctf-writeups",title:"hxp 36C3 CTF Writeups",description:"The writeups for the challenges I solved in the first MAJOR CTF that I participated in after a long time.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-30_hxp-36c3-ctf/"}},{id:"post-watevrctf-2019-writeups-mainly-rev-and-pwn",title:"watevrCTF 2019 Writeups (Mainly Rev and Pwn)",description:"My writeups for the challenges I solved in the CTF. I mainly focused on Rev and Pwn categories.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-15_watevr-ctf-2019-writeups/"}},{id:"post-tuctf-2019-pwn-amp-rev-challenges",title:"TUCTF 2019 - Pwn &amp; Rev Challenges",description:"My writeups for some of the PWN challenges of TUCTF 2019.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-02_tuctf-pwn-2019/"}},{id:"post-ritsec-ctf-2019",title:"RITSEC CTF 2019",description:"My writeups for RITSEC CTF 2019. A bit late, but I hope this helps someone!",section:"Posts",handler:()=>{window.location.href="/post/2019-11-19_ritsec-ctf-2019/"}},{id:"post-codefest-39-19-ctf-writeups",title:"Codefest&#39;19 CTF Writeups",description:"The Capture the Flag event for Codefest&#39;19 was hosted from 8 pm IST, 23rd August 2019 to 12 noon IST, 24th August 2019 on Hackerrank.",section:"Posts",handler:()=>{window.location.href="/post/2019-08-25_codefest19-ctf-writeups/"}},{id:"post-angstromctf-writeups",title:"AngstromCTF Writeups",description:"These are the writeups to the problems I solved during the AngstromCTF.",section:"Posts",handler:()=>{window.location.href="/post/2018-03-23_angstromctf-writeups/"}},{id:"post-neverlan-ctf-2018-writeups",title:"NeverLAN CTF 2018 Writeups",description:"These are the writeups of the problems I solved over the weekend for the NeverLAN CTF 2018.",section:"Posts",handler:()=>{window.location.href="/post/2018-02-27_neverlan-ctf-2018-writeups/"}},{id:"news-paper-accepted-at-the-workshop-on-multilingual-surface-realisation-acl",title:"Paper accepted at the Workshop on Multilingual Surface Realisation, ACL",description:"",section:"News",handler:()=>{window.location.href="/news/srst-acl/"}},{id:"news-won-a-student-scholarship-to-attend-blackhat-asia-2019-singapore-100-students-were-selected-from-82-countries",title:"Won a student scholarship to attend BlackHat Asia 2019, Singapore. 100 students were...",description:"",section:"News"},{id:"news-started-to-work-as-a-ta-for-the-artificial-intelligence-course-offered-to-sophomores-of-the-cse-department-of-iit-bhu-varanasi",title:"Started to work as a TA for the Artificial Intelligence course offered to...",description:"",section:"News"},{id:"news-got-my-first-job-and-started-my-career-in-the-field-of-ai-as-a-research-data-scientist-at-mastercard-ai-garage",title:"Got my first job and started my career in the field of AI...",description:"",section:"News"},{id:"news-silver-medal-shopee-price-match-guarantee-competition",title:"Silver medal - Shopee - Price Match Guarantee Competition",description:"",section:"News",handler:()=>{window.location.href="/news/silver-kaggle/"}},{id:"news-paper-accepted-at-the-30th-international-conference-on-artificial-neural-networks-icann-2021",title:"Paper accepted at the 30th International Conference on Artificial Neural Networks (ICANN 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/icann/"}},{id:"news-paper-accepted-at-the-28th-international-conference-on-neural-information-processing-iconip-2021",title:"Paper accepted at the 28th International Conference on Neural Information Processing (ICONIP 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/iconip/"}},{id:"news-joined-level-ai-as-a-machine-learning-engineer-in-nlp",title:"Joined Level AI as a Machine Learning Engineer in NLP.",description:"",section:"News"},{id:"news-promoted-to-senior-ml-engineer-at-level-ai",title:"Promoted to Senior ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-lead-ml-engineer-at-level-ai",title:"Promoted to Lead ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-principal-ml-engineer-at-level-ai",title:"Promoted to Principal ML Engineer at Level AI!",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%68%72%65%79%61%6E%73%68.%70%65%74%74%73%77%6F%6F%64@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=x8LmoJIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shreyansh26","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/shreyansh26","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/shreyansh_26","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@shreyansh26","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/shreyanshs","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>