<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding Multi-Head Latent Attention (MLA) | Shreyansh Singh </title> <meta name="author" content="Shreyansh Singh"> <meta name="description" content="A mathematical and code deep-dive on one of the key innovations from Deepseek - Multihead Latent Attention (MLA)"> <meta property="og:site_name" content="Shreyansh Singh"> <meta property="og:type" content="article"> <meta property="og:title" content="Shreyansh Singh | Understanding Multi-Head Latent Attention (MLA)"> <meta property="og:url" content="https://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/"> <meta property="og:description" content="A mathematical and code deep-dive on one of the key innovations from Deepseek - Multihead Latent Attention (MLA)"> <meta property="og:image" content="https://shreyansh26.github.io/assets/img/posts_images/mla/mla_cover.png"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary_large_image"> <meta name="twitter:title" content="Understanding Multi-Head Latent Attention (MLA)"> <meta name="twitter:description" content="A mathematical and code deep-dive on one of the key innovations from Deepseek - Multihead Latent Attention (MLA)"> <meta name="twitter:image" content="https://shreyansh26.github.io/assets/img/posts_images/mla/mla_cover.png"> <meta name="twitter:site" content="@shreyansh_26"> <meta name="twitter:creator" content="@shreyansh_26"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css2?family=Spectral:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link defer href="/assets/css/toc-custom.css?018b6fb1db2a03192ea04ff844223cb2" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/profile_pictures/favicon.ico?7def459033b808c2a47593bed4cfb002"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shreyansh</span> Singh </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/post/index.html">Posts </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/bookshelf/">Bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/resume/Resume_Shreyansh.pdf">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row toc-layout"> <div class="col-sm-3 toc-sidebar"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9 toc-content"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding Multi-Head Latent Attention (MLA)</h1> <p class="post-meta"> Published on November 08, 2025 by Shreyansh Singh </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/mla"> <i class="fa-solid fa-hashtag fa-sm"></i> mla</a>   ·   <a href="/blog/category/llms"> <i class="fa-solid fa-tag fa-sm"></i> LLMs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/mla/mla_cover.png" alt="Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. Source - https://arxiv.org/abs/2405.04434."> <figcaption>Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. Source - https://arxiv.org/abs/2405.04434.</figcaption> <br> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr> <p><strong>Code</strong> - <a href="https://github.com/shreyansh26/multihead-latent-attention" rel="external nofollow noopener" target="_blank">https://github.com/shreyansh26/multihead-latent-attention</a></p> <p>Deepseek introduced Multi-Head Latent Attention (MLA) in the <a href="https://arxiv.org/abs/2405.04434" rel="external nofollow noopener" target="_blank">Deepseek-v2 paper</a> as a way to improve the efficiency of attention computation during inference by reducing the KV cache bottleneck. MLA achieves better performance than Multi-Head Attention (MHA).</p> <p>Grouped-Query Attention (GQA) and Multi-Query Attention (MQA) reduce Key/Value (KV) duplication, shrinking the KV cache and cutting bandwidth. Multi-Head Latent Attention (MLA) goes further: it introduces a low-rank latent space that factorizes attention, enabling both efficient training and extremely efficient inference with a simple algebraic “absorption” trick.</p> <p>This post walks from MHA → GQA → MQA → MLA, then shows the fusion and absorption optimizations, with concrete PyTorch code and equations you can render in Markdown.</p> <h2 id="revisiting-multi-head-attention-mha">Revisiting Multi-Head Attention (MHA)</h2> <p>MHA projects input tokens into per-head Query/Key/Value, computes attention per head, then merges:</p> <p>Given hidden size (D), number of heads (H), and head dimension (d) where (D = H \cdot d):</p> <ul> <li>Queries: \(Q \in \mathbb{R}^{B \times S \times H \times d}\)</li> <li>Keys: \(K \in \mathbb{R}^{B \times S \times H \times d}\)</li> <li>Values: \(V \in \mathbb{R}^{B \times S \times H \times d}\)</li> <li>Attention per head: \(\mathrm{Attn}(Q_i, K_i, V_i) = \mathrm{Softmax}\!\left(\frac{Q_i K_i^\top}{\sqrt{d}}\right) V_i\)</li> </ul> <p>Code reference (simplified from our <a href="https://github.com/shreyansh26/multihead-latent-attention/blob/main/mha.py" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">mha.py</code></a>):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_bsd</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">kv_cache</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">return_torch_ref</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x_bsd</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">q_bsqh</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_proj</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="n">k_blkh</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_proj</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="n">v_blkh</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_proj</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="n">q_bsqh</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">q_bsqh</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis</span><span class="p">)</span>
    <span class="n">k_blkh</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">k_blkh</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis</span><span class="p">)</span>
    <span class="n">q_bqsh</span> <span class="o">=</span> <span class="n">q_bsqh</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">k_bklh</span> <span class="o">=</span> <span class="n">k_blkh</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">v_bklh</span> <span class="o">=</span> <span class="n">v_blkh</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">out_bsd</span> <span class="o">=</span> <span class="nf">naive_attention</span><span class="p">(</span><span class="n">q_bqsh</span><span class="p">,</span> <span class="n">k_bklh</span><span class="p">,</span> <span class="n">v_bklh</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span>
    <span class="n">out_bsd</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">o_proj</span><span class="p">(</span><span class="n">out_bsd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out_bsd</span>
</code></pre></div></div> <p>Inefficiency: we compute and store (K,V) per head. For long sequences, the KV cache dominates memory and communication.</p> <h2 id="gqa-grouped-query-attention">GQA: Grouped-Query Attention</h2> <p>GQA shares Keys/Values across groups of query heads: \(H\) query heads share \(H_\text{kv}\) KV heads (with \(H_\text{kv} &lt; H\)). Complexity and KV cache both drop by a factor of \(H / H_\text{kv}\) compared to MHA, while preserving multiple query heads for expressivity.</p> <p>Trade-off: less KV diversity per query head; often negligible loss in modeling capacity with slight improvement in inference efficiency.</p> <h2 id="mqa-multi-query-attention">MQA: Multi-Query Attention</h2> <p>MQA goes to the limit: one shared KV head for all queries \(H_\text{kv}=1\). KV cache drops by \(\approx H\times\) versus MHA; cross-device communication shrinks markedly. For long-context inference, this is a big win.</p> <p>Downside: a single KV head may reduce modeling capacity if used naïvely. MLA addresses this by introducing a low-rank latent structure that preserves expressivity while keeping runtime costs low.</p> <h2 id="mla-multi-head-latent-attention">MLA: Multi-Head Latent Attention</h2> <p>MLA factorizes attention via low-rank latent projections. Notation follows our reference:</p> <ul> <li>Latent compression:</li> </ul> \[\mathbf{c}^{KV}_t = W^{DKV}\, \mathbf{x}_t,\quad \mathbf{c}^{Q}_t = W^{DQ}\, \mathbf{x}_t,\] <p>where \(W^{DKV} \in \mathbb{R}^{r_{kv} \times D}\), \(W^{DQ} \in \mathbb{R}^{r_q \times D}\).</p> <ul> <li>Per-head decompression:</li> </ul> \[\mathbf{k}^{N}_t = W^{UK}\, \mathbf{c}^{KV}_t,\quad \mathbf{v}^{N}_t = W^{UV}\, \mathbf{c}^{KV}_t,\quad \mathbf{q}^{N}_t = W^{UQ}\, \mathbf{c}^{Q}_t,\] <p>where \(W^{UK} \in \mathbb{R}^{\text{nh}_{kv} * d_{\text{qk}_{nope}} \times r_{kv}}\), \(W^{UV} \in \mathbb{R}^{\text{nh}_{kv} * d_v \times r_{kv}}\), \(W^{UQ} \in \mathbb{R}^{\text{nh}_{q} * d_{\text{qk}_{nope}} \times r_{q}}\).</p> <ul> <li>Decoupled RoPE:</li> </ul> \[\mathbf{k}^{R}_t = \mathrm{RoPE}(W^{KR}\, \mathbf{x}_t),\quad \mathbf{q}^{R}_t = \mathrm{RoPE}(W^{QR}\, \mathbf{c}^{Q}_t),\] <p>where \(W^{KR} \in \mathbb{R}^{d_{\text{qk}_{rope}} \times D}\), \(W^{QR} \in \mathbb{R}^{\text{nh}_{q} * d_{\text{qk}_{rope}} \times r_{q}}\).</p> <p>and we concatenate for each head (i):</p> \[\mathbf{k}_{t,i} = [\,\mathbf{k}^N_{t,i};\ \mathbf{k}^R_t\,],\qquad \mathbf{q}_{t,i} = [\,\mathbf{q}^N_{t,i};\ \mathbf{q}^R_{t,i}\,].\] <p>The forward in our <a href="https://github.com/shreyansh26/multihead-latent-attention/blob/6d47fa3a9ec8105fede03023bb3bce8c4537d48e/mla.py#L10" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">MLA</code></a> implementation mirrors this shape construction:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># MLA.forward (selected lines)
</span><span class="n">c_kv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_dkv</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>  <span class="c1"># [B, S, r_kv]
</span><span class="n">c_q</span>  <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_dq</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>   <span class="c1"># [B, S, r_q]
</span>
<span class="n">k_r</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_kr</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>                       <span class="c1"># [B, S, dR]
</span><span class="n">k_r</span> <span class="o">=</span> <span class="n">k_r</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
<span class="n">k_r</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">k_r</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis_qk</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [B, 1, S, dR]
</span>
<span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">c_kv</span> <span class="o">=</span> <span class="n">cache</span><span class="p">.</span><span class="n">compressed_kv</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">c_kv</span><span class="p">)</span>  <span class="c1"># [B, S_kv, r_kv]
</span>    <span class="n">k_r</span>  <span class="o">=</span> <span class="n">cache</span><span class="p">.</span><span class="n">k_rope</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">k_r</span><span class="p">)</span>          <span class="c1"># [B, 1, S_kv, dR]
</span>
<span class="n">k_n</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uk</span><span class="p">(</span><span class="n">c_kv</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len_kv</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_nope_head_dim</span><span class="p">)</span>
<span class="n">k_n</span> <span class="o">=</span> <span class="n">k_n</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>                    <span class="c1"># [B, H_kv, S_kv, dN]
</span><span class="n">k</span>   <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">k_r</span><span class="p">.</span><span class="nf">repeat_interleave</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">k_n</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">q_r</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_qr</span><span class="p">(</span><span class="n">c_q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
<span class="n">q_r</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">q_r</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis_qk</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1"># [B, H, S, dR]
</span><span class="n">q_n</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uq</span><span class="p">(</span><span class="n">c_q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_nope_head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">q</span>   <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">q_r</span><span class="p">,</span> <span class="n">q_n</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uv</span><span class="p">(</span><span class="n">c_kv</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len_kv</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">v_head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="nf">sdpa_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_o</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div> <p>Intuition: MLA maintains multi-head queries, but routes them through a shared latent bottleneck for \((K,V)\) (and optionally for parts of \(Q\)). This preserves per-head specialization via \(W^{UQ}\), \(W^{UK}\), \(W^{UV}\), while dramatically reducing the “surface area” of the KV cache.</p> <h3 id="fusion-fewer-intermediate-tensors-same-math">Fusion: fewer intermediate tensors, same math</h3> <p>We can fuse linears to reduce memory traffic:</p> <ul> <li>Combine \(W^{DKV}\) and \(W^{KR}\) into a single projection (<code class="language-plaintext highlighter-rouge">w_dkv_kr</code>).</li> <li>Combine \(W^{UK}\) and \(W^{UV}\) into a single projection (<code class="language-plaintext highlighter-rouge">w_uk_uv</code>) then split.</li> <li>Combine \(W^{QR}\) and \(W^{UQ}\) into a single projection (<code class="language-plaintext highlighter-rouge">w_qr_uq</code>) then split for \(\mathbf{q}^N\) and \(\mathbf{q}^C\).</li> </ul> <p>Snippet from <a href="https://github.com/shreyansh26/multihead-latent-attention/blob/6d47fa3a9ec8105fede03023bb3bce8c4537d48e/mla.py#L111" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">MLAFused.forward</code></a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c_q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_dq</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>                 <span class="c1"># [B, S, r_q]
</span><span class="n">c_kv_kr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_dkv_kr</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>         <span class="c1"># [B, S, r_kv + dR]
</span><span class="n">c_kv</span><span class="p">,</span> <span class="n">k_r</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">c_kv_kr</span><span class="p">,</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">k_r</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">k_r</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis_qk</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">c_kv</span> <span class="o">=</span> <span class="n">cache</span><span class="p">.</span><span class="n">compressed_kv</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">c_kv</span><span class="p">)</span>
    <span class="n">k_r</span>  <span class="o">=</span> <span class="n">cache</span><span class="p">.</span><span class="n">k_rope</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">k_r</span><span class="p">)</span>

<span class="n">k_n_v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uk_uv</span><span class="p">(</span><span class="n">c_kv</span><span class="p">)</span>             <span class="c1"># [B, S_kv, H_kv * (dN + dV)]
</span><span class="n">k_n</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">k_n_v</span><span class="p">,</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span>
                             <span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># reshape, build k, build q via w_qr_uq, attend, project out...
</span></code></pre></div></div> <p>Fusion preserves semantics but minimizes reads/writes of large intermediate tensors—especially important under long sequence lengths where bandwidth dominates.</p> <h3 id="absorption-inference-time-mqa-with-latent-routing">Absorption: inference-time MQA with latent routing</h3> <p>At inference we can algebraically “absorb” \(W^{UK}\) into the query path and \(W^{UV}\) into the output path. Starting with</p> \[\mathbf{q}_{t,i} = [\,\mathbf{q}^C_{t,i}; \mathbf{q}^R_{t,i}\,],\qquad \mathbf{k}_t = [\,\mathbf{k}^C_t;\ \mathbf{k}^R_t\,],\] <p>define</p> \[\hat{\mathbf{q}}_{t,i} = \big[(W^{UK}_i)^\top \mathbf{q}^C_{t,i};\ \mathbf{q}^R_{t,i}\big],\qquad \hat{\mathbf{k}}_t = \big[\mathbf{c}^{KV}_t;\ \mathbf{k}^R_t\big].\] <p>Then attention can be computed against a single shared latent KV head \(\mathbf{c}^{KV}\) (plus shared RoPE key), and the per-head value projection is postponed to the output:</p> \[\hat{\mathbf{o}}_{t,i} = \sum_{j=1}^{t} \mathrm{softmax}_j\!\left(\frac{\hat{\mathbf{q}}_{t,i}^\top \hat{\mathbf{k}}_j}{\sqrt{d + d^R}}\right) \mathbf{c}^{KV}_j,\quad \mathbf{y}_t = W^{O} \,[\, W^{UV}_1 \hat{\mathbf{o}}_{t,1};\dots; W^{UV}_H \hat{\mathbf{o}}_{t,H}\,].\] <p>Our <a href="https://github.com/shreyansh26/multihead-latent-attention/blob/6d47fa3a9ec8105fede03023bb3bce8c4537d48e/mla.py#L158" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">MLAFusedAbsorbed</code></a> implements exactly this MQA-like inference path:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Keys: single shared head [k_r, c_kv]
</span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">k_r</span><span class="p">,</span> <span class="n">c_kv</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, 1, S_kv, dR + r_kv]
</span>
<span class="c1"># Queries: per-head RoPE + absorbed-nope to r_kv
</span><span class="n">q_r</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_qr</span><span class="p">(</span><span class="n">c_q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
<span class="n">q_r</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">q_r</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis_qk</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">q_n</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uq_absorbed</span><span class="p">(</span><span class="n">c_q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">kv_lora_rank</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">q</span>   <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">q_r</span><span class="p">,</span> <span class="n">q_n</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Values: the shared latent c_kv as single head
</span><span class="n">v</span> <span class="o">=</span> <span class="n">c_kv</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>                               <span class="c1"># [B, 1, S_kv, r_kv]
</span><span class="n">out</span> <span class="o">=</span> <span class="nf">sdpa_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span>  <span class="c1"># MQA-like compute
</span><span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_o_absorbed</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>                        <span class="c1"># absorbs W^{UV} into W^O
</span></code></pre></div></div> <p>Effect: KV cache stores \(\mathbf{c}^{KV}\) once per token (plus a small shared RoPE key). Communication is essentially MQA, but per-head specialization is retained via the absorbed query/output linears.</p> <h2 id="complexity-and-kv-cache-discussion">Complexity and KV cache discussion</h2> <p>Let:</p> <ul> <li>\(B\): batch size, \(S\): sequence length, \(H\): attention heads,</li> <li>\(H_{kv}\): KV heads in GQA/MLA, \(d\): head dim, \(d_{\text{qk}_{rope}}\): RoPE dim,</li> <li>\(r_q, r_{kv}\): low-rank dimensions for query/kv latents.</li> </ul> <p>Rough per-token storage for the KV cache (ignoring dtype constants):</p> <ul> <li>MHA: \(O(H \cdot S \cdot d)\) for \(K\) and \(O(H \cdot S \cdot d)\) for \(V\).</li> <li>GQA: \(O(H_{kv} \cdot S \cdot d)\) per \(K,V\).</li> <li>MQA: \(O(S \cdot d)\) per \(K,V\).</li> <li>MLA: \(O(S \cdot r_{kv})\) for \(\mathbf{c}^{KV}_t\) and \(O(S \cdot d_{\text{qk}_{rope}})\) for \(\mathbf{k}^R_t\)</li> </ul> <p>Communication between devices during decode scales with KV cache size too; MLA’s absorbed path therefore inherits MQA’s excellent scaling while maintaining multi-head query diversity.</p> <p>Compute:</p> <ul> <li>Matmuls with \(W^{DKV}\) and \(W^{DQ}\) are shared per token, independent of \(H\).</li> <li>Per-head expansions via \(W^{UQ}, W^{UK}, W^{UV}\) are relatively cheap when \(r_q, r_{kv} \ll D\).</li> <li>Absorption swaps some inner-loop per-token head matmuls for outer-loop linears, keeping the high-arithmetic-intensity parts in efficient GEMMs.</li> </ul> <h3 id="kv-cache-storage-size-comparison">KV Cache storage size comparison</h3> <p>MLA has to cache \(\mathbf{c}^{KV}\) and \(\mathbf{k}^R\) for each token, which is \(r_{kv} + d_{\text{qk}_{rope}}\) per token. In the Deepseek v2 and v3 configs, \(r_{kv} = 4 d_{\text{qk}_{nope}}\) and \(d_{\text{qk}_{rope}} = 0.5 * d_{\text{qk}_{nope}}\).</p> <p>The table below shows the KV cache size comparison for the different attention mechanisms.</p> <table> <thead> <tr> <th>Attention Mechanism</th> <th>KV Cache per Token</th> </tr> </thead> <tbody> <tr> <td>MHA</td> <td>\(2n_h d_h l\)</td> </tr> <tr> <td>GQA</td> <td>\(2n_g d_h l\)</td> </tr> <tr> <td>MQA</td> <td>\(2d_h l\)</td> </tr> <tr> <td>MLA</td> <td>\((r_{kv} + d_{\text{qk}_{rope}}) l \approx \frac{9}{2} d_{\text{qk}_{nope}} l\)</td> </tr> </tbody> </table> <h2 id="conclusion">Conclusion</h2> <p>MLA reframes attention as a low-rank routing problem. During training, it behaves much like GQA but with smaller activations; during inference, absorption yields an MQA-like footprint with per-head specialization preserved through the query/output paths. If your production bottleneck is KV cache size or cross-device bandwidth, MLA’s absorbed path is a direct drop-in to claw back latency without sacrificing modeling power.</p> <hr> <p>These are my notes on MLA and hopefully it proves useful to someone looking to understand MLA better.</p> <p><strong>Here is the code</strong> - <a href="https://github.com/shreyansh26/multihead-latent-attention" rel="external nofollow noopener" target="_blank">https://github.com/shreyansh26/multihead-latent-attention</a></p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26" rel="external nofollow noopener" target="_blank">Twitter</a>, <a href="https://github.com/shreyansh26" rel="external nofollow noopener" target="_blank">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/" rel="external nofollow noopener" target="_blank">LinkedIn</a>.</p> </div> </article> <div id="giscus_thread" style="max-width: 1440px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shreyansh26/shreyansh26.github.io","data-repo-id":"R_kgDOMn767Q","data-category":"General","data-category-id":"DIC_kwDOMn767c4CiLUv","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Shreyansh Singh. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?317646887210841a1b23a5eedc9fba39"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZEZ7673Y7G"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZEZ7673Y7G");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-posts",title:"Posts",description:"",section:"Navigation",handler:()=>{window.location.href="/post/index.html"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-bookshelf",title:"Bookshelf",description:"A list of books I&#39;ve read and some of which I&#39;ve found interesting :)",section:"Navigation",handler:()=>{window.location.href="/bookshelf/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/assets/pdf/resume/Resume_Shreyansh.pdf"}},{id:"post-paper-summary-14-physics-of-language-models-part-3-1-knowledge-storage-and-extraction",title:"Paper Summary #14 - Physics of Language Models: Part 3.1, Knowledge Storage and...",description:"My notes from the Physics of Language Models series of papers.",section:"Posts",handler:()=>{window.location.href="/post/2026-01-17_physics-of-lms-3-1-knowledge-storage-and-extraction/"}},{id:"post-understanding-multi-head-latent-attention-mla",title:"Understanding Multi-Head Latent Attention (MLA)",description:"A mathematical and code deep-dive on one of the key innovations from Deepseek - Multihead Latent Attention (MLA)",section:"Posts",handler:()=>{window.location.href="/post/2025-11-08_multihead-latent-attention/"}},{id:"post-deriving-the-gradient-for-the-backward-pass-of-layer-normalization",title:"Deriving the Gradient for the Backward Pass of Layer Normalization",description:"Understanding the math behind Layer Normalization and deriving the gradients for the backward pass.",section:"Posts",handler:()=>{window.location.href="/post/2025-06-04_layernorm-gradients/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-compute-and-instruction-throughput",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Compute and Instruction Throughput",description:"My notes from the talk on maximizing compute and instruction throughput at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-04-04_gtc25-maximize-compute-instruction-throughput/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-2",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"Second part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-2/"}},{id:"post-notes-from-gtc-39-25-cuda-techniques-to-maximize-memory-bandwidth-and-hide-latency-part-1",title:"Notes from GTC&#39;25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency -...",description:"First part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.",section:"Posts",handler:()=>{window.location.href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/"}},{id:"post-faster-cross-encoder-inference-unleashing-torch-compile-for-speed",title:"Faster Cross-Encoder Inference: Unleashing torch.compile for speed",description:"A quick writeup on accelerating a Jina Cross-Encoder using torch.compile",section:"Posts",handler:()=>{window.location.href="/post/2025-03-02_cross-encoder-inference-torch-compile/"}},{id:"post-paper-summary-13-physics-of-language-models-part-2-1-grade-school-math-and-the-hidden-reasoning-process",title:"Paper Summary #13 - Physics of Language Models: Part 2.1, Grade-School Math and...",description:"My notes from the Physics of Language Models series of papers.",section:"Posts",handler:()=>{window.location.href="/post/2024-09-21_physics-of-lms-2-1-grade-school-math-and-the-hidden-reasoning-process/"}},{id:"post-paper-summary-12-image-recaptioning-in-dall-e-3",title:"Paper Summary #12 - Image Recaptioning in DALL-E 3",description:"The image recaptioning technique used in DALL-E 3 was extended to videos in Sora.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_dalle3_image_recaptioner/"}},{id:"post-paper-summary-11-sora",title:"Paper Summary #11 - Sora",description:"OpenAI announced a ground-breaking text-to-video diffusion model capable of generating high-definition videos up to 60 seconds long.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_sora_openai/"}},{id:"post-paper-summary-10-gemini-1-5-pro",title:"Paper Summary #10 - Gemini 1.5 Pro",description:"Google DeepMind announced a multimodal LLM with support of up to 10M context length.",section:"Posts",handler:()=>{window.location.href="/post/2024-02-18_gemini_pro_google/"}},{id:"post-solving-substitution-ciphers-using-markov-chain-monte-carlo-mcmc",title:"Solving Substitution Ciphers using Markov Chain Monte Carlo (MCMC)",description:"Deciphering substitution ciphers can be framed as a Markov chain problem and a simple Monte Carlo sampling approach can help solve them very efficiently",section:"Posts",handler:()=>{window.location.href="/post/2023-07-22_solving_substitution_cipher_using_mcmc/"}},{id:"post-paper-summary-9-sophia-a-scalable-stochastic-second-order-optimizer-for-language-model-pre-training",title:"Paper Summary #9 - Sophia: A Scalable Stochastic Second-order Optimizer for Language Model...",description:"Understanding Sophia - A new fast, scalable second-order optimizer which beats Adam on LLM pretraining.",section:"Posts",handler:()=>{window.location.href="/post/2023-05-28_sophia_scalable_second_order_optimizer_llms/"}},{id:"post-paper-summary-8-flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness",title:"Paper Summary #8 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",description:"Understanding FlashAttention which is the most efficient exact attention implementation out there, which optimizes for both memory requirements and wall-clock time.",section:"Posts",handler:()=>{window.location.href="/post/2023-03-26_flash-attention/"}},{id:"post-paper-summary-7-efficient-transformers-a-survey",title:"Paper Summary #7 - Efficient Transformers: A Survey",description:"A survey paper of improvements over the original Transformer architecture in terms of memory-efficiency.",section:"Posts",handler:()=>{window.location.href="/post/2022-10-10_efficient_transformers_survey/"}},{id:"post-deploying-machine-learning-models-using-gcp-39-s-google-ai-platform-a-detailed-tutorial",title:"Deploying Machine Learning models using GCP&#39;s Google AI Platform - A Detailed Tutorial...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using GCP, specifically the Google AI Platform and use Streamlit to access the model through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-03-06_model_deployment_using_gcp_google_ai_platform/"}},{id:"post-deploying-machine-learning-models-using-aws-lambda-and-github-actions-a-detailed-tutorial",title:"Deploying Machine Learning models using AWS Lambda and Github Actions - A Detailed...",description:"A step-wise tutorial to demonstrate the steps required to deploy a ML model using AWS Lambda, Github Actions, API Gateway and use Streamlit to access the model API through a UI.",section:"Posts",handler:()=>{window.location.href="/post/2022-01-23_model_deployment_using_aws_lambda/"}},{id:"post-ppml-series-3-federated-learning-for-mobile-keyboard-prediction",title:"PPML Series #3 - Federated Learning for Mobile Keyboard Prediction",description:"Understanding how your mobile keyboard (Gboard, specifically) performs the next word prediction task and performs model training and updates",section:"Posts",handler:()=>{window.location.href="/post/2021-12-27_federated_learning_mobile_keyboard/"}},{id:"post-ppml-series-2-federated-optimization-algorithms-fedsgd-and-fedavg",title:"PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg",description:"A mathematical deep dive on a Federated Optimization algorithm - FedAvg and comparing it with a standard approach - FedSGD.",section:"Posts",handler:()=>{window.location.href="/post/2021-12-18_federated_optimization_fedavg/"}},{id:"post-ppml-series-1-an-introduction-to-federated-learning",title:"PPML Series #1 - An introduction to Federated Learning",description:"A short general introduction to Federated Learning (FL) for folks interested in privacy-preserving machine learning (PPML).",section:"Posts",handler:()=>{window.location.href="/post/2021-12-11_intro_to_federated_learning/"}},{id:"post-paper-summary-6-language-models-are-unsupervised-multitask-learners",title:"Paper Summary #6 - Language Models are Unsupervised Multitask Learners",description:"The GPT2 model which aimed to perform complex NLP tasks while relying only on a language model trained in a completely unsupervised fashion.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/"}},{id:"post-paper-summary-5-xlnet-generalized-autoregressive-pretraining-for-language-understanding",title:"Paper Summary #5 - XLNet: Generalized Autoregressive Pretraining for Language Understanding",description:"XLNet tries to overcome the limitations of BERT by having a autoregressive component while also capturing the bidirectional context.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/"}},{id:"post-paper-summary-4-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding",title:"Paper Summary #4 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding...",description:"The ground breaking paper that introduced the famous BERT model. This started the inflow of a large number of BERT-based language understanding models.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/"}},{id:"post-paper-summary-3-improving-language-understanding-by-generative-pre-training",title:"Paper Summary #3 - Improving Language Understanding by Generative Pre-Training",description:"The first paper in the GPT set of models. This is OpenAI&#39;s GPT-1.",section:"Posts",handler:()=>{window.location.href="/post/2021-05-02_language_understanding_generative_pretraining/"}},{id:"post-paper-summary-2-deep-contextualized-word-representations-elmo",title:"Paper Summary #2 - Deep contextualized word representations (ELMo)",description:"The second post in the paper notes series. This time we take a look at ELMo.",section:"Posts",handler:()=>{window.location.href="/post/2021-04-25_deep_contextualized_word_representations_elmo/"}},{id:"post-paper-summary-1-attention-is-all-you-need",title:"Paper Summary #1 - Attention Is All You Need",description:"The first of the paper summary series. This is where I briefly summarise the important papers that I read for my job or just for fun :P",section:"Posts",handler:()=>{window.location.href="/post/2021-04-18_attention_is_all_you_need/"}},{id:"post-deep-learning-in-the-browser-exploring-tf-js-webdnn-and-onnx-js",title:"Deep Learning in the Browser - Exploring TF.js, WebDNN and ONNX.js",description:"A quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2021-01-25_deep_learning_in_the_browser/"}},{id:"post-quick-tutorial-to-deploy-your-ml-models-using-fastapi-and-docker",title:"Quick tutorial to deploy your ML models using FastAPI and Docker",description:"Just a quick tutorial to set up a small scale deployment for your ML or DL model",section:"Posts",handler:()=>{window.location.href="/post/2020-11-30_fast_api_docker_ml_deploy/"}},{id:"post-androids-encryption-crypto-pwn2win-ctf-2020",title:"Androids Encryption (Crypto) - Pwn2Win CTF 2020",description:"My writeup for Androids Encryption challenge in the Pwn2Win CTF 2020",section:"Posts",handler:()=>{window.location.href="/post/2020-06-01_androids_encryption-pwn2win-2020/"}},{id:"post-malwaretech-39-s-vm1-reversing-challenge",title:"MalwareTech&#39;s VM1 Reversing Challenge",description:"My writeup for the VM1 reversing challenge posted by MalwareTech on his website.",section:"Posts",handler:()=>{window.location.href="/post/2020-01-04_malwaretech-vm1-challenge/"}},{id:"post-hxp-36c3-ctf-writeups",title:"hxp 36C3 CTF Writeups",description:"The writeups for the challenges I solved in the first MAJOR CTF that I participated in after a long time.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-30_hxp-36c3-ctf/"}},{id:"post-watevrctf-2019-writeups-mainly-rev-and-pwn",title:"watevrCTF 2019 Writeups (Mainly Rev and Pwn)",description:"My writeups for the challenges I solved in the CTF. I mainly focused on Rev and Pwn categories.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-15_watevr-ctf-2019-writeups/"}},{id:"post-tuctf-2019-pwn-amp-rev-challenges",title:"TUCTF 2019 - Pwn &amp; Rev Challenges",description:"My writeups for some of the PWN challenges of TUCTF 2019.",section:"Posts",handler:()=>{window.location.href="/post/2019-12-02_tuctf-pwn-2019/"}},{id:"post-ritsec-ctf-2019",title:"RITSEC CTF 2019",description:"My writeups for RITSEC CTF 2019. A bit late, but I hope this helps someone!",section:"Posts",handler:()=>{window.location.href="/post/2019-11-19_ritsec-ctf-2019/"}},{id:"post-codefest-39-19-ctf-writeups",title:"Codefest&#39;19 CTF Writeups",description:"The Capture the Flag event for Codefest&#39;19 was hosted from 8 pm IST, 23rd August 2019 to 12 noon IST, 24th August 2019 on Hackerrank.",section:"Posts",handler:()=>{window.location.href="/post/2019-08-25_codefest19-ctf-writeups/"}},{id:"post-angstromctf-writeups",title:"AngstromCTF Writeups",description:"These are the writeups to the problems I solved during the AngstromCTF.",section:"Posts",handler:()=>{window.location.href="/post/2018-03-23_angstromctf-writeups/"}},{id:"post-neverlan-ctf-2018-writeups",title:"NeverLAN CTF 2018 Writeups",description:"These are the writeups of the problems I solved over the weekend for the NeverLAN CTF 2018.",section:"Posts",handler:()=>{window.location.href="/post/2018-02-27_neverlan-ctf-2018-writeups/"}},{id:"news-paper-accepted-at-the-workshop-on-multilingual-surface-realisation-acl",title:"Paper accepted at the Workshop on Multilingual Surface Realisation, ACL",description:"",section:"News",handler:()=>{window.location.href="/news/srst-acl/"}},{id:"news-won-a-student-scholarship-to-attend-blackhat-asia-2019-singapore-100-students-were-selected-from-82-countries",title:"Won a student scholarship to attend BlackHat Asia 2019, Singapore. 100 students were...",description:"",section:"News"},{id:"news-started-to-work-as-a-ta-for-the-artificial-intelligence-course-offered-to-sophomores-of-the-cse-department-of-iit-bhu-varanasi",title:"Started to work as a TA for the Artificial Intelligence course offered to...",description:"",section:"News"},{id:"news-got-my-first-job-and-started-my-career-in-the-field-of-ai-as-a-research-data-scientist-at-mastercard-ai-garage",title:"Got my first job and started my career in the field of AI...",description:"",section:"News"},{id:"news-silver-medal-shopee-price-match-guarantee-competition",title:"Silver medal - Shopee - Price Match Guarantee Competition",description:"",section:"News",handler:()=>{window.location.href="/news/silver-kaggle/"}},{id:"news-paper-accepted-at-the-30th-international-conference-on-artificial-neural-networks-icann-2021",title:"Paper accepted at the 30th International Conference on Artificial Neural Networks (ICANN 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/icann/"}},{id:"news-paper-accepted-at-the-28th-international-conference-on-neural-information-processing-iconip-2021",title:"Paper accepted at the 28th International Conference on Neural Information Processing (ICONIP 2021)...",description:"",section:"News",handler:()=>{window.location.href="/news/iconip/"}},{id:"news-joined-level-ai-as-a-machine-learning-engineer-in-nlp",title:"Joined Level AI as a Machine Learning Engineer in NLP.",description:"",section:"News"},{id:"news-promoted-to-senior-ml-engineer-at-level-ai",title:"Promoted to Senior ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-lead-ml-engineer-at-level-ai",title:"Promoted to Lead ML Engineer at Level AI!",description:"",section:"News"},{id:"news-promoted-to-principal-ml-engineer-at-level-ai",title:"Promoted to Principal ML Engineer at Level AI!",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%68%72%65%79%61%6E%73%68.%70%65%74%74%73%77%6F%6F%64@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=x8LmoJIAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/shreyansh26","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/shreyansh26","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/shreyansh_26","_blank")}},{id:"socials-medium",title:"Medium",section:"Socials",handler:()=>{window.open("https://medium.com/@shreyansh26","_blank")}},{id:"socials-instagram",title:"Instagram",section:"Socials",handler:()=>{window.open("https://instagram.com/shreyanshs","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>