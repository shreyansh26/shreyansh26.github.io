<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://shreyansh26.github.io/Personal-Website-New/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shreyansh26.github.io/Personal-Website-New/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-05T17:34:03+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/feed.xml</id><title type="html">blank</title><subtitle>Shreyansh&apos;s personal website. </subtitle><entry><title type="html">Paper Summary #12 - Image Recaptioning in DALL-E 3</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2024-02-18_dalle3_image_recaptioner/" rel="alternate" type="text/html" title="Paper Summary #12 - Image Recaptioning in DALL-E 3"/><published>2024-02-18T08:00:00+00:00</published><updated>2024-02-18T08:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/image-recaptioning-dalle3</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2024-02-18_dalle3_image_recaptioner/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Technical Paper</strong>: <a href="https://cdn.openai.com/papers/dall-e-3.pdf">Improving Image Generation with Better Captions</a></p> <p>OpenAI’s Sora is built upon the image captioning model which was described in quite some detail in the DALL-E 3 technical report.</p> <hr/> <p>In general, in text-image datasets, the captions omit background details or common sense relationships, e.g. sink in a kitchen or stop signs along the road. They also omit the position and count of objects in the picture, color and size of the objects and any text present in the image.</p> <p>OpenAI trained a captioner model to solve this.</p> <p>An image captioner is similar to a language model that predicts the next token conditioned on the image and the past generated tokens. Since images are composed of many thousands of pixel values, conditioning on all of this information is very complex and inefficient. The solution DALL-E 3 used was to use CLIP’s compressed representational space to condition upon. OpenAI jointly pre-trained a captioner with a CLIP and a language modeling objective using this formulation on the text and image pairs.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/lm.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>However, this still doesn’t solve the reluctance of the captioning model to provide details, as mentioned above. The fine-tuning of this captioner to be more descriptive and helpful for the text-to-image task is done in two phases.</p> <p>First, they built a small dataset of captions that describe only the main subject of the image. The captioner is fine-tuned on this dataset. Now the model is more biased towards describing the main subject of the image.</p> <p>Next, they created a dataset of long, highly-descriptive captions describing the contents of each image in the fine-tuning dataset. In addition to the main subject, these captions describe the surroundings, background, text found in the image, styles, coloration, etc. as well. The captioner is then fine-tuned on this dataset to generate descriptive synthetic captions.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/synthetic_captions.png" alt="Examples of alt-text accompanying selected images scraped from the internet, short synthetic captions (SSC), and descriptive synthetic captions (DSC)."/> <figcaption>Examples of alt-text accompanying selected images scraped from the internet, short synthetic captions (SSC), and descriptive synthetic captions (DSC).</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>There is a still a problem though…</strong></p> <p>Text-to-Image diffusion models have a tendency to overfit to the text distribution. Using synthetic captions can lead to such issues since the captioner model can have many modal behaviours that won’t be apparent but can bias the text-to-image model when trained on the synthetic captions.<br/> Examples of where this might occur is in letter casing, where punctuation appears in the caption (e.g. does it always end with a period?), how long the captions are, or stylistic tendencies such as starting all captions with the words “a” or “an”.</p> <p>OpenAI overcame this issue by regularizing the text inputs to a distribution that is similar to the humans. The ground-truth captions provide this already since they were drawn from a distribution of human-written text. The regularization can be introduced during training the model by randomly selecting either the ground truth or synthetic caption with a fixed percent chance. DALLE-3 used 95% synthetic captions and 5% ground truth captions.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/perf.png" alt="CLIP scores for text-to-image models trained on different caption types. Left is evaluation results with ground truth captions on our evaluation dataset. Right uses the descriptive synthetic captions from the same dataset"/> <figcaption>CLIP scores for text-to-image models trained on different caption types. Left is evaluation results with ground truth captions on our evaluation dataset. Right uses the descriptive synthetic captions from the same dataset</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/synthetic_ratio.png" alt="CLIP scores for text-to-image models trained on various blending ratios of descriptive synthetic captions and ground-truth captions. Evaluation performed using ground truth captions."/> <figcaption>CLIP scores for text-to-image models trained on various blending ratios of descriptive synthetic captions and ground-truth captions. Evaluation performed using ground truth captions.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The above figure shows that using very high percentage of synthetic captions maximizes the performance of the models. But increasing the synthetic caption ratio implies biasing the model to the distribution of long, highly-descriptive captions emitted by the captioning model. OpenAI used GPT-4 to upsamples any caption into a highly descriptive one.</p> <p>Here are some examples -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/gpt_upsample.png" alt="Effect of using 'upsampled' drawbench captions to create samples with DALL-E 3. Original drawbench captions on top, upsampled captions on bottom. Images are best of 4 for each caption."/> <figcaption>Effect of using 'upsampled' drawbench captions to create samples with DALL-E 3. Original drawbench captions on top, upsampled captions on bottom. Images are best of 4 for each caption.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Below is the prompt OpenAI used to “upsample” the captions.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/upsample_prompt.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="Computer Vision"/><category term="image-captioning"/><category term="generative-ai"/><summary type="html"><![CDATA[The image recaptioning technique used in DALL-E 3 was extended to videos in Sora.]]></summary></entry><entry><title type="html">Paper Summary #11 - Sora</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2024-02-18_sora_openai/" rel="alternate" type="text/html" title="Paper Summary #11 - Sora"/><published>2024-02-18T07:00:00+00:00</published><updated>2024-02-18T07:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/sora-openai</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2024-02-18_sora_openai/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sora_openai/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Technical Paper</strong>: <a href="https://openai.com/sora">Sora - Creating video from text</a><br/> <strong>Blog</strong>: <a href="https://openai.com/research/video-generation-models-as-world-simulators">Video generation models as world simulators</a></p> <p>These are just short notes / excerpts from the technical paper for quick lookup.</p> <hr/> <p>Sora is quite a breakthrough. It is able to understand and simulate the physical world, generating up to 60s long high-definition videos while maintaining the quality, scene continuation and following the user’s prompt.</p> <p>Key papers Sora is built upon -</p> <ul> <li><a href="https://arxiv.org/abs/2212.09748">Diffusion Transformer (DiT)</a></li> <li><a href="https://arxiv.org/abs/2112.10752">Latent Diffusion Models</a></li> <li><a href="https://cdn.openai.com/papers/dall-e-3.pdf">DALL-E 3 Image Recaptioning</a></li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sora_openai/diffusion.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Sora (being a diffusion transformer) uses the idea from ViT of using patches. The videos are converted to patches by first first compressing videos into a lower-dimensional latent space (as in the Latent Diffusion Models paper) and then decomposing the representation into spacetime patches.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sora_openai/patches1.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The network takes raw video as input and outputs a latent representation that is compressed temporally and spatially. The video training and generation is done within this latent space. A decoder model is also trained that maps generated latents back to pixel space.</p> <p>A sequence of spacetime patches is extracted from the compressed video. The patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. During inference time, an appropriately-sized grid (for the random patches) can be selected to control the size of the generated videos.</p> <p>Hence, while past approaches resized, cropped or trimmed videos to a standard size, Sora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. Training on the native aspect ratios was in fact helpful for the model to improve composition and framing. This is similar to what Adept did in <a href="https://www.adept.ai/blog/fuyu-8b">Fuyu-8B</a>.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sora_openai/fuyu.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="how-does-it-adhere-to-text-so-well">How does it adhere to text so well?</h3> <p>OpenAI trained a very descriptive captioner model which they used to generate captions for all the videos in their training set (probably a finetuned GPT-4V ?). Training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.</p> <p>They also use query expansion - using GPT to turn short user prompts into longer detailed captions that are sent to the video model.</p> <p>More details about the image captioner in the next blog post.</p> <h3 id="what-are-some-emergent-capabilities-that-are-exhibited-with-scale">What are some emergent capabilities that are exhibited with scale?</h3> <ul> <li>3D consistency - Sora videos can shift and rotate elements based on the camera motion.</li> <li>Long-range coherence and object permanence</li> <li>Interacting with the world - Though not perfect, Sora can sometimes remember the state of the world after an action was taken.</li> <li>Simulating digital worlds - Sora can generate Minecraft videos, wherein it can simultaneously control the player with a basic policy while also rendering the world and its dynamics in high fidelity.</li> </ul> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="Computer Vision"/><category term="diffusion"/><category term="image-generation"/><category term="video-generation"/><category term="generative-ai"/><summary type="html"><![CDATA[OpenAI announced a ground-breaking text-to-video diffusion model capable of generating high-definition videos up to 60 seconds long.]]></summary></entry><entry><title type="html">Paper Summary #10 - Gemini 1.5 Pro</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2024-02-18_gemini_pro_google/" rel="alternate" type="text/html" title="Paper Summary #10 - Gemini 1.5 Pro"/><published>2024-02-18T06:00:00+00:00</published><updated>2024-02-18T06:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/gemini15-pro</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2024-02-18_gemini_pro_google/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Technical Paper</strong>: <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</a><br/> <strong>Blog</strong>: <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024">Our next-generation model: Gemini 1.5</a></p> <p>These are just short notes / excerpts from the technical paper for quick lookup. Actual implementational details are anyways missing in the technical paper.</p> <hr/> <p>Gemini 1.5 Pro is a sparse MoE Transformer-based model (seems like a trend these days, after GPT-4’s rumors and Mixtral).</p> <p>It supports upto 10M context multimodal context length which is about a day of 22 hours of audio recordings, more than ten times the entirety of the 1440 page book “War and Peace”, the entire Flax codebase (41,070 lines of code), or three hours of video at 1 frame-per-second.</p> <p>Near-perfect retrieval across complete context length. In all modalities, i.e., text, video and audio, it achieves 100% accurate needle-in-a-haystack recall for 530k tokens, 99.7% accurate recall for 1M tokens and 99.2% for 10M contexts.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/niah1.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/text_niah.png" alt="Text Haystack. This figure compares Gemini 1.5 Pro with GPT-4 Turbo for the text needle-in-a-haystack task. Green cells indicate the model successfully retrieved the secret number, gray cells indicate API errors, and red cells indicate that the model response did not contain the secret number. The top row shows results for Gemini 1.5 Pro, from 1k to 1M tokens (top left), and from 1M to 10M tokens (top right). The bottom row shows results on GPT-4 Turbo up to the maximum supported context length of 128k tokens. The results are color-coded to indicate: green for successful retrievals and red for unsuccessful ones."/> <figcaption>Text Haystack. This figure compares Gemini 1.5 Pro with GPT-4 Turbo for the text needle-in-a-haystack task. Green cells indicate the model successfully retrieved the secret number, gray cells indicate API errors, and red cells indicate that the model response did not contain the secret number. The top row shows results for Gemini 1.5 Pro, from 1k to 1M tokens (top left), and from 1M to 10M tokens (top right). The bottom row shows results on GPT-4 Turbo up to the maximum supported context length of 128k tokens. The results are color-coded to indicate: green for successful retrievals and red for unsuccessful ones.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/video_niah.png" alt="Video Haystack. This figure compares Gemini 1.5 Pro with GPT-4V for the video needle-in-a-haystack task, where the models are given video clips of different lengths up to three hours of video and are asked to retrieve a secret word embedded as text at different points within the clip. All video clips are sampled at one frame-per-second (1 fps). The first pair of 10 × 50 haystack plots on the left compare Gemini 1.5 Pro with GPT-4V on the first hour of the documentary. The x-axis represents the video duration which ranges from 1.2 minutes to 1 hour, and the y-axis represents the depth, namely the relative offset of the needle (e.g., the top left cell represents providing the model with the first 1.2 minutes and randomly sampling a frame in the first ten percent of that trimmed video where the needle is inserted). A green cell indicates that the model successfully retrieved the needle, whereas a gray cell indicates an API error. Whereas the GPT-4V API supports video lengths only up to around the first 3 minutes of the full hour, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages up to an hour of video, as shown by the all-green plot. Finally, the 10 × 10 grid on the right shows Gemini 1.5 Pro’s perfect retrieval capabilities across three full hours of video, constructed by concatenating two copies of the documentary back-to-back."/> <figcaption>Video Haystack. This figure compares Gemini 1.5 Pro with GPT-4V for the video needle-in-a-haystack task, where the models are given video clips of different lengths up to three hours of video and are asked to retrieve a secret word embedded as text at different points within the clip. All video clips are sampled at one frame-per-second (1 fps). The first pair of 10 × 50 haystack plots on the left compare Gemini 1.5 Pro with GPT-4V on the first hour of the documentary. The x-axis represents the video duration which ranges from 1.2 minutes to 1 hour, and the y-axis represents the depth, namely the relative offset of the needle (e.g., the top left cell represents providing the model with the first 1.2 minutes and randomly sampling a frame in the first ten percent of that trimmed video where the needle is inserted). A green cell indicates that the model successfully retrieved the needle, whereas a gray cell indicates an API error. Whereas the GPT-4V API supports video lengths only up to around the first 3 minutes of the full hour, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages up to an hour of video, as shown by the all-green plot. Finally, the 10 × 10 grid on the right shows Gemini 1.5 Pro’s perfect retrieval capabilities across three full hours of video, constructed by concatenating two copies of the documentary back-to-back.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/audio_niah.png" alt="Audio Haystack. This figure presents the audio version of the needle-in-a-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo. In this setting, the needle is a short segment of audio that is inserted within a very large audio segment (of up to 22 hours) containing concatenated audio clips. The task is to retrieve the 'secret keyword' which is revealed in the needle. Red indicates that the model did not identify the keyword, whereas green indicates that the model identified the keyword correctly"/> <figcaption>Audio Haystack. This figure presents the audio version of the needle-in-a-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo. In this setting, the needle is a short segment of audio that is inserted within a very large audio segment (of up to 22 hours) containing concatenated audio clips. The task is to retrieve the 'secret keyword' which is revealed in the needle. Red indicates that the model did not identify the keyword, whereas green indicates that the model identified the keyword correctly</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>However, with about 100 needles (with the model requiring to retrieve them all), the performance drops to 70% recall up to 128K tokens, and &gt;60% recall up to 1M tokens. GPT-4 is 50% recall at 128k tokens (its maximum context).</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/multiple_niah.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Gemini 1.5 Pro beats RAG-based systems for all modalities. Although this may not be cost efficient right now unless there are multiple queries and the long context can be prefix-cached.</p> <p>Excellent use of context for learning new skills. It was able to learn to translate from/to English to/from Kalamang, an extremely low-resource language (spoken by less than 200 people in the world), with just a single set of linguistic documentation, a dictionary, and ≈ 400 parallel sentences. That’s it!</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/kalamang.png" alt="Given a reference grammar book and a bilingual wordlist (dictionary), Gemini 1.5 Pro is able to translate from English to Kalamang with similar quality to a human who learned from the same materials."/> <figcaption>Given a reference grammar book and a bilingual wordlist (dictionary), Gemini 1.5 Pro is able to translate from English to Kalamang with similar quality to a human who learned from the same materials.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/kalamang_results.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Gemini 1.5 Pro is exceeds Gemini 1.0 Ultra on text. However the latter is still better on vision and audio tasks, but the compute-efficiency and speed of shipping does suggest some ideas of distillation coming into the picture.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/results_comparison.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Figures 4 and 5 in the Technical report are super cool. In Figure 4, Gemini 1.5 is able to extract the exact scene with the page number from a textbook given a hand-drawn sketch as the prompt.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/fig4_paper.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>In Figure 5 they show that it is able to do the same in videos as well. The model returns a detailed description and timestamp of the scene.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/fig5_paper.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The cumulative average negative log-likelihood (NLL) as a function of token position in long documents follows a power-law quite accurately up to 1M tokens for long-documents and about 2M tokens for code but deviates at the 10M scale.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/pplx.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Though it <a href="https://youtu.be/wa0MT8OwHuk">seems like the inference speeds are quite slow</a> for long contexts at the moment, but the capabilities are absolutely insane!</p> <hr/> <p>An exciting future ahead!</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="LLMs"/><category term="llm"/><category term="multimodal"/><category term="transformer"/><summary type="html"><![CDATA[Google DeepMind announced a multimodal LLM with support of up to 10M context length.]]></summary></entry><entry><title type="html">Solving Substitution Ciphers using Markov Chain Monte Carlo (MCMC)</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2023-07-22_solving_substitution_cipher_using_mcmc/" rel="alternate" type="text/html" title="Solving Substitution Ciphers using Markov Chain Monte Carlo (MCMC)"/><published>2023-07-23T06:00:00+00:00</published><updated>2023-07-23T06:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/substitution-cipher-using-mcmc</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2023-07-22_solving_substitution_cipher_using_mcmc/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/substitution_cipher_using_mcmc/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>I was reading about Markov Chain Monte Carlo (MCMC) recently and discovered a very famous application of using them to decrypt substitution ciphers. This blog is meant to serve as notes on how the problem can be framed as a Markov chain and how a simple yet smart Monte Carlo sampling approach can help solve it very efficiently. In this blog post, I won’t be explaining what Markov process and MCMC is, however I’ll add some references for that at the end of this post.</p> <p><a href="https://github.com/shreyansh26/Solving-Substitution-Ciphers-using-MCMC"><strong>Code for reference on Github</strong></a></p> <h3 id="assumptions">Assumptions</h3> <p>We assume a very simple setting in which we only deal with the 26 lowercase alphabets of the English language and the space character. This can be extended to more characters, but that isn’t imperative to understand how the solution works. It can be extended for a larger set of characters with a bit of effort and without changing the core algorithm.</p> <h3 id="solution">Solution</h3> <p>The input we have is just the ciphertext from an English plaintext. How do we go from the ciphertext to the plaintext using MCMC?</p> <p><strong>Scoring Function</strong></p> <p>We need a scoring function to check how good our current state is. The state here the current mapping we have for the alphabets (for the substitution cipher).</p> <p>The scoring function can be defined in multiple ways. One option can be to check fraction of decrypted words that appear in an English dictionary. Another option is to define the score to be the probability of seeing the given sequence of decrypted characters, which can be calculated as the product of consecutive bigram probabilities, using the pair-wise probabilities based on an existing long piece of English text.</p> <p>In my implementation, I have used the second option. So, if the plaintext/ciphertext is say, “good life is…”, the score will be the product of probabilities -</p> \[\\ \begin{align} Pr[(g,o)] * Pr[(o,o)] * Pr[(o,d)] * Pr[(d, &lt;space&gt;)] * ... \textrm{and so on} \end{align} \\\] <p>Here \(Pr[(g,o)]\) is the estimate of the probability that \(o\) follows \(g\) in typical English text.</p> <p><strong>Transition Function</strong></p> <p>Once we have the scoring function, the transition can be defined as follows -</p> <p><strong>Algorithm</strong></p> <p>Repeat for N iterations<br/> $\quad$Take current mapping $m$<br/> $\quad$Switch the image of 2 random symbols to produce mapping $m’$<br/> $\quad$Compute the score for $m’$ and $m$<br/> $\quad$If score for $m’$ is higher than score for $m$<br/> $\qquad$Accept<br/> $\quad$If not, then with a small probability <br/> $\qquad$Accept <br/> $\quad$Else<br/> $\qquad$Reject</p> <p>This transition function ensures that the stationary distribution of the chain puts much higher weight on better mappings.</p> <p>This is how it would look when translated into code - <script src="https://gist.github.com/shreyansh26/9e117289ae36e1b353581672335466b9.js"></script></p> <p>The above chain runs extremely fast and solves the cipher in a couple thousand iterations.</p> <p>The entire code for this project including the bigram frequency/probability calculation and the MCMC-based solver is <a href="https://github.com/shreyansh26/Solving-Substitution-Ciphers-using-MCMC">here on Github</a>.</p> <h3 id="additional-references-on-markov-chainsmcmc">Additional references on Markov Chains/MCMC</h3> <ul> <li><a href="https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/">Markov Chain Monte Carlo Without all the Bullshit</a></li> <li><a href="https://stats.stackexchange.com/questions/165/how-would-you-explain-markov-chain-monte-carlo-mcmc-to-a-layperson">How would you explain Markov Chain Monte Carlo (MCMC) to a layperson?</a></li> </ul> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="Mathematics"/><category term="Cryptography"/><category term="sampling"/><category term="probability"/><category term="mcmc"/><summary type="html"><![CDATA[Deciphering substitution ciphers can be framed as a Markov chain problem and a simple Monte Carlo sampling approach can help solve them very efficiently]]></summary></entry><entry><title type="html">Paper Summary #9 - Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2023-05-28_sophia_scalable_second_order_optimizer_llms/" rel="alternate" type="text/html" title="Paper Summary #9 - Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training"/><published>2023-05-28T00:00:00+00:00</published><updated>2023-05-28T00:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/sophia-optimizer</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2023-05-28_sophia_scalable_second_order_optimizer_llms/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Paper</strong>: Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training<br/> <strong>Link</strong>: <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a><br/> <strong>Authors</strong>: Hong Liu, Zhiyuan Li, David Hall, Percy Liang, Tengyu Ma<br/> <strong>Code</strong>: <a href="https://github.com/Liuhong99/Sophia">https://github.com/Liuhong99/Sophia</a></p> <hr/> <p><strong>I have also released an annotated version of the paper. You can find it <a href="https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/Sophia%20-%20A%20Scalable%20Stochastic%20Second-order%20Optimizer%20for%20Language%20Model%20Pretraining.pdf">here</a>.</strong></p> <p>Sophia is probably one of the most interesting papers I have read recently and I really liked how well it was written. This post is basically the notes that I had made while reading the paper, which is why it is not exactly a blog post and most of it is verbatim copied from the paper. But since there are a lot of optimization-theory related concepts which have been mentioned in the paper, I have tried to add my own set of references which I have read in the past that helped me understand the paper better. Hopefully it helps someone!</p> <h1 id="overview">Overview</h1> <ul> <li>The goal of this work is to propose a new optimizer for pre-training LLMs that can improve pre-training efficiency with a faster optimizer, which either reduces the time and cost to achieve the same pre-training loss, or alternatively achieves better pre-training loss with the same budget.</li> <li>Adam and its variants have become the somewhat default optimizers which are used in LLM pretraining.</li> <li>Designing fast optimizers for LLMs is challenging because - <ul> <li>The benefit of the first-order pre-conditioner (the \(1/\sqrt{v_t}\) factor) in Adam is still not well understood.</li> <li>The choice of pre-conditioners is constrained because we can only afford light-weight options whose overhead can be offset by the speed-up in the number of iterations.</li> <li>Among the recent works on light-weight gradient-based pre-conditioners, <a href="https://arxiv.org/abs/2302.06675">Lion</a> stood out as it is substantially faster than Adam on vision Transformers and diffusion models but only achieves limited speed-up on LLMs.</li> </ul> </li> <li>This paper introduces Sophia, <strong>S</strong>econd-<strong>o</strong>rder Cli<strong>p</strong>ped Stoc<strong>h</strong>ast<strong>i</strong>c Optimiz<strong>a</strong>tion, a light-weight second-order optimizer that uses an inexpensive stochastic estimate of the diagonal of the Hessian as a pre-conditioner and a clipping mechanism to control the worst-case update size.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/fig1.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>Key Results - <ul> <li>Sophia achieves the same validation pre-training loss with 50% fewer number of steps than Adam.</li> <li>Sophia maintains almost the memory and average time per step and therefore the speedup also translates to 50% less total compute and 50% less wall-clock time.</li> <li>The scaling law based on model size from 125M to 770M is in favor of Sophia over Adam - the gap between Sophia and Adam with 100K steps increases as the model size increases. Sophia on a 540M-parameter model with 100K steps gives the same validation loss as Adam on a 770M-parameter model with 100K steps.<br/> <br/></li> </ul> </li> <li>Checking the performance and scalability of this optimizer for pre-training much larger model sizes would (although expensive) be interesting to see.</li> <li>Sophia estimates the diagonal entries of the Hessian of the loss using a mini-batch of examples every \(k\) steps (\(k=10\) in the paper).</li> <li>The paper considers two options for diagonal Hessian estimators - <ul> <li>Hutchinson’s unbiased estimator - an unbiased estimator that uses a Hessian-vector product with the same run-time as a mini-batch gradient up to a constant factor</li> <li>Gauss-Newton-Bartlett (GNB) estimator - a biased estimator that uses one mini-batch gradient calculated with resampled labels. <br/><br/></li> </ul> </li> <li>Both the estimators introduce only a 5% overhead per step (on average).</li> <li>Sophia updates the parameter with an exponential moving average (EMA) of the gradient divided by the EMA of the diagonal Hessian estimate, subsequently clipped by a scalar.</li> <li>Due to the Hessian-based pre-conditioner, Sophia adapts more efficiently, than Adam does, to the heterogeneous curvatures in different parameter dimensions, which can often occur in the landscape of LLMs losses and cause instability or slowdown.</li> <li>In terms of the loss landscape, Sophia has a more aggressive pre-conditioner than Adam - Sophia applies a stronger penalization to updates in sharp dimensions (where the Hessian is large) than the flat dimensions (where the Hessian is small), <b>ensuring a uniform <i><ins>loss decrease</ins></i> across all parameter dimensions</b>. In contrast, <strong>Adam’s updates are mostly uniform across all parameter dimensions, leading to a slower loss decrease in flat dimensions</strong>.</li> <li>Sophia’s clipping mechanism controls the worst-case size of the updates in all directions, safeguarding against the negative impact of inaccurate Hessian estimates, rapid Hessian changes over time, and non-convex landscape.</li> </ul> <h1 id="motivations">Motivations</h1> <h2 id="heterogeneous-curvatures">Heterogeneous Curvatures</h2> <ul> <li>Loss functions in modern deep learning problems often have different curvatures across different parameter dimensions.</li> <li>The paper demonstrates the limitations of Adam and Gradient Descent by considering a two dimensional loss function - \(L(\theta_{[1]},\theta_{[2]}) = L_1(\theta_{[1]}) + L_2(\theta_{[2]})\), where</li> </ul> \[\\ \begin{align} L_{1}(\theta_{[1]}) = 8(\theta_{[1]} - 1)^2(1.3\theta_{[1]}^2 + 2\theta_{[1]} + 1) \end{align} \\\] \[\\ \begin{align} L_2(\theta_{[2]}) = 1/2 (\theta_{[2]} - 4)^2 \end{align} \\\] <ul> <li>Here \(L_1\) is much sharper than \(L_2\).</li> <li>Another optimizer, SignGD is also compared which is quite old but can be understood as a simplified version of Adam, which does not involve taking the EMA for gradients and second moments of the gradients. The update then simplifies to -</li> </ul> \[\eta \cdot \nabla L(\theta)/|\nabla L(\theta)| = \eta\cdot \textup{sign}(\nabla L(\theta))\] <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/fig2.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>## Limitations of GD and SignGD (Adam)</p> <ul> <li>The optimal learning rate of Gradient Descent should be proportional to the inverse of the curvature, i.e., the Hessian/second derivative at the local minimum. A good set of resources to understand this in detail are some notes from University of Toronto <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L01_intro.pdf">Section 2.2</a> and <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L02_Taylor_approximations.pdf">Section 4.1</a>.</li> <li>So, if the curvatures of \(L_1\) and \(L_2\) at the local minima are \(h_1\) and \(h_2\) respectively (and thus \(h_1 &gt; h_2\) ). So the largest shared learning rate can only be \(1/h_1\). Hence, the convergence in \(\theta_{[2]}\) dimension is slow as also shown in the figure above. <br/> <br/></li> <li>The update size of SignGD is the learning rate \(\eta\) in all dimensions. Hence, intuitively, the same update size translates to less progress in decreasing the loss in the flat direction than in the sharp direction. In the yellow curve in the above figure, the progress of SignGD in the flat dimension \(\theta_{[2]}\) is slow and along \(\theta_{[1]}\), the iterate quickly travels to the valley in the first three steps and then starts to bounce. To fully converge in the sharp dimension, the learning rate \(\eta\) needs to decay to \(0\), which will exacerbate the slow convergence in the flat dimension \(θ_{[2]}\). The trajectory of Adam is similar to SignGD and shown by the red curve in the figure. <br/><br/></li> <li>The behavior of SignGD and Adam above indicates that a more aggressive pre-conditioning is needed - sharp dimensions should have relatively smaller updates than flat dimensions so that the decrease of loss is equalized in all dimensions.</li> <li>Prior work on second order optimization suggest that the optimal pre-conditioner is the Hessian which captures the curvature on each dimension.</li> <li>The Newton’s method, does something similar - the update is the gradient divided by the Hessian in each dimension.</li> </ul> \[\theta_{[1]} \leftarrow \theta_{[1]} - \eta \cdot L'_1(\theta_{[1]}) / h_1\ \ \textup{and}\ \ \theta_{[2]} \leftarrow \theta_{[2]} - \eta \cdot L'_2(\theta_{[2]})/ h_2\] <p><br/></p> <h2 id="limitations-of-newtons-method">Limitations of Newton’s method</h2> <ul> <li>Vanilla Newton’s method could converge to a global maximum when the local curvature is negative.</li> <li>As shown in the blue curve in the figure, Newton’s method quickly converges to a saddle point instead of a local minimum.</li> <li>Since the curvature may change rapidly along a trajectory, the second order information can often become unreliable. <br/><br/></li> <li>Sophia addresses this by considering only pre-conditioners that capture positive curvature, and introduce a per-coordinate clipping mechanism to mitigate the rapid change of Hessian. Applying those changes results in the following update -</li> </ul> \[\theta_{[1]} \leftarrow \theta_{[1]} - \eta \cdot \textup{clip}(\frac{ L'_1(\theta_{[1]})}{\max\{h_1,\epsilon\}} ,\rho)\ \textup{and}\ \theta_{[2]} \leftarrow \theta_{[2]} - \eta \cdot \textup{clip}(\frac{ L'_2(\theta_{[2]})}{\max\{h_2,\epsilon\}},\rho)\] <ul> <li>Here, \(\rho\) is a constant to control the worst-case update size, \(\epsilon\) is a very small constant (e.g., 1e-12) to avoid dividing by 0.</li> <li>The beauty here is that when the curvature of some dimension is rapidly changing or negative and thus the second-order information is misleading and possibly leads to a huge update before clipping, the clipping mechanism kicks in and the optimizer defaults to SignGD (even though this is sub-optimal for benign situations).</li> <li>The black curve in the figure starts off similarly to SignGD due to the clipping mechanism in the non-convex region, making descent opposed to converging to a local maximum. In the convex valley, it converges to the global minimum with a few steps.</li> <li>Compared with SignGD and Adam, it makes much faster progress in the flat dimension \(\theta_{[2]}\) (because the update is bigger in dimension \(\theta_{[2]}\)), while avoiding bouncing in the sharp dimension \(\theta_{[1]}\) (because the update is significantly shrunk in the sharp dimension \(\theta_{[1]}\)).</li> </ul> <h1 id="sophia-second-order-clipped-stochastic-optimization">Sophia: <strong>S</strong>econd-<strong>o</strong>rder Cli<strong>p</strong>ped Stoc<strong>h</strong>ast<strong>i</strong>c Optimiz<strong>a</strong>tion</h1> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/sophia.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>## EMA of diagonal Hessian estimates</p> <ul> <li>The diagonal Hessian estimates definitely have overheads, so it is computed at every \(k\) steps.</li> <li>At every time step \(t\), where \(t \ \textup{mod} \ k = 1\), the estimator returns an estimate \(h_t\) of the diagonal of the Hessian of the mini-batch loss.</li> <li>Similar to the gradient of the mini-batch loss function, the estimated diagonal Hessian can also have large noise.</li> <li>The EMA performed at every \(k\) steps helps denoise the estimates.</li> </ul> \[h_t = \beta_2 h_{t-k} + (1 - \beta_2) \hat{h}_{t} \ \textup{ if } t \ \mathrm{mod} \ k = 1 ; \textup{ else } h_t = h_{t-1}\] <h2 id="per-coordinate-clipping">Per-coordinate clipping</h2> <ul> <li>I personally found this very innovative.</li> <li>As mentioned in the previous section, the inaccuracy of Hessian estimates and the change of Hessian along the trajectory can make the second-order information unreliable.</li> <li> \[\theta_{t+1} \leftarrow \theta_{t} - \eta_t \cdot \textup{clip}(m_t / \max\{h_t,\epsilon\}, \rho)\] </li> <li>When any entry of \(h_t\) is negative, e.g., \(h_t[i] &lt; 0\), the corresponding entry in the pre-conditioned gradient \(m_t[i]/\max\{h_t[i],\epsilon\} = m_t[i]/\epsilon\) is extremely large and has the same sign as \(m_t[i]\), and thus \(\eta\cdot \textup{clip}(m_t[i] / \max\{h_t[i],\epsilon\}, \rho) = \eta\rho\cdot \textup{sign}(m_t[i])\), which is the same as stochastic momentum SignSGD.</li> <li>Sophia uses stochastic momentum SignSGD as a backup when the Hessian is negative (or mistakenly estimated to be negative or very small.)</li> <li>The clipping mechanism controls the worst-case size of the updates in all parameter dimensions to be at most \(\rho\), which also improves the stability (which could be a severe issue for second-order methods).</li> </ul> <h2 id="diagonal-hessian-estimators">Diagonal Hessian Estimators</h2> <ul> <li>I’ll keep this section short and brief, for the simple reason that the math is extremely interesting but also a bit difficult to understand. So, I’ll add references to the best of my knowledge and I encourage the reader to go through them thoroughly.</li> </ul> <h3 id="hutchinsons-unbiased-estimator">Hutchinson’s unbiased estimator</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/hutchinson.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>For any loss function \(\ell(\theta)\) on parameters \(\theta\in \mathbb{R}^d\), the Hutchinson’s estimator can be used to obtain an unbiased estimator for the diagonal of the Hessian.</li> <li>First, draw \(u\in \mathbb{R}^d\) from the spherical Gaussian distribution \(\mathcal{N}(0,\mathrm{I}_d)\), and then output \(\hat{h} = u \odot (\nabla^2 \ell(\theta) u)\), where \(\odot\) denotes the element-wise product, and \(\nabla^2 \ell(\theta) u\) is the HVP of the Hessian with \(u\).</li> <li>Using HVP (from <a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#hessian-vector-products-using-both-forward-and-reverse-mode">JAX</a> or <a href="https://pytorch.org/docs/stable/generated/torch.autograd.functional.hvp.html">Pytorch</a> allows us to efficiently compute the product of the Hessian and a vector without the actual computation of the full Hessian matrix.</li> <li>Effectively, \(\mathbb{E}[\hat h] = \mathrm{diag}(\nabla^2 \ell(\theta))\). <br/><br/></li> <li><strong>How does it work?</strong> The idea for the above comes from the original paper from Hutchinson - “A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines” which was about estimating the trace of a matrix. The idea there was -</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/fig3.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>Dimensionally, and somewhat intuitively, it kinda makes sense to change the dot product to an elementwise product to get the non-aggregated estimate of the diagonal (since trace is the sum of the diagonal elements).</li> <li>This idea was also used in the <a href="https://arxiv.org/abs/2006.00719">“Hessian Diagonal Approximation” section of the AdaHessian paper</a>, and I would also encourage the reader to go through <a href="https://www-users.cse.umn.edu/~saad/PDF/umsi-2005-082.pdf">Section 2 of the paper - “An Estimator for the Diagonal of a Matrix”</a>, which the AdaHessian paper cites as well.</li> </ul> <h3 id="gauss-newton-bartlett-gnb-estimator">Gauss-Newton-Bartlett (GNB) estimator</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/gnb.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>The math for this section is particularly interesting and I would redirect you to my <a href="https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/Sophia%20-%20A%20Scalable%20Stochastic%20Second-order%20Optimizer%20for%20Language%20Model%20Pretraining.pdf">annotated paper</a> where I have mentioned references and also worked out some of the math.</li> <li>I’d highly recommend reading <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L02_Taylor_approximations.pdf">Section 4.4 of these notes</a> and optionally <a href="https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2022/readings/L03_metrics.pdf">Section 4 of these notes</a> (not absolutely required, just helps to understand the footnote in the Sophia paper).</li> <li>Ultimately, the GNB estimator used by the authors is \(B \cdot \nabla_\theta \widehat L(\theta) \odot \nabla_\theta \widehat L(\theta)\) where</li> </ul> \[\\ \begin{align} \widehat L(\theta) = \frac{1}{B}\sum_{b=1}^B \ce(f(\theta, x_b), \hat{y}_b) \end{align} \\\] <p>here \(\hat{y}_b\) are not the labels corresponding to \(x_b\). They are just <strong>sampled labels</strong> from the mini-batch.</p> <ul> <li>The reason we can do that is in the math, which comes from the combination of <ol> <li>The claim in the paper that the second-order derivative of the loss w.r.t. the logits only depends on the logits and the true labels \(y\).</li> <li>Bartlett’s first identity, which generally holds for the negative log-likelihood loss of any probabilistic model and which states - \(\forall b, \mathbb{E}_{\hat{y}_b}\nabla L_\textup{ce}(f(\theta,x_b),\hat{y}_b) = 0\)</li> </ol> </li> <li>Also, the above estimator is an unbiased estimator for the diagonal of the Gauss-Newton matrix, which is a biased estimator for the diagonal of the Hessian.</li> <li>Additional References - <a href="https://www.sfu.ca/~lockhart/richard/830/12_3/Summaries/8.pdf">Bartlett Identities</a></li> </ul> <h3 id="comparison-of-hessian-estimators">Comparison of Hessian estimators</h3> <ul> <li>The Hutchinson’s estimator does not assume any structure of the loss, but requires a Hessian-vector product.</li> <li>The GNB estimator only estimates the Gauss-Newton term but always gives a positive semi-definite (non-negative) diagonal Hessian estimate. The PSDness ensures that the pre-conditioned update is always a descent direction.</li> <li>The Gauss-Newton Matrix is guaranteed to be PSD if the above-mentioned second-order derivative of the loss w.r.t. the logits is PSD. For proof, <a href="https://andrew.gibiansky.com/blog/machine-learning/gauss-newton-matrix/">refer this</a>.</li> </ul> <h1 id="experiments">Experiments</h1> <ul> <li>There are tons of details in the paper regarding the experiments. I’ll just mention the key points here.</li> </ul> <h2 id="experimental-setup">Experimental Setup</h2> <h3 id="baselines">Baselines</h3> <ul> <li>Main comparison with AdamW and Lion</li> <li>AdamW for GPT2 hyperparams - WD = 0.1, \(\beta_1 = 0.9\) and \(\beta_2 = 0.95\)</li> <li>Lion for GPT2 hyperparams - \(\beta_1 = 0.95\) and \(\beta_2 = 0.98\)</li> </ul> <h3 id="implementation">Implementation</h3> <ul> <li>Batch Size = 480</li> <li>Cosine LR schedule with the final LR equal to 0.05 times the peak LR</li> <li>Standard gradient clipping (by norm) threshold 1.0</li> <li>Fixed 2k steps of LR warm-up</li> <li>For Sophia, the authors use \(\beta_1 = 0.96\), \(\beta_2 = 0.99\), \(\epsilon=\) 1e-12 and update diagonal Hessian every 10 steps.</li> <li>Sophia-H (which refers to Sophia with Hutchinson estimator) uses \(\rho=0.01\), and only a subset of 32 examples from the mini-batch to calculate the diagonal Hessian to further reduce overhead.</li> <li>Sophia-G (which refers to Sophia with GNB estimator) uses \(\rho=20\), and use a subset of 240 examples from the mini-batch to calculate the diagonal Gauss-Newton.</li> <li>All models are trained in bfloat16.</li> <li>The 125M and 355M models are trained on A5000 GPUs, while the 770M models are trained on A100 GPUs. Total amount of compute spent on all experiments is about 6000 hours on A100s and 10000 hours on A5000s. This amounts to 4.38e21 FLOPs.</li> </ul> <h1 id="results">Results</h1> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/fig4.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>Sophia consistently achieves better validation loss than AdamW and Lion.</li> <li>As the model size grows, the gap between Sophia and baselines also becomes larger. Sophia-H and Sophia-G both achieve a 0.04 smaller validation loss on the 355M model, Sophia-H achieves a 0.05 smaller validation loss on the 770M model with the same 100k steps.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/fig5.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>According to the scaling laws in this regime, an improvement in loss of 0.05 is equivalent to 2x improvement in terms of number of steps or total compute to achieve the same validation loss</li> <li>Sophia is 2x faster in terms of number of steps, total compute and wall-clock time.</li> <li><strong>The scaling law is in favor of Sophia-H over AdamW</strong> <ul> <li>The 540M model trained by Sophia-H has smaller loss than the 770M model trained by AdamW.</li> <li>The 355M model trained by Sophia-H has comparable loss as the 540M model trained by AdamW.</li> </ul> </li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/fig6.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li><strong>Few-shot Evaluation on Downstream Tasks (SuperGLUE)</strong> <ul> <li>GPT-2 medium and GPT-2 large pre-trained with Sophia have better few-shot accuracy on most subtasks.</li> <li>Models pre-trained with Sophia-H have comparable few-shot accuracy as models pre-trained with AdamW for 2x number of steps.</li> </ul> </li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/fig7.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li><strong>Sensitivity to \(\rho\) and \(\beta_2\), and transferability of hyperparameters</strong> <ul> <li>While performing grid search on hyperparams on a smaller 30M model, the authors found that all combinations have a similar performance. Moreover, this hyperparameter choice is transferable across model sizes. For all the experiments on 125M, 355M and 770M, we use the hyperparameters searched on the 30M model, which is ρ = 0.01, β2 = 0.99.</li> </ul> </li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/fig8.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li><strong>Training Stability</strong> <ul> <li>Gradient clipping (by norm) is an important technique in language model pre-training as it avoids messing up the moment of gradients with one mini-batch gradient computed from rare data.</li> <li>In practice, the frequency that gradients clipping is triggered is related to the training stability - if the gradient is frequently clipped, the iterate can be in a very unstable state.</li> <li>Although all methods use the same clipping threshold 1.0, Sophia-H seldomly triggers gradient clipping, while AdamW and Lion trigger gradient clipping in more than 10% of the steps</li> </ul> </li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sophia_optimizer/fig9.png" alt="Source - &lt;a href='https://arxiv.org/abs/2305.14342'&gt;https://arxiv.org/abs/2305.14342&lt;/a&gt;"/> <figcaption>Source - <a href="https://arxiv.org/abs/2305.14342">https://arxiv.org/abs/2305.14342</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>Another common trick of pre-training deep Transformers is scaling the product of keys and values by the inverse of the layer index [<a href="https://crfm.stanford.edu/2021/08/26/mistral.html">Mistral</a>]. This stabilizes training and increases the largest possible learning rate. <ul> <li>Without this trick, the maximum learning rate of AdamW and Lion on GPT-2 medium (355M) can only be 1.5e-4, which is much smaller than 3e-4 with the trick (the loss will blow up with 3e-4 without the trick). Moreover, the loss decreases much slower without the trick as shown. In all the experiments, Sophia-H does not require scaling the product of keys and values by the inverse of the layer index.</li> </ul> </li> </ul> <h1 id="limitations">Limitations</h1> <ul> <li>Scaling up to larger models and datasets <ul> <li>The paper only experiments with GPT-2 pretraining on OpenWebText for model sizes up to 770M params.</li> <li>Although it is faster and better than Adam and Lion in these set of experiments, and the scaling laws and pre-training stability are encouraging, it remains to be seen how well Sophia scales on larger models.</li> </ul> </li> <li>Holistic downstream evaluation <ul> <li>The paper has only experimented with 4 SuperGLUE tasks and although the results are encouraging, a better downstream evaluation is still important.</li> <li>To note - The limitation in downstream evaluation is also due to the limited model size, because language models at this scale do not have enough capabilities such as in-context learning, and mathematical reasoning.</li> </ul> </li> <li>Evaluation on other domains <ul> <li>This paper focuses on optimizers for LLMs, it should be evaluated in other domains like CV, RL and Multimodal tasks.</li> </ul> </li> </ul> <hr/> <p>A very exciting paper. Hope people can test it out on even bigger models and across multiple domains and we may potentially have an optimizer finally dethroning Adam!</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="Deep Learning"/><category term="ML Theory"/><category term="transformers"/><category term="optimizer"/><category term="deep-learning"/><category term="paper-summaries"/><summary type="html"><![CDATA[Understanding Sophia - A new fast, scalable second-order optimizer which beats Adam on LLM pretraining.]]></summary></entry><entry><title type="html">Paper Summary #8 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2023-03-26_flash-attention/" rel="alternate" type="text/html" title="Paper Summary #8 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"/><published>2023-03-26T00:00:00+00:00</published><updated>2023-03-26T00:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/flash-attention</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2023-03-26_flash-attention/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Paper</strong>: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness<br/> <strong>Link</strong>: <a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a><br/> <strong>Authors</strong>: Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré<br/> <strong>Code</strong>: <a href="https://github.com/HazyResearch/flash-attention">https://github.com/HazyResearch/flash-attention</a></p> <hr/> <p><strong>I have also released an annotated version of the paper. You can find it <a href="https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/MLSys/FlashAttention%20-%20Fast%20and%20Memory-Efficient%20Exact%20Attention.pdf">here</a>.</strong></p> <p><strong>[Update] - I implemented a simplified version of FlashAttention (without the CUDA and SRAM memory optimizations) in PyTorch. <a href="https://github.com/shreyansh26/FlashAttention-PyTorch">Check it out on Github.</a></strong></p> <p>I finished reading the FlashAttention paper recently and thought that it would be good to have a technical write-up of the paper, so that it can help me understand the concept well. I decided to make it public and hopefully it can help anyone reading this.</p> <h2 id="overview">Overview</h2> <p>Attention as we know, in its standard implementation is an \(O(N^2)\) operation, where N is the sequence length. There are many approximate attention methods out there like Reformer, SMYRF, Performer and others (<a href="https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/">you can find more details on a few of these in my previous blog</a>) which aim to reduce the compute requirements to linear or near-linear in sequence length, but many of them do not display wall-clock speedup against standard attention. They focus on FLOP reduction (which doesn’t always correlate with wall-clock speed) and tend to ignore overheads from memory access (IO). FlashAttention aims to incorporate IO-awareness i.e. dividing operations between faster and slower levels of GPU memory to make the whole computation faster. The algorithm uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. FlashAttention can also be extended to block-spare attention and this results in the fastest approximate (or not) attention algorithm out there.</p> <p>All this helps to improve the training time of Transformer models - a 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3× speedup on GPT-2 (seq. length 1K). This memory-efficient approach also helps to incorporate a longer context (up to 16k/64k tokens) which also results in better models (0.7 better perplexity on GPT-2).</p> <p>I’ll describe more details in the future sections.</p> <h2 id="background---hardware-performance">Background - Hardware Performance</h2> <p>Since FlashAttention computes exact attention, and the major crux of their work is the efficient hardware usage, it is important to know a bit about GPU memory and the performance characteristics of various kinds of operations on it.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/gpu_mem.png" alt="A100 GPU Memory Hierarchy. Source - &lt;a href='https://arxiv.org/abs/2205.14135'&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;"/> <figcaption>A100 GPU Memory Hierarchy. Source - <a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="gpu-memory-hierarchy">GPU Memory Hierarchy</h3> <p>For a A100 GPU with 40GB of High Memory Bandwidth (HBM), a rough diagram of the memory hierarchy is shown above. The SRAM memory us spread across 108 streaming multiprocessors (SMs), 192KB for each. As one can see, the on-chip SRAM is much faster the HBM but is much smaller than size. In terms of compute, the theoretical peak throughput for BFloat16 using Tensor Core is 312 TFLOPS. With time, compute has gotten much faster relative to memory speed, hence processes (operations) are increasingly bottlenecked by memory (HBM) access. Thus, the goal of the FlashAttention paper was to use the SRAM as well as efficiently as possible to speed up the computation.</p> <h3 id="execution-model">Execution Model</h3> <p>The typical way in which GPUs operate are that they use a large number of threads to perform an operation, which is called a kernel. The input is loaded from the HBM to the registers and SRAM, and written back to the HBM after computation.</p> <h3 id="performance-characteristics">Performance Characteristics</h3> <p>There is a term called <strong>arithmetic intensity</strong> which is given by the number of arithmetic operations per byte of memory access. It helps to understand the bottleneck of an operation. An operation can be characterized as compute-bound (also called math-bound) or memory-bound.</p> <ul> <li> <p><strong>Compute-bound</strong> - When the bottleneck is the compute i.e., the time taken by the operation is determined by how many arithmetic operations there are since the time taken due to HBM accesses is relative lower. E.g. of such operations are matrix multiplication with large inner dimension, and convolution with large number of channels.</p> </li> <li> <p><strong>Memory-bound</strong> - When the bottleneck is the memory i.e., the time taken by the operation is determined by the number of memory accesses there are since the time spent in computation is relative lower. E.g. of such processes are most other operation like elementwise operations - activation, dropout and reduction operations - sum, softmax, batch normalization, layer normalization.</p> </li> </ul> <p>To understand this better, let’s analyze it mathematically. Let \(N_{op}\) be the number of arithmetic/floating point operations, \(N_{byte}\) be the number of memory accesses, \({BW}_{compute}\) and \({BW}_{memory}\) be the compute and memory bandwidth respectively, the time taken for compute operations and memory accesses can be determined as -</p> \[\\ \begin{align} t_{compute} = \frac{N_{op}}{BW_{compute}} \\ t_{memory} = \frac{N_{byte}}{BW_{memory}} \end{align} \\\] <p>The operation is compute-bound if \(t_{compute}\) is greater than \(t_{memory}\) and vice-versa for memory bound. Which mathematically becomes -</p> <p><strong>For compute-bound</strong><br/> \(\\ \begin{align} \frac{N_{op}}{N_{byte}} \gt \frac{BW_{compute}}{BW_{memory}} \end{align} \\\)</p> <p><strong>For memory-bound</strong><br/> \(\\ \begin{align} \frac{N_{op}}{N_{byte}} \lt \frac{BW_{compute}}{BW_{memory}} \end{align} \\\)</p> <p>As mentioned above as well, matrix multiplication for large inner dimensions is compute bound but below that it is memory bound. If using FP32 and plugging in numbers for A100 40GB, then for \(N \lt 74\), the \(N \times N\) multiplication is memory bound, but compute bound when \(N\) is greater than that. A great and detailed resource to understand this theory is this <a href="https://leimao.github.io/blog/Math-Bound-VS-Memory-Bound-Operations/">blog post by Lei Mao</a>.</p> <h3 id="kernel-fusion">Kernel Fusion</h3> <p>Kernel Fusion is often down by compilers to fuse together multiple elementwise operations. It is used to accelerate memory-bound operations. The basic ideas is that instead of loading the input from the HBM, performing the operation and writing back to the HBM and repeating that for each operation applied to the same input, the operation can be fused so that all of the operations are performed at once when the input is loaded from the HBM.</p> <p>However, one must note that when performing model training, the effectiveness of kernel fusion is reduced as the intermediate values still have to be written to the HBM to save for the backward pass.</p> <h2 id="background---standard-attention">Background - Standard Attention</h2> <p>For anyone familiar with transformers, this equation is well-known -</p> \[Attention(Q, K, V) = softmax(\frac{QK^\mathsf{T}}{\sqrt{d_k}})V\] <p>Here, the sequences \(Q, K, V \in \mathbb{R}^{N \times d}\) where \(N\) is the sequence length and \(d\) is the head dimension. The attention output, above, can be denoted by \(O \in \mathbb{R}^{N \times d}\). The equation can be broken down as -</p> \[\mathbf{S} = \mathbf{QK^\mathsf{T}} \in \mathbb{R}^{N \times N},\quad \mathbf{P} = softmax(\mathbf{S}) \in \mathbb{R}^{N \times N},\quad \mathbf{O} = \mathbf{PV} \in \mathbb{R}^{N \times d}\] <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/att2.png" alt="Scaled Dot Product Attention"/> <figcaption>Scaled Dot Product Attention</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>In standard attention implementations, the \(\mathbf{S}\) and \(\mathbf{P}\) matrices are materialized in the HBM, which takes \(O(N^2)\) memory. Also, most operations are memory-bound/elementwise operations, e.g. softmax applied on \(\mathbf{P}\), masking applied to \(\mathbf{S}\), dropout applied to \(\mathbf{P}\). This leads to slow wall-clock time.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/standard-att-algo.png" alt="Standard Attention Implementation"/> <figcaption>Standard Attention Implementation</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="flashattention---algorithm-details">FlashAttention - Algorithm details</h2> <p>As one may understand, the materialization of the \(N \times N\) attention matrix on the HBM and its repeated reading and writing is a major bottleneck. To solve this, two main things need to be done -</p> <ol> <li>Computing the softmax reduction without access to the whole input</li> <li>Not storing the large intermediate attention matrix for the backward pass</li> </ol> <p>Two established techniques, namely <strong>tiling</strong> and <strong>recomputation</strong> are used to solve this.</p> <ol> <li>Tiling - The attention computation is restructured to split the input into blocks and performing the softmax operation incrementally by making several passes over the input blocks.</li> <li>Recomputation - The softmax normalization factor from the forward pass is stored to quickly recompute attention on-chip in the backward pass, which is faster than the standard attention approach of reading the intermediate matrix from HBM.</li> </ol> <p>This does lead to increased FLOPs due to recomputation, however FlashAttention runs both faster (up to 7.6x on GPT-2) and uses less memory — linear in sequence length, due to the massively reduced amount of HBM access.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/gpt2-att-speedup.png" alt="Speedup over the PyTorch implementation of attention on GPT-2"/> <figcaption>Speedup over the PyTorch implementation of attention on GPT-2</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="understanding-the-algorithm">Understanding the algorithm</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/flash-attention-schematic.png" alt="FlashAttention Forward Pass Algorithm"/> <figcaption>FlashAttention Forward Pass Algorithm</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The main idea behind the algorithm is to split the inputs \(\mathbf{Q, K, V}\) into blocks, loading them from slow HBM to fast SRAM and then computing the attention output w.r.t those blocks. The output of each block is scaled by the right normalization factor before adding them up, which gives the correct result.</p> \[\mathbf{S} = \mathbf{\tau QK^\mathsf{T}} \in \mathbb{R}^{N \times N},\quad \mathbf{S}^\mathrm{masked} = \mathrm{MASK}(S) \in \mathbb{R}^{N \times N},\quad \mathbf{P} = softmax(\mathbf{S^\mathrm{masked}}) \in \mathbb{R}^{N \times N},\] \[\mathbf{P}^\mathrm{dropped} = \mathrm{dropout}(\mathbf{P}, p_\mathrm{drop}), \quad \mathbf{O} = \mathbf{P^\mathrm{dropped}V} \in \mathbb{R}^{N \times d},\] <p>where \(\tau \in \mathbb{R}\) is some softmax scaling factor (typically \(\frac{1}{\sqrt{d}}\)), \(\mathrm{MASK}\) is some masking function that sets some entries of the input to \(-\infty\) and keep other entries the same, and \(\mathrm{dropout}(x, p)\) applies dropout to 𝑥 elementwise (i.e., output \(\frac{x}{1-p}\) with probability \(1 − p\) and output \(0\) with probability \(p\) for each element \(x\))</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/flash-attention-forward-algo.png" alt="FlashAttention Forward Pass Algorithm"/> <figcaption>FlashAttention Forward Pass Algorithm</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h4 id="tiling">Tiling</h4> <p>The key part in understanding the block-wise computation of attention in the algorithm above is the block-wise computation of the softmax. The paper explains it well though. The softmax of a vector \(x \in \mathbb{R}^B\) can be computed as -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/softmax-1.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>And for vectors \(x^\mathrm{(1)}, x^\mathrm{(2)} \in \mathbb{R}^B\), the softmax of the concatenated \(x = [x^\mathrm{(1)}, x^\mathrm{(2)}] \in \mathbb{R}^{2B}\) is given by -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/softmax-2.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Let’s understand this better. In the above equations, \(m(x)\) holds the maximum between \(m(x^\mathrm{(1)})\) and \(m(x^\mathrm{(2)})\). Now, \(m(x^\mathrm{(1)})\) is the maximum element of \(x^\mathrm{(1)}\) and \(m(x^\mathrm{(2)})\) is the maximum element of \(x^\mathrm{(2)}\) which means that \(m(x)\) is basically the maximum of the whole concatenated vector. The beauty is that this was done blockwise.</p> <p>So, if statistics \((m(x), l(x))\) are tracked then softmax can be computed one block at a time. In line 12 of the algorithm, \(\tilde{m_{ij}}\) has the maximum element of each row of \(S_{ij}^\mathrm{masked}\), and next in line 13, \(m_i^\mathrm{new}\) holds the row-wise maximum of the \(m_i\) till now and the new one i.e., \(\tilde{m_{ij}}\). Hence \(m_i\) is updated every column from the outer loop and eventually stores the row-wise max of the matrix \(\mathbf{S}\). The same logic goes for \(l_i\) and the matrix \(\mathbf{P}\). The results are combined to get the output attention matrix in line 15.</p> <h4 id="recomputation">Recomputation</h4> <p>The backward pass of FlashAttention requires the \(\mathbf{S}\) and \(\mathbf{P}\) matrices to compute the gradients w.r.t \(\mathbf{Q}\), \(\mathbf{K}\), \(\mathbf{V}\). However, they are \(N \times N\) matrices and as it can be seen in the algorithm above, they aren’t stored explicitly. The trick is to use the output \(\mathbf{O}\) and the softmax normalization statistics \((m, l)\), we can recompute the attention matrix \(\mathbf{S}\) and \(\mathbf{P}\) easily in the backward pass from blocks of \(\mathbf{Q}\), \(\mathbf{K}\), \(\mathbf{V}\) in SRAM. even with more FLOPs, the recomputation step speeds up the backward pass due to reduced HBM accesses. The backward pass is very interesting too but slightly more complicated hence I’ll probably cover it in a separate post. One can cover the Appendix B of the paper to learn more.</p> <p>Kernel Fusion is also used to implement the algorithm in one CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then writing the result back to HBM. This avoids repeatedly reading and writing of inputs and outputs from and to HBM.</p> <p><strong>Important Information</strong> - <em>The FlashAttention algorithm computed \(\mathbf{O} = softmax(QK^\mathsf{T})V\) with \(O(N^2d)\) FLOPs and requires \(O(N)\) additional memory beyond inputs and output (for the \((l, m)\) statistics).</em></p> <p>The proof for the FLOPs calculation is given in Appendix C of the paper, which should be checked out by the curious reader.</p> <p><strong>Important Information</strong> - <em>Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be the size of SRAM with \(d \leq M \leq Nd\). Standard attention requires \(\Theta(Nd + N^2)\) HBM accesses while FlashAttention requires \(\Theta(N^2d^2M^{-1})\) HBM accesses.</em></p> <p>For typical values of \(d\) (64-128) and \(M\) (around 100KB), \(d^2\) is many times smaller than \(M\), and thus FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to both faster execution and a lower memory footprint.</p> <p>The authors also go on to show that the number of HBM accesses by FlashAttention is a lower-bound. There can be no implementation which can asymptotically improve on the number of HBM accesses for all values of \(M\) when doing exact attention calculation.</p> <p>As the block size increases, the number of HBM accesses decreases as there are less passes over the input, and the runtime also decreases. However, beyond 256, the runtime starts getting bottlenecked by factors like arithmetic operations. And there is also a limit on how large we can choose the block size to be, as we want it to be able to fit in the SRAM.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/res-1.png" alt="&lt;strong&gt;Left&lt;/strong&gt; - Comparison of standard attention and FlashAttention for GPT-2 medium on A100. Despite the higher FLOPs (due to the recomputation step in backward pass), the lesser number of HBM access leads to a much faster runtime. &lt;strong&gt;Right&lt;/strong&gt; - The effect of block size on the forward runtime and HBM accesses."/> <figcaption><strong>Left</strong> - Comparison of standard attention and FlashAttention for GPT-2 medium on A100. Despite the higher FLOPs (due to the recomputation step in backward pass), the lesser number of HBM access leads to a much faster runtime. <strong>Right</strong> - The effect of block size on the forward runtime and HBM accesses.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="block-sparse-flashattention">Block-Sparse FlashAttention</h3> <p>As mentioned in the overview, FlashAttention can be used to make a approximate attention algorithm as well. The authors call it Block-Sparse FlashAttention and it is the fastest approximate attention algorithm. The memory complexity is smaller than FlashAttention by a factor proportional to the sparsity.</p> <p>For inputs \(\mathbf{Q, K, V} \in \mathbb{R}^{N \times d}\) and a mask \(\tilde{\mathbf{M}} \in \{ 0,1 \}^{N \times N}\), we want to calculate -</p> \[\mathbf{S} = \mathbf{QK^\mathsf{T}} \in \mathbb{R}^{N \times N},\quad \mathbf{P} = softmax(\mathbf{S} \odot \mathbb{1}_{\tilde{\mathbf{\mathrm{M}}}}) \in \mathbb{R}^{N \times N},\quad \mathbf{O} = \mathbf{PV} \in \mathbb{R}^{N \times d}\] <p>Given a pre-defined block sparsity mask \(\mathbf{M} \in \{ 0,1 \}^{N/B_r \times N/B_c}\), Algorithm 2 above can be adapted to only compute the nonzero blocks of the attention matrix. We can just skip the zero blocks. The Algorithm shown below describes the forward pass of Block-sparse FlashAttention.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/blocksparse-flash-attention-forward-algo.png" alt="Blcok-Sparse FlashAttention Forward Pass Algorithm"/> <figcaption>Blcok-Sparse FlashAttention Forward Pass Algorithm</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Important Information</strong> - <em>Let \(N\) be the sequence length, \(d\) be the head dimension, and \(M\) be the size of SRAM with \(d \leq M \leq Nd\). Block-sparse FlashAttention requires \(\Theta(Nd + N^2d^2M^{-1}s)\) HBM accesses where \(s\) is the fraction of nonzero blocks in the block-sparsity mask.</em></p> <p>For large sequence lengths, \(s\) is set to \(N^{-1/2}\) or \(N^{-1} \log N\) resulting in \(\Theta(N \sqrt{N})\) or \(\Theta(N \log N)\) IO complexity. As the sparsity increases, the runtime of block-sparse FlashAttention improves proportionally.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/res-2.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="experiments">Experiments</h2> <p>There are tons of results in the paper. But the TL;DR is that FlashAttention beats all other exact attention algorithms in both training speed and quality of the models/down stream models especially when pushed to the limits of sequence length. I’ll add the plots and graphs for their various results here. Additional results are present in the paper.</p> <h3 id="training-speed">Training Speed</h3> <h4 id="bert">BERT</h4> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/res-3.png" alt="Training time of BERT-large. starting from the same initialization provided by the MLPerf benchmark, to reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8×A100 GPUs."/> <figcaption>Training time of BERT-large. starting from the same initialization provided by the MLPerf benchmark, to reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8×A100 GPUs.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h4 id="gpt-2">GPT-2</h4> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/res-4.png" alt="GPT-2 small and medium using FlashAttention achieve up to 3× speed up compared to Huggingface implementation and up to 1.7× compared to Megatron-LM. Training time reported on 8×A100s GPUs."/> <figcaption>GPT-2 small and medium using FlashAttention achieve up to 3× speed up compared to Huggingface implementation and up to 1.7× compared to Megatron-LM. Training time reported on 8×A100s GPUs.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h4 id="long-range-arena">Long-range Arena</h4> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/res-5.png" alt="The performance of standard attention, FlashAttention, block-sparse FlashAttention, and approximate attention baselines on the Long-Range-Arena benchmarks. Each task has a different sequence length varying between 1024 and 4096."/> <figcaption>The performance of standard attention, FlashAttention, block-sparse FlashAttention, and approximate attention baselines on the Long-Range-Arena benchmarks. Each task has a different sequence length varying between 1024 and 4096.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Block-sparse FlashAttention is faster than all of the approximate attention methods that were tested.</p> <h3 id="model-quality">Model Quality</h3> <h4 id="language-modeling-with-long-context">Language Modeling with Long Context</h4> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/res-6.png" alt="GPT-2 small with FlashAttention, with 4× larger context length compared to Megatron-LM, is still 30% faster while achieving 0.7 better perplexity. Training time on 8×A100 GPUs is reported."/> <figcaption>GPT-2 small with FlashAttention, with 4× larger context length compared to Megatron-LM, is still 30% faster while achieving 0.7 better perplexity. Training time on 8×A100 GPUs is reported.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h4 id="long-document-classification">Long Document Classification</h4> <p>Since FlashAttention allows training on longer sequences, it improves performance on such datasets. MIMIC-III contains intensive care unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights that were allegedly violated. Both of these datasets contain very long text documents. The average number of tokens in MIMIC-III is 2395 tokens and the longest document contains 14562 tokens.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/res-7.png" alt="Sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length 8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language."/> <figcaption>Sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length 8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h4 id="path-x-and-path-256">Path-X and Path-256</h4> <p>These are challenging tasks from the long range arena benchmark where the task is to classify whether two points in a black and white 128×128 (or 256×256) image have a path connecting them, and the images are fed to the transformer one pixel at a time. No transformer model in the past has been able to model these tasks effectively. They have either ran out of memory or achieved random performance. FlashAttention yields the first Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the first sequence model that can achieve better-than-random performance on Path-256 (sequence length 64K).</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/res-8.png" alt="First Transformer model that can achieve non-random performance on Path-X and Path-256. Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy."/> <figcaption>First Transformer model that can achieve non-random performance on Path-X and Path-256. Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="benchmarking-attention">Benchmarking Attention</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/flash_attention/res-9.png" alt="**Left** - runtime of forward pass + backward pass. **Right** - attention memory usage"/> <figcaption>**Left** - runtime of forward pass + backward pass. **Right** - attention memory usage</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h4 id="runtime">Runtime</h4> <p>FlashAttention beats all exact attention baselines and is about 3× faster than the PyTorch implementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with sequence length, but FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with FlashAttention at sequences between 512 and 1024. On the other hand, block-sparse FlashAttention is faster than all implementations of exact, sparse, and approximate attention that are available, across all sequence lengths.</p> <h4 id="memory-footprint">Memory Footprint</h4> <p>FlashAttention and block-sparse FlashAttention have the same memory footprint, which grows linearly with sequence length. FlashAttention is up to 20× more memory efficient than exact attention baselines, and is more memory-efficient than the approximate attention baselines. All other algorithms except for Linformer run out of memory on an A100 GPU before 64K, and FlashAttention is still 2× more efficient than Linformer.</p> <hr/> <p>A great paper overall, tremendous impact and personally, I had loads to learn from it!</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="MLSys"/><category term="mlsys"/><category term="transformers"/><category term="efficiency"/><category term="attention"/><category term="paper-summaries"/><summary type="html"><![CDATA[Understanding FlashAttention which is the most efficient exact attention implementation out there, which optimizes for both memory requirements and wall-clock time.]]></summary></entry><entry><title type="html">Paper Summary #7 - Efficient Transformers: A Survey</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2022-10-10_efficient_transformers_survey/" rel="alternate" type="text/html" title="Paper Summary #7 - Efficient Transformers: A Survey"/><published>2022-10-10T00:00:00+00:00</published><updated>2022-10-10T00:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/efficient-transformers</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2022-10-10_efficient_transformers_survey/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Paper</strong>: Efficient Transformers: A Survey<br/> <strong>Link</strong>: <a href="https://arxiv.org/abs/2009.06732">https://arxiv.org/abs/2009.06732</a><br/> <strong>Authors</strong>: Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler</p> <hr/> <p>I wanted to summarize this paper for a long time now because of the immense amount of information in this paper. Thanks to the <a href="https://cohere.for.ai/">Cohere For AI</a> community for having a session on this paper which made me revisit this.</p> <h1 id="what">What?</h1> <p>This is a survey paper on the various memory-efficiency based improvements on the original Transformers architecture by Vaswani et al. But wait, for those unaware, how is the Transformers architecture inefficient?</p> <ul> <li>The attention operation has a quadratic complexity over the sequence length L, also sometimes represented using N (since each token attends to other set of tokens in the sequence)</li> <li>The Attention operation of \(QK^T\) uses \(N^2\) time and memory. Here (in no-batching case) $Q, K, V$ (query, key and value matrices) have dimensions \(N \times d\) where \(d\) is the dimension of query, key and value vectors.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/attention.PNG" alt="Attention calculation"/> <figcaption>Attention calculation</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h1 id="memory-efficient-transformers">Memory-Efficient Transformers</h1> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/arch-vaswani.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/summary.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="low-rank-methods">Low-Rank methods</h2> <h3 id="linformer---httpsarxivorgabs200604768">Linformer - <a href="https://arxiv.org/abs/2006.04768">https://arxiv.org/abs/2006.04768</a></h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/linformer.png" alt="Left and bottom-right show architecture and example of our proposed multihead linear self-attention. Top right shows inference time vs. sequence length for various Linformer models."/> <figcaption>Left and bottom-right show architecture and example of our proposed multihead linear self-attention. Top right shows inference time vs. sequence length for various Linformer models.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>In Linformer, the original Key and Value matrices are projected from \(N \times d\) to a reduced \(k \times d\).</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/linformer-dets.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The above operations only require \(O(N * k)\) time and space complexity. Thus, if we can choose a very small projected dimension \(k\), such that \(k \lt\lt N\), then we can significantly reduce the memory and space consumption.</p> <h3 id="performer---httpsarxivorgabs200914794">Performer - <a href="https://arxiv.org/abs/2009.14794">https://arxiv.org/abs/2009.14794</a></h3> <p>The goal in the Performer paper was to reduce the complexity of attention calculation \((QK^T)V\) of \(O(L^2 * d)\) to \(O(L * d^2)\) by transforming the order of operations and using a kernel operation to approximate the softmax operation so that the order of operations can be changed.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/performer.png" alt="An overview from the paper"/> <figcaption>An overview from the paper</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>From, a <a href="https://chiaracampagnola.io/2020/10/29/from-transformers-to-performers/">great blog on the Performer paper</a> -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/performer-dets.png" alt="Change of operation order"/> <figcaption>Change of operation order</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/performer-dets2.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/performer-dets3.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="learnable-patterns">Learnable Patterns</h2> <h3 id="clustered-attention---httpsarxivorgabs200704825--httpsclustered-transformersgithubioblog">Clustered Attention - <a href="https://arxiv.org/abs/2007.04825">https://arxiv.org/abs/2007.04825</a> + <a href="https://clustered-transformers.github.io/blog/">https://clustered-transformers.github.io/blog/</a></h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/clusteredatt.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>First cluster the queries into non-overlapping clusters.</li> <li>Attention weights \(A^c\) are computed using the centroids instead of computing them for every query</li> <li>Use clustered attention weights \(A^c\) to compute new Values \(V^c\)</li> <li>Use the same attention weights and new values for queries that belong to same cluster.</li> <li>Computational complexity becomes \(O(N * C * max(D_k, D_v))\)</li> </ul> <p>They also propose an Improved Clustered Attention in their blog. The complexity comaprisons are here -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/clusteredatt-dets.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="reformer---httpsarxivorgabs200104451">Reformer - <a href="https://arxiv.org/abs/2001.04451">https://arxiv.org/abs/2001.04451</a></h3> <p>Uses the concept of Locality sensitive hashing (LSH) attention, where the goal is to not store the entire \(QK^T\) matrix but only the \(\textrm{softmax}(QK^T)\), which is dominated by the largest elements in a typically sparse matrix. For each query \(q\) we only need to pay attention to the keys \(k\) that are closest to \(q\). For example, if \(k\) is of length 64K, for each \(q\) we could only consider a small subset of the 32 or 64 closest keys. So the attention mechanism finds the nearest neighbor keys of a query but in an inefficient manner.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/reformer-lsh.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>Calcluate LSH hashes of Queries and Keys (\(Q\) and \(K\))</li> <li>Make chunks and compute attention only for vectors in the same bucket</li> </ul> <p>The paper also introduces the concept of Reversible residual networks (RevNets). In the residual connections in Transformers, one needs to store the activations in each layer in memory in order to calculate gradients during backpropagation. RevNets are composed of a series of reversible blocks. In RevNet, each layer’s activations can be reconstructed exactly from the subsequent layer’s activations, which enables us to perform backpropagation without storing the activations in memory.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/reformer-revnet.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Reformer applies the RevNet idea to the Transformer by combining the attention and feed-forward layers inside the RevNet block. Now F becomes an attention layer and G becomes the feed-forward layer:</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/reformer-revnet2.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The reversible residual layers allows storing activations only once during the training process instead of N times.</p> <p>The memory complexity of Reformer is \(O(N * log(N))\).</p> <h2 id="memory-based">Memory-based</h2> <h3 id="big-bird-httpsarxivorgabs200714062--httpshuggingfacecoblogbig-bird">Big Bird <a href="https://arxiv.org/abs/2007.14062">https://arxiv.org/abs/2007.14062</a> + <a href="https://huggingface.co/blog/big-bird">https://huggingface.co/blog/big-bird</a></h3> <p>BigBird relies on block sparse attention and can handle sequences up to a length of 4096 at a much lower computational cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.</p> <p>BigBird proposes three ways of allowing long-term attention dependencies while staying computationally efficient -</p> <ul> <li><strong>Global attention</strong> - Introduce some tokens which will attend to every token and which are attended by every token. The authors call this the ‘internal transformer construction (ITC)’ in which a subset of indices is selected as global tokens. This can be interpreted as a model-memory-based approach.</li> <li><strong>Sliding attention</strong> - Tokens close to each other, attend together. In BigBird, each query attends to \(w/2\) tokens to the left and \(w/2\) tokens to the right. This corresponds to a fixed pattern (FP) approach.</li> <li><strong>Random attention</strong> - Select some tokens randomly which will transfer information by transferring to other tokens which in turn can transfer to other tokens. This may reduce the cost of information travel from one token to other. Each query attends to r random keys. This pattern is fixed</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/bigbird-graph.gif" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>BigBird block sparse attention is a combination of sliding, global &amp; random connections (total 10 connections) as shown in gif above. While a graph of normal attention (bottom) will have all 15 connections (note: total 6 nodes are present). One can simply think of normal attention as all the tokens attending globally.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/bigbird-full.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The attention calculation in BigBird is slightly complex and I would refer to the <a href="https://huggingface.co/blog/big-bird#bigbird-block-sparse-attention">Huggingface blog</a> for it -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/bigbird-attention-gif.gif" alt="blue -&gt; global blocks, red -&gt; random blocks, orange -&gt; sliding blocks"/> <figcaption>blue -&gt; global blocks, red -&gt; random blocks, orange -&gt; sliding blocks</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The memory complexity of the self-attention is linear, i.e., \(O(N)\). The BigBird model does not introduce new parameters beyond the Transformer model.</p> <h2 id="complexity-summary-of-various-models">Complexity Summary of various models</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/efficient_transformers/complexity-summary.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr/> <p>There are many more papers discussed in the survey. I will add their summaries here as I go through them.</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="MLSys"/><category term="LLMs"/><category term="mlsys"/><category term="transformers"/><category term="efficiency"/><category term="attention"/><category term="paper-summaries"/><summary type="html"><![CDATA[A survey paper of improvements over the original Transformer architecture in terms of memory-efficiency.]]></summary></entry><entry><title type="html">Deploying Machine Learning models using GCP’s Google AI Platform - A Detailed Tutorial</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2022-03-06_model_deployment_using_gcp_google_ai_platform/" rel="alternate" type="text/html" title="Deploying Machine Learning models using GCP’s Google AI Platform - A Detailed Tutorial"/><published>2022-03-06T00:00:00+00:00</published><updated>2022-03-06T00:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/model-deployment-gcp</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2022-03-06_model_deployment_using_gcp_google_ai_platform/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr/> <p>In my <a href="https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/">last post</a> I had written about deploying models on AWS. So, I though it would only be fitting to write one for GCP, for all the GCP lovers out there.</p> <p>GCP has a service called the AI Platform which, as the name suggest, is responsible for training and hosting ML/AI models.</p> <p>Just like the last post, this post, through a PoC, describes -</p> <ol> <li>How to add a trained model to a Google Cloud bucket</li> <li>Host the saved model on the AI Platform</li> <li>Create a Service Account to use the model hosted on AI Platform externally</li> <li>Make a Streamlit app to make a UI to access the hosted model</li> </ol> <p><strong>All the code can be found in my <a href="https://github.com/shreyansh26/Iris_classification-GCP-AI-Platform-PoC">Github repository</a>.</strong></p> <p>The repository also contains the code to train, save and test a simple ML model on the Iris Dataset.</p> <p>The Iris dataset is a small dataset which contains attributes of the flower - Sepal length, Sepal width, Petal length and Petal width. The goal of the task is to classify based on these dimensions, the type of the Iris, which in the dataset is among three classes - Setosa, Versicolour and Virginica.</p> <h2 id="package-requirements">Package Requirements</h2> <ul> <li>A Google Cloud account and a Google Cloud Project (using GCP will cause money if you don’t have any of the free $300 credits you get when you first sign up)</li> <li>Python 3.6+</li> <li>A simple <code class="language-plaintext highlighter-rouge">pip install -r requirements.txt</code> from the <a href="https://github.com/shreyansh26/Iris_classification-GCP-AI-Platform-PoC/tree/master/iris_classification">iris_classification</a> directory will install the other Python packages required.</li> </ul> <h2 id="steps-to-follow">Steps to follow</h2> <p>In this PoC, I will be training and deploying a simple ML model. If you follow this tutorial, deploying complex models should be fairly easy as well.</p> <h3 id="1-training-and-deploying-the-model-locally">1. Training and Deploying the model locally</h3> <ol> <li>Clone this repo <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/shreyansh26/Iris_classification-GCP-AI-Platform-PoC
</code></pre></div> </div> </li> <li>Create a virtual environment - I use <a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda</a>, but you can use any method (virtualenv, venv) <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create -n iris_project python=3.8
conda activate iris_project
</code></pre></div> </div> </li> <li>Install the required dependencies <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install -r requirements.txt
</code></pre></div> </div> </li> <li>Train the model <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd iris_classification/src
python train.py
</code></pre></div> </div> </li> <li>Verify the model trained correctly using pytest <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pytest
</code></pre></div> </div> </li> <li>Activate Streamlit and run <code class="language-plaintext highlighter-rouge">app.py</code> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>streamlit run app.py
</code></pre></div> </div> </li> </ol> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/ini-streamlit.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Right now, the <code class="language-plaintext highlighter-rouge">Predict GCP</code> button will give an error on clicking. It requires a json configuration file which we will obtain when we deploy our model. To get the <code class="language-plaintext highlighter-rouge">Predict AWS</code> button working for your model, refer to a separate <a href="https://shreyansh26.github.io/post/2021-12-28_model_deployment_using_aws_lambda/">tutorial</a> I made on that.</p> <h3 id="2-storing-the-model-in-a-gcp-bucket">2. Storing the model in a GCP Bucket</h3> <p>The saved <code class="language-plaintext highlighter-rouge">model.pkl</code> has to be stored in a Google Storage Bucket. First, create a bucket.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/gcp-bucket.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The rest of the inputs can be kept as default.</p> <p>And then upload the <code class="language-plaintext highlighter-rouge">model.pkl</code> to the bucket.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/bucket-upload.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <h3 id="3-hosting-the-model-on-ai-platform">3. Hosting the model on AI Platform</h3> <p>Using the AI Platform, we need to create a model</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/aiplatform-models.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/aiplatfrom-create.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Next, create a version of the model.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/version.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Choose the bucket location which has the <code class="language-plaintext highlighter-rouge">model.pkl</code> file.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/version2.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/version3.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The model will take some time to be hosted.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/version4.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="4-creating-a-service-account">4. Creating a Service Account</h3> <p>Finally, head to <code class="language-plaintext highlighter-rouge">IAM -&gt; Service Accounts</code> and add a Service Account which basically allows to use the model hosted on AI Platform externally.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/service.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Next, select <code class="language-plaintext highlighter-rouge">AI Platform Developer</code> as the role and click <code class="language-plaintext highlighter-rouge">Done</code>.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/service2.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Now, in the <code class="language-plaintext highlighter-rouge">Service Accounts</code> console, we see that there are no keys. Yet.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/service3.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>We go to <code class="language-plaintext highlighter-rouge">Manage Keys</code></p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/service4.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Creating the key downloads a JSON file which basically has the key our code will be using.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/service5.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The following configurations should be updated in the <code class="language-plaintext highlighter-rouge">app.py</code> file.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/code.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="testing-the-hosted-model">Testing the hosted model</h2> <p>After making the appropriate changes to the configuration, running</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>streamlit run app.py
</code></pre></div></div> <p>allows you to get the predictions from the GCP hosted model as well.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_gcp/fin-streamlit.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="and-we-are-done">AND WE ARE DONE!</h3> <p>Hope this gives you a good idea on how to deploy ML models on GCP. Obviously, there can be extensions which can be done.</p> <ul> <li>Github Actions could be used to automate the whole deployment process.</li> <li>Google App Engine could be used to deploy and host the Streamlit app.</li> </ul> <hr/> <p>That’s all for now! I hope this tutorial helps you deploy your own models to Google Cloud Platform easily. Make sure to read the pricing for each GCP product (if you are not using the initial free credits) you use to avoid being charged unknowingly.</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="MLOps"/><category term="model-deployment"/><category term="gcp"/><category term="streamlit"/><summary type="html"><![CDATA[A step-wise tutorial to demonstrate the steps required to deploy a ML model using GCP, specifically the Google AI Platform and use Streamlit to access the model through a UI.]]></summary></entry><entry><title type="html">Deploying Machine Learning models using AWS Lambda and Github Actions - A Detailed Tutorial</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2022-01-23_model_deployment_using_aws_lambda/" rel="alternate" type="text/html" title="Deploying Machine Learning models using AWS Lambda and Github Actions - A Detailed Tutorial"/><published>2022-01-23T00:00:00+00:00</published><updated>2022-01-23T00:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/model-deployment-aws</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2022-01-23_model_deployment_using_aws_lambda/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr/> <p>Quite a while back, I had written a <a href="https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/">post</a> in which I described how to package your Machine Learning models using Docker and deploy them using Flask.</p> <p>This post, through a PoC, describes -</p> <ol> <li>How to package your model using Docker (similar as last <a href="https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/">post</a>)</li> <li>How to push the Docker container to Amazon ECR</li> <li>Add a Lambda Function for your model</li> <li>Make a REST API using Amazon API Gateway to access your model</li> <li>Automate the whole process using Github Actions, so that any updates to the model can take effect immediately</li> <li>Make a Streamlit app to make a UI to access the REST API (for the model deployed on AWS)</li> </ol> <p><strong>All the code can be found in my <a href="https://github.com/shreyansh26/Iris_classification-AWS-Lambda-PoC">Github repository</a>.</strong></p> <p>The repository also contains the code to train, save and test a simple ML model on the Iris Dataset.</p> <p>The Iris dataset is a small dataset which contains attributes of the flower - Sepal length, Sepal width, Petal length and Petal width. The goal of the task is to classify based on these dimensions, the type of the Iris, which in the dataset is among three classes - Setosa, Versicolour and Virginica.</p> <h2 id="package-requirements">Package Requirements</h2> <ul> <li>An Amazon Web Services account (I intentionally use a simple ML model to deploy as it remains in the AWS Free tier constraints across all the services I mention above. Larger models will require more storage and hence could be chargeable.)</li> <li>Python 3.6+</li> <li>A simple <code class="language-plaintext highlighter-rouge">pip install -r requirements.txt</code> from the <a href="https://github.com/shreyansh26/Iris_classification-AWS-Lambda-PoC/tree/master/iris_classification">https://github.com/shreyansh26/Iris_classification-AWS-Lambda-PoC/tree/master/iris_classification</a> directory will install the other Python packages required.</li> </ul> <h2 id="steps-to-follow">Steps to follow</h2> <p>In this PoC, I will be training and deploying a simple ML model. If you follow this tutorial, deploying complex models should be fairly easy as well. (I had to scratch my head a lot though!)</p> <h3 id="1-training-and-deploying-the-model-locally">1. Training and Deploying the model locally</h3> <ol> <li>Clone this repo <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/shreyansh26/Iris_classification-AWS-Lambda-PoC
</code></pre></div> </div> </li> <li>Create a virtual environment - I use <a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda</a>, but you can use any method (virtualenv, venv) <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create -n iris_project python=3.8
conda activate iris_project
</code></pre></div> </div> </li> <li>Install the required dependencies <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install -r requirements.txt
</code></pre></div> </div> </li> <li>Train the model <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd iris_classification/src
python train.py
</code></pre></div> </div> </li> <li>Verify the model trained correctly using pytest <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pytest
</code></pre></div> </div> </li> <li>Activate Streamlit and run <code class="language-plaintext highlighter-rouge">app.py</code> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>streamlit run app.py
</code></pre></div> </div> </li> </ol> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/ini-streamlit.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Right now, the <code class="language-plaintext highlighter-rouge">Predict AWS</code> button will give an error on clicking. It is required to set up an API of your own that the code will send the POST request to.</p> <p>A <code class="language-plaintext highlighter-rouge">main.py</code> file contains the event handler which will be used by Lambda later.</p> <h3 id="2-packaging-the-model">2. Packaging the model</h3> <p>I have included a Dockerfile which is used to package the model. Later I will automate all this using Github Actions.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cd iris_classification
docker build --tag iris_classification:latest .
</code></pre></div></div> <h3 id="3-push-the-docker-container-to-amazon-ecr">3. Push the Docker container to Amazon ECR</h3> <p>First, create a private repository. The free tier only allows for 500MB of storage in a month in a private repository.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/ecr1.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Use the following set of commands to push the local Docker container to the created repository.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 863244415814.dkr.ecr.us-east-1.amazonaws.com

docker tag iris_classification:latest 863244415814.dkr.ecr.us-east-1.amazonaws.com/iris_classification:latest

docker push 863244415814.dkr.ecr.us-east-1.amazonaws.com/iris_classification:latest
</code></pre></div></div> <p>You may have to run</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws configure
</code></pre></div></div> <p>and provide your AWS Access Key ID and your AWS Secret Access Key to run the above commands successfully.</p> <h3 id="4-create-a-lambda-function">4. Create a Lambda function</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/lambda1.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The container image URI can be selected from the AWS console itself.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/lambda2.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <h3 id="5-test-the-lambda">5. Test the Lambda</h3> <p>We can now test that the Lambda is correctly handling the request as we want it to. AWS allows for that. When we click on the Lambda function, it allows a Test option as well.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/lambda3.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The test works and gives the correct result!!</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/lambda4.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="6-create-an-api-from-the-amazon-api-gateway">6. Create an API from the Amazon API Gateway</h3> <p>Make sure to make a REST API.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/api1.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/api2.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Add a <code class="language-plaintext highlighter-rouge">/classify</code> resource to the API and and add a <code class="language-plaintext highlighter-rouge">POST</code> method to the API. Add a POST request to the API under a <code class="language-plaintext highlighter-rouge">/classify</code> resource.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/api3.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Integrate the Lambda function with the API.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/api4.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Now, if you head back to the Lambda functions page, you will see that a Trigger has been added to the function.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/api5.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The endpoint is clearly visible in the screenshot. It will be something like <code class="language-plaintext highlighter-rouge">https://{SOME_ID}.execute-api.us-east-1.amazonaws.com/test/classify</code>.</p> <h3 id="7-test-the-rest-api">7. Test the REST API</h3> <p>We use a client like Postman to check the API.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/postman.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h4 id="and-it-works">AND IT WORKS!</h4> <p>Programmatically, we can also check that the API works.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>

<span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://ti53furxkb.execute-api.us-east-1.amazonaws.com/test/classify</span><span class="sh">'</span>

<span class="n">myobj</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">data</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">[</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.8</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">4.7</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]</span>
    <span class="p">]</span>
<span class="p">}</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">json</span> <span class="o">=</span> <span class="n">myobj</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">text</span><span class="p">)</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{"prediction": ["virginica", "versicolor"], "log_proba": [[-35.82910355985537, -1.5907654693356144, -0.22786665344763715], [-26.20011949521101, -0.0783441410298827, -2.585560434227453]]}
</code></pre></div></div> <h4 id="this-works-too">THIS WORKS TOO!</h4> <p><b> The above API URL and endpoint will not work for you. You should replace it with your own endpoint URL. </b></p> <p> </p> <h2 id="streamlit-app-to-test-the-model">Streamlit app to test the model</h2> <p>After making the appropriate changes to the configuration, running</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>streamlit run app.py
</code></pre></div></div> <p>allows you to get the predictions from the AWS hosted model as well.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/model_deployment_aws/fin-streamlit.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="time-to-automate-the-whole-thing-using-github-actions">Time to automate the whole thing using Github Actions</h2> <p>We use Github Actions to automate this whole process i.e. pushing the container to ECR, updating the Lambda function. The API then points to updated Lambda function automatically.</p> <p>First, we will need to add the <code class="language-plaintext highlighter-rouge">AWS_ACCESS_KEY_ID</code> and <code class="language-plaintext highlighter-rouge">AWS_SECRET_ACCESS_KEY</code> to Github secrets (in the Github repo settings).</p> <p>You can refer to the yml file in <a href="https://github.com/shreyansh26/Iris_classification-AWS-Lambda-PoC/tree/master/.github/workflows">.github/workflows</a> to see how the automation works. The Github Action is triggered when a pull request is made to the <code class="language-plaintext highlighter-rouge">master</code> branch.</p> <p>If required, you can also restrict any pushes to the master branch from Github (<a href="https://stackoverflow.com/questions/46146491/prevent-pushing-to-master-on-github">link</a>).</p> <h3 id="and-we-are-done">AND WE ARE DONE!</h3> <hr/> <p>That’s all for now! I hope this tutorial helps you deploy your own models to AWS Lambda easily. Make sure to read the pricing for each AWS product you use to avoid being charged unknowingly.</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="MLOps"/><category term="model-deployment"/><category term="aws"/><category term="streamlit"/><category term="github-actions"/><summary type="html"><![CDATA[A step-wise tutorial to demonstrate the steps required to deploy a ML model using AWS Lambda, Github Actions, API Gateway and use Streamlit to access the model API through a UI.]]></summary></entry><entry><title type="html">PPML Series #3 - Federated Learning for Mobile Keyboard Prediction</title><link href="https://shreyansh26.github.io/Personal-Website-New/post/2021-12-27_federated_learning_mobile_keyboard/" rel="alternate" type="text/html" title="PPML Series #3 - Federated Learning for Mobile Keyboard Prediction"/><published>2021-12-27T00:00:00+00:00</published><updated>2021-12-27T00:00:00+00:00</updated><id>https://shreyansh26.github.io/Personal-Website-New/post/federated-learning-mobile-keyboard</id><content type="html" xml:base="https://shreyansh26.github.io/Personal-Website-New/post/2021-12-27_federated_learning_mobile_keyboard/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/federated_learning_mobile_keyboard/featured.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr/> <h2 id="introduction">Introduction</h2> <p>Gboard — the Google keyboard, is a virtual keyboard for smartphones with support for more than 900+ language varieties and over 1 billion installs. In addition to decoding noisy signals from input modalities including tap and word-gesture typing, Gboard provides auto-correction, word completion, and next-word prediction features.</p> <p>Next-word predictions provide a tool for facilitating text entry and is plays an important role in improving user experience. Based on a small amount of user-generated preceding text, language models (LMs) can predict the most probable next word or phrase.</p> <p>The above figure shows an example: given the text, “I love you”, Gboard predicts the user is likely to type “and”, “too”, or “so much” next. The centre position in the suggestion strip is reserved for the highest-probability candidate, while the second and third most likely candidates occupy the left and right positions, respectively.</p> <p><strong>Important</strong> - The technical details shared in this post are based on the <a href="https://arxiv.org/abs/1811.03604">paper</a> which was published by Google in 2019. So, some details may be out-of-date, but the core idea behind the solution should still pretty much be the same. Checkout may annotated version of the paper <a href="https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Federated%20Learning%20for%20Mobile%20Keyboard%20Prediction.pdf">here</a>.</p> <p>The primary (static) language model for the English language in Gboard is a Katz smoothed Bayesian interpolated 5-gram LM containing 1.25 million n-grams, including 164,000 unigrams. You can read more about it in <a href="https://research.google/pubs/pub37567/">this paper</a>. We won;t go into this much as the focus of the post is on the next-word prediction task.</p> <p>Another important point one should keep in mind is that mobile keyboard models are constrained in multiple ways - the models should be small, the inference time should be low. Users typically expect a visible keyboard response within 20 milliseconds of an input event. And, given the frequency with which mobile keyboard apps are used, client device batteries could be quickly depleted if CPU consumption were not constrained. As a result, language models are usually limited to tens of megabytes in size with vocabularies of hundreds of thousands of words.</p> <p>In the paper, the authors also discussed about how RNNs and more specifically LSTMs can be used for language modeling since they can utilize an arbitrary and dynamically-sized context window.</p> <h2 id="where-does-federated-learning-come-in">Where does Federated Learning come in?</h2> <p>For the task of next-word prediction, publicly available datasets could have been used. However, the training distribution of those datasets does not match the population distribution. Using sample user-generated text will require efforts such as logging, infrastructure, dedicated storage and security. And even still, some users might not be comfortable with the collection and remote storage of their personal data.</p> <p>For these reasons, the authors use federated learning. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices. An RNN model is trained from scratch in the server and federated environments and achieves recall improvements with respect to the baseline, which is a <a href="https://dl.acm.org/doi/10.5555/972695.972698">n-gram finite state transducer (FST)</a>.</p> <h2 id="model-architecture-and-training">Model Architecture and Training</h2> <p>A variant of LSTM called Coupled Input and Forget Gate (CIFG) is used. The coupled input and forget gate variant uses only one gate for modulating the input and the cell recurrent self-connections, i.e., <i>f</i> = 1 - <i>i</i>. Read more about CIFG in <a href="https://arxiv.org/abs/1804.04849">this paper</a>. Since CIFG uses a single gate to control both the input and recurrent cell self-connections, the number of parameters per cell is reduced by 25%. For time step <em>t</em>, input gate <i>i<sub>t</sub></i> and forget gate <i>f<sub>t</sub></i> have the relation</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/federated_learning_mobile_keyboard/cifg.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The CIFG architecture is advantageous for the mobile device environment because the number of computations and the parameter set size are reduced with no impact on model performance. The model is trained using Tensorflow and on-device inference is supported by Tensorflow Lite. Client device requirements limit the dictionary size to 10,000 words. CIFG’s input and output embedding size is 96. A single layer of CIFG with 670 units is used. Overall, 1.4 million parameters comprise the network. After weight quantization, the model shipped to Gboard devices is 1.4 megabytes in size.</p> <p>The training of the model was done using the FederatedAveraging (FedAvg) algorithm, which I wrote about in my <a href="https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/">previous blog post</a>.</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <h2 id="experiments">Experiments</h2> <p>The paper shows the performance of the CIFG and the FST model on three datasets - server-hosted logs data, client-held data and in live production experiments.</p> <h4 id="server-based-logs">Server-based logs</h4> <p>Server-based training of the CIFG next-word prediction model relies on data logged from Gboard users who have opted to share snippets of text while typing in <em>Google apps</em>. The text is truncated to contain short phrases of a few words, and snippets are only sporadically logged from individual users. Prior to training, logs are anonymized and stripped of personally identifiable information. Additionally, snippets are only used for training if they begin with a start of sentence token.</p> <p>Asynchronous stochastic gradient descent with a learning rate equal to 10<sup>-3</sup> and no weight decay or momentum is used to train the server CIFG. Adaptive gradient methods like Adam and AdaGrad do not improve convergence. The network converges after 150 million steps of SGD.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/federated_learning_mobile_keyboard/server-sgd.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h4 id="federated-training-with-client-caches">Federated training with client caches</h4> <p>As with the logs data, each client cache stores text belonging to the device owner, as well as prediction candidates generated by the decoder. Devices must have at least 2 gigabytes of memory available. Additionally, the clients are only allowed to participate if they are charging, connected to an un-metered network, and idle.</p> <p>The FedAvg algorithm is used here. Between 100 and 500 client updates are required to close each round of federated training in Gboard. The server update is achieved via the Momentum optimizer, using Nesterov accelerated gradient, a momentum hyperparameter of 0.9, and a server learning rate of 1.0 .</p> <p>On average, each client processes approximately 400 example sentences during a single training epoch. The federated CIFG converges after 3000 training rounds, over the course of which 600 million sentences are processed by 1.5 million clients.</p> <p>N-gram model recall is measured by comparing the decoder candidates stored in the on-device training cache to the actual user-entered text.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/federated_learning_mobile_keyboard/cache-sgd.PNG" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="results">Results</h2> <p>Recall (metric) for the highest likelihood candidate is important for Gboard because users are more prone to read and utilize predictions in the centre suggestion spot. Both top-1 and top-3 recall are of interest here.</p> <p>Server-hosted logs data and client device-owned caches are used to measure prediction recall. Although each contain snippets of data from actual users, the client caches are believed to more accurately represent the true typing data distribution. Cache data, unlike logs, are not truncated in length and are not restricted to keyboard usage in Google-owned apps.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/federated_learning_mobile_keyboard/tab3.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/federated_learning_mobile_keyboard/tab4.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Prediction impression recall is measured by dividing the number of predictions that match the user-entered text by the number of times users are shown prediction candidates. The prediction impression recall metric is typically lower than the standard recall metric. Zero-state prediction events (in which users open the Gboard app but do not commit any text) increase the number of impressions but not matches.</p> <p>The prediction click-through rate (CTR), defined as the ratio of the number of clicks on prediction candidates to the number of proposed prediction candidates.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/federated_learning_mobile_keyboard/tab5.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/federated_learning_mobile_keyboard/tab6.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{max-width:100%;width:auto;height:auto}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}` figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{` width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>For both the server training and the federated training, the CIFG model improves the top-1 and top-3 recall with respect to the baseline n-gram FST model.</p> <p>These gains are impressive given that the n-gram model uses an order of magnitude larger vocabulary and includes personalized components such as user history and contacts language models.</p> <p>The results also demonstrate that the federated CIFG performs better on recall metrics than the server-trained CIFG. Comparisons on server-hosted logs data show the recall of the two models is comparable, though the logs are not as representative of the true typing distribution.</p> <p>Different flavors of SGD are used in each training context—the results show that federated learning provides a preferable alternative to server-based training of neural language models.</p> <h2 id="conclusion">Conclusion</h2> <p>CIFG language model trained from scratch using federated learning can outperform an identical server trained CIFG model and baseline n-gram model on the keyboard next-word prediction task.</p> <hr/> <p><strong>I have also released an annotated version of the paper. If you are interested, you can find it <a href="https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Federated%20Learning%20for%20Mobile%20Keyboard%20Prediction.pdf">here</a>.</strong></p> <p>That’s all for now!</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="PPML"/><category term="federated learning"/><category term="ppml"/><category term="paper-summaries"/><summary type="html"><![CDATA[Understanding how your mobile keyboard (Gboard, specifically) performs the next word prediction task and performs model training and updates]]></summary></entry></feed>