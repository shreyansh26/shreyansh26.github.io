<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://shreyansh26.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shreyansh26.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-17T14:48:56+00:00</updated><id>https://shreyansh26.github.io/feed.xml</id><title type="html">blank</title><subtitle>Shreyansh&apos;s personal website. </subtitle><entry><title type="html">Understanding Multi-Head Latent Attention (MLA)</title><link href="https://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/" rel="alternate" type="text/html" title="Understanding Multi-Head Latent Attention (MLA)"/><published>2025-11-08T00:00:00+00:00</published><updated>2025-11-08T00:00:00+00:00</updated><id>https://shreyansh26.github.io/post/multihead-latent-attention</id><content type="html" xml:base="https://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/mla/mla_cover.png" alt="Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. Source - https://arxiv.org/abs/2405.04434."/> <figcaption>Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference. Source - https://arxiv.org/abs/2405.04434.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr/> <p><strong>Code</strong> - <a href="https://github.com/shreyansh26/multihead-latent-attention">https://github.com/shreyansh26/multihead-latent-attention</a></p> <p>Deepseek introduced Multi-Head Latent Attention (MLA) in the <a href="https://arxiv.org/abs/2405.04434">Deepseek-v2 paper</a> as a way to improve the efficiency of attention computation during inference by reducing the KV cache bottleneck. MLA achieves better performance than Multi-Head Attention (MHA).</p> <p>Grouped-Query Attention (GQA) and Multi-Query Attention (MQA) reduce Key/Value (KV) duplication, shrinking the KV cache and cutting bandwidth. Multi-Head Latent Attention (MLA) goes further: it introduces a low-rank latent space that factorizes attention, enabling both efficient training and extremely efficient inference with a simple algebraic “absorption” trick.</p> <p>This post walks from MHA → GQA → MQA → MLA, then shows the fusion and absorption optimizations, with concrete PyTorch code and equations you can render in Markdown.</p> <h2 id="revisiting-multi-head-attention-mha">Revisiting Multi-Head Attention (MHA)</h2> <p>MHA projects input tokens into per-head Query/Key/Value, computes attention per head, then merges:</p> <p>Given hidden size (D), number of heads (H), and head dimension (d) where (D = H \cdot d):</p> <ul> <li>Queries: \(Q \in \mathbb{R}^{B \times S \times H \times d}\)</li> <li>Keys: \(K \in \mathbb{R}^{B \times S \times H \times d}\)</li> <li>Values: \(V \in \mathbb{R}^{B \times S \times H \times d}\)</li> <li>Attention per head: \(\mathrm{Attn}(Q_i, K_i, V_i) = \mathrm{Softmax}\!\left(\frac{Q_i K_i^\top}{\sqrt{d}}\right) V_i\)</li> </ul> <p>Code reference (simplified from our <a href="https://github.com/shreyansh26/multihead-latent-attention/blob/main/mha.py"><code class="language-plaintext highlighter-rouge">mha.py</code></a>):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_bsd</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">kv_cache</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">return_torch_ref</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x_bsd</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">new_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">q_bsqh</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">q_proj</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="n">k_blkh</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">k_proj</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="n">v_blkh</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">v_proj</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="n">q_bsqh</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">q_bsqh</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis</span><span class="p">)</span>
    <span class="n">k_blkh</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">k_blkh</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis</span><span class="p">)</span>
    <span class="n">q_bqsh</span> <span class="o">=</span> <span class="n">q_bsqh</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">k_bklh</span> <span class="o">=</span> <span class="n">k_blkh</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">v_bklh</span> <span class="o">=</span> <span class="n">v_blkh</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">out_bsd</span> <span class="o">=</span> <span class="nf">naive_attention</span><span class="p">(</span><span class="n">q_bqsh</span><span class="p">,</span> <span class="n">k_bklh</span><span class="p">,</span> <span class="n">v_bklh</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span>
    <span class="n">out_bsd</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">o_proj</span><span class="p">(</span><span class="n">out_bsd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out_bsd</span>
</code></pre></div></div> <p>Inefficiency: we compute and store (K,V) per head. For long sequences, the KV cache dominates memory and communication.</p> <h2 id="gqa-grouped-query-attention">GQA: Grouped-Query Attention</h2> <p>GQA shares Keys/Values across groups of query heads: \(H\) query heads share \(H_\text{kv}\) KV heads (with \(H_\text{kv} &lt; H\)). Complexity and KV cache both drop by a factor of \(H / H_\text{kv}\) compared to MHA, while preserving multiple query heads for expressivity.</p> <p>Trade-off: less KV diversity per query head; often negligible loss in modeling capacity with slight improvement in inference efficiency.</p> <h2 id="mqa-multi-query-attention">MQA: Multi-Query Attention</h2> <p>MQA goes to the limit: one shared KV head for all queries \(H_\text{kv}=1\). KV cache drops by \(\approx H\times\) versus MHA; cross-device communication shrinks markedly. For long-context inference, this is a big win.</p> <p>Downside: a single KV head may reduce modeling capacity if used naïvely. MLA addresses this by introducing a low-rank latent structure that preserves expressivity while keeping runtime costs low.</p> <h2 id="mla-multi-head-latent-attention">MLA: Multi-Head Latent Attention</h2> <p>MLA factorizes attention via low-rank latent projections. Notation follows our reference:</p> <ul> <li>Latent compression:</li> </ul> \[\mathbf{c}^{KV}_t = W^{DKV}\, \mathbf{x}_t,\quad \mathbf{c}^{Q}_t = W^{DQ}\, \mathbf{x}_t,\] <p>where \(W^{DKV} \in \mathbb{R}^{r_{kv} \times D}\), \(W^{DQ} \in \mathbb{R}^{r_q \times D}\).</p> <ul> <li>Per-head decompression:</li> </ul> \[\mathbf{k}^{N}_t = W^{UK}\, \mathbf{c}^{KV}_t,\quad \mathbf{v}^{N}_t = W^{UV}\, \mathbf{c}^{KV}_t,\quad \mathbf{q}^{N}_t = W^{UQ}\, \mathbf{c}^{Q}_t,\] <p>where \(W^{UK} \in \mathbb{R}^{\text{nh}_{kv} * d_{\text{qk}_{nope}} \times r_{kv}}\), \(W^{UV} \in \mathbb{R}^{\text{nh}_{kv} * d_v \times r_{kv}}\), \(W^{UQ} \in \mathbb{R}^{\text{nh}_{q} * d_{\text{qk}_{nope}} \times r_{q}}\).</p> <ul> <li>Decoupled RoPE:</li> </ul> \[\mathbf{k}^{R}_t = \mathrm{RoPE}(W^{KR}\, \mathbf{x}_t),\quad \mathbf{q}^{R}_t = \mathrm{RoPE}(W^{QR}\, \mathbf{c}^{Q}_t),\] <p>where \(W^{KR} \in \mathbb{R}^{d_{\text{qk}_{rope}} \times D}\), \(W^{QR} \in \mathbb{R}^{\text{nh}_{q} * d_{\text{qk}_{rope}} \times r_{q}}\).</p> <p>and we concatenate for each head (i):</p> \[\mathbf{k}_{t,i} = [\,\mathbf{k}^N_{t,i};\ \mathbf{k}^R_t\,],\qquad \mathbf{q}_{t,i} = [\,\mathbf{q}^N_{t,i};\ \mathbf{q}^R_{t,i}\,].\] <p>The forward in our <a href="https://github.com/shreyansh26/multihead-latent-attention/blob/6d47fa3a9ec8105fede03023bb3bce8c4537d48e/mla.py#L10"><code class="language-plaintext highlighter-rouge">MLA</code></a> implementation mirrors this shape construction:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># MLA.forward (selected lines)
</span><span class="n">c_kv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_dkv</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>  <span class="c1"># [B, S, r_kv]
</span><span class="n">c_q</span>  <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_dq</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>   <span class="c1"># [B, S, r_q]
</span>
<span class="n">k_r</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_kr</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>                       <span class="c1"># [B, S, dR]
</span><span class="n">k_r</span> <span class="o">=</span> <span class="n">k_r</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
<span class="n">k_r</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">k_r</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis_qk</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [B, 1, S, dR]
</span>
<span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">c_kv</span> <span class="o">=</span> <span class="n">cache</span><span class="p">.</span><span class="n">compressed_kv</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">c_kv</span><span class="p">)</span>  <span class="c1"># [B, S_kv, r_kv]
</span>    <span class="n">k_r</span>  <span class="o">=</span> <span class="n">cache</span><span class="p">.</span><span class="n">k_rope</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">k_r</span><span class="p">)</span>          <span class="c1"># [B, 1, S_kv, dR]
</span>
<span class="n">k_n</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uk</span><span class="p">(</span><span class="n">c_kv</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len_kv</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_nope_head_dim</span><span class="p">)</span>
<span class="n">k_n</span> <span class="o">=</span> <span class="n">k_n</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>                    <span class="c1"># [B, H_kv, S_kv, dN]
</span><span class="n">k</span>   <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">k_r</span><span class="p">.</span><span class="nf">repeat_interleave</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">k_n</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">q_r</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_qr</span><span class="p">(</span><span class="n">c_q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
<span class="n">q_r</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">q_r</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis_qk</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>   <span class="c1"># [B, H, S, dR]
</span><span class="n">q_n</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uq</span><span class="p">(</span><span class="n">c_q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_nope_head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">q</span>   <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">q_r</span><span class="p">,</span> <span class="n">q_n</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uv</span><span class="p">(</span><span class="n">c_kv</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len_kv</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">v_head_dim</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="nf">sdpa_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_o</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div> <p>Intuition: MLA maintains multi-head queries, but routes them through a shared latent bottleneck for \((K,V)\) (and optionally for parts of \(Q\)). This preserves per-head specialization via \(W^{UQ}\), \(W^{UK}\), \(W^{UV}\), while dramatically reducing the “surface area” of the KV cache.</p> <h3 id="fusion-fewer-intermediate-tensors-same-math">Fusion: fewer intermediate tensors, same math</h3> <p>We can fuse linears to reduce memory traffic:</p> <ul> <li>Combine \(W^{DKV}\) and \(W^{KR}\) into a single projection (<code class="language-plaintext highlighter-rouge">w_dkv_kr</code>).</li> <li>Combine \(W^{UK}\) and \(W^{UV}\) into a single projection (<code class="language-plaintext highlighter-rouge">w_uk_uv</code>) then split.</li> <li>Combine \(W^{QR}\) and \(W^{UQ}\) into a single projection (<code class="language-plaintext highlighter-rouge">w_qr_uq</code>) then split for \(\mathbf{q}^N\) and \(\mathbf{q}^C\).</li> </ul> <p>Snippet from <a href="https://github.com/shreyansh26/multihead-latent-attention/blob/6d47fa3a9ec8105fede03023bb3bce8c4537d48e/mla.py#L111"><code class="language-plaintext highlighter-rouge">MLAFused.forward</code></a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c_q</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_dq</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>                 <span class="c1"># [B, S, r_q]
</span><span class="n">c_kv_kr</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_dkv_kr</span><span class="p">(</span><span class="n">x_bsd</span><span class="p">)</span>         <span class="c1"># [B, S, r_kv + dR]
</span><span class="n">c_kv</span><span class="p">,</span> <span class="n">k_r</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">c_kv_kr</span><span class="p">,</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">kv_lora_rank</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">k_r</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">k_r</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis_qk</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">if</span> <span class="n">cache</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">c_kv</span> <span class="o">=</span> <span class="n">cache</span><span class="p">.</span><span class="n">compressed_kv</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">c_kv</span><span class="p">)</span>
    <span class="n">k_r</span>  <span class="o">=</span> <span class="n">cache</span><span class="p">.</span><span class="n">k_rope</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">k_r</span><span class="p">)</span>

<span class="n">k_n_v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uk_uv</span><span class="p">(</span><span class="n">c_kv</span><span class="p">)</span>             <span class="c1"># [B, S_kv, H_kv * (dN + dV)]
</span><span class="n">k_n</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">k_n_v</span><span class="p">,</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_nope_head_dim</span><span class="p">,</span>
                             <span class="n">self</span><span class="p">.</span><span class="n">num_key_value_heads</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">v_head_dim</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># reshape, build k, build q via w_qr_uq, attend, project out...
</span></code></pre></div></div> <p>Fusion preserves semantics but minimizes reads/writes of large intermediate tensors—especially important under long sequence lengths where bandwidth dominates.</p> <h3 id="absorption-inference-time-mqa-with-latent-routing">Absorption: inference-time MQA with latent routing</h3> <p>At inference we can algebraically “absorb” \(W^{UK}\) into the query path and \(W^{UV}\) into the output path. Starting with</p> \[\mathbf{q}_{t,i} = [\,\mathbf{q}^C_{t,i}; \mathbf{q}^R_{t,i}\,],\qquad \mathbf{k}_t = [\,\mathbf{k}^C_t;\ \mathbf{k}^R_t\,],\] <p>define</p> \[\hat{\mathbf{q}}_{t,i} = \big[(W^{UK}_i)^\top \mathbf{q}^C_{t,i};\ \mathbf{q}^R_{t,i}\big],\qquad \hat{\mathbf{k}}_t = \big[\mathbf{c}^{KV}_t;\ \mathbf{k}^R_t\big].\] <p>Then attention can be computed against a single shared latent KV head \(\mathbf{c}^{KV}\) (plus shared RoPE key), and the per-head value projection is postponed to the output:</p> \[\hat{\mathbf{o}}_{t,i} = \sum_{j=1}^{t} \mathrm{softmax}_j\!\left(\frac{\hat{\mathbf{q}}_{t,i}^\top \hat{\mathbf{k}}_j}{\sqrt{d + d^R}}\right) \mathbf{c}^{KV}_j,\quad \mathbf{y}_t = W^{O} \,[\, W^{UV}_1 \hat{\mathbf{o}}_{t,1};\dots; W^{UV}_H \hat{\mathbf{o}}_{t,H}\,].\] <p>Our <a href="https://github.com/shreyansh26/multihead-latent-attention/blob/6d47fa3a9ec8105fede03023bb3bce8c4537d48e/mla.py#L158"><code class="language-plaintext highlighter-rouge">MLAFusedAbsorbed</code></a> implements exactly this MQA-like inference path:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Keys: single shared head [k_r, c_kv]
</span><span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">k_r</span><span class="p">,</span> <span class="n">c_kv</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, 1, S_kv, dR + r_kv]
</span>
<span class="c1"># Queries: per-head RoPE + absorbed-nope to r_kv
</span><span class="n">q_r</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_qr</span><span class="p">(</span><span class="n">c_q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">qk_rope_head_dim</span><span class="p">)</span>
<span class="n">q_r</span> <span class="o">=</span> <span class="nf">apply_rotary_emb</span><span class="p">(</span><span class="n">q_r</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">freqs_cis_qk</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">q_n</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_uq_absorbed</span><span class="p">(</span><span class="n">c_q</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_attention_heads</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">kv_lora_rank</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">q</span>   <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">q_r</span><span class="p">,</span> <span class="n">q_n</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Values: the shared latent c_kv as single head
</span><span class="n">v</span> <span class="o">=</span> <span class="n">c_kv</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>                               <span class="c1"># [B, 1, S_kv, r_kv]
</span><span class="n">out</span> <span class="o">=</span> <span class="nf">sdpa_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">is_causal</span><span class="o">=</span><span class="n">is_causal</span><span class="p">)</span>  <span class="c1"># MQA-like compute
</span><span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">w_o_absorbed</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>                        <span class="c1"># absorbs W^{UV} into W^O
</span></code></pre></div></div> <p>Effect: KV cache stores \(\mathbf{c}^{KV}\) once per token (plus a small shared RoPE key). Communication is essentially MQA, but per-head specialization is retained via the absorbed query/output linears.</p> <h2 id="complexity-and-kv-cache-discussion">Complexity and KV cache discussion</h2> <p>Let:</p> <ul> <li>\(B\): batch size, \(S\): sequence length, \(H\): attention heads,</li> <li>\(H_{kv}\): KV heads in GQA/MLA, \(d\): head dim, \(d_{\text{qk}_{rope}}\): RoPE dim,</li> <li>\(r_q, r_{kv}\): low-rank dimensions for query/kv latents.</li> </ul> <p>Rough per-token storage for the KV cache (ignoring dtype constants):</p> <ul> <li>MHA: \(O(H \cdot S \cdot d)\) for \(K\) and \(O(H \cdot S \cdot d)\) for \(V\).</li> <li>GQA: \(O(H_{kv} \cdot S \cdot d)\) per \(K,V\).</li> <li>MQA: \(O(S \cdot d)\) per \(K,V\).</li> <li>MLA: \(O(S \cdot r_{kv})\) for \(\mathbf{c}^{KV}_t\) and \(O(S \cdot d_{\text{qk}_{rope}})\) for \(\mathbf{k}^R_t\)</li> </ul> <p>Communication between devices during decode scales with KV cache size too; MLA’s absorbed path therefore inherits MQA’s excellent scaling while maintaining multi-head query diversity.</p> <p>Compute:</p> <ul> <li>Matmuls with \(W^{DKV}\) and \(W^{DQ}\) are shared per token, independent of \(H\).</li> <li>Per-head expansions via \(W^{UQ}, W^{UK}, W^{UV}\) are relatively cheap when \(r_q, r_{kv} \ll D\).</li> <li>Absorption swaps some inner-loop per-token head matmuls for outer-loop linears, keeping the high-arithmetic-intensity parts in efficient GEMMs.</li> </ul> <h3 id="kv-cache-storage-size-comparison">KV Cache storage size comparison</h3> <p>MLA has to cache \(\mathbf{c}^{KV}\) and \(\mathbf{k}^R\) for each token, which is \(r_{kv} + d_{\text{qk}_{rope}}\) per token. In the Deepseek v2 and v3 configs, \(r_{kv} = 4 d_{\text{qk}_{nope}}\) and \(d_{\text{qk}_{rope}} = 0.5 * d_{\text{qk}_{nope}}\).</p> <p>The table below shows the KV cache size comparison for the different attention mechanisms.</p> <table> <thead> <tr> <th>Attention Mechanism</th> <th>KV Cache per Token</th> </tr> </thead> <tbody> <tr> <td>MHA</td> <td>\(2n_h d_h l\)</td> </tr> <tr> <td>GQA</td> <td>\(2n_g d_h l\)</td> </tr> <tr> <td>MQA</td> <td>\(2d_h l\)</td> </tr> <tr> <td>MLA</td> <td>\((r_{kv} + d_{\text{qk}_{rope}}) l \approx \frac{9}{2} d_{\text{qk}_{nope}} l\)</td> </tr> </tbody> </table> <h2 id="conclusion">Conclusion</h2> <p>MLA reframes attention as a low-rank routing problem. During training, it behaves much like GQA but with smaller activations; during inference, absorption yields an MQA-like footprint with per-head specialization preserved through the query/output paths. If your production bottleneck is KV cache size or cross-device bandwidth, MLA’s absorbed path is a direct drop-in to claw back latency without sacrificing modeling power.</p> <hr/> <p>These are my notes on MLA and hopefully it proves useful to someone looking to understand MLA better.</p> <p><strong>Here is the code</strong> - <a href="https://github.com/shreyansh26/multihead-latent-attention">https://github.com/shreyansh26/multihead-latent-attention</a></p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="LLMs"/><category term="attention"/><category term="mla"/><summary type="html"><![CDATA[A mathematical and code deep-dive on one of the key innovations from Deepseek - Multihead Latent Attention (MLA)]]></summary></entry><entry><title type="html">Deriving the Gradient for the Backward Pass of Layer Normalization</title><link href="https://shreyansh26.github.io/post/2025-06-04_layernorm-gradients/" rel="alternate" type="text/html" title="Deriving the Gradient for the Backward Pass of Layer Normalization"/><published>2025-06-04T00:00:00+00:00</published><updated>2025-06-04T00:00:00+00:00</updated><id>https://shreyansh26.github.io/post/layenorm-backward</id><content type="html" xml:base="https://shreyansh26.github.io/post/2025-06-04_layernorm-gradients/"><![CDATA[<div class="outer"> <figure class="image" style="width: 70%;"> <img src="/assets/img/posts_images/layer_norm_backward/layer_norm_4o.png" alt="Source: GPT-4o image generation"/> <figcaption>Source: GPT-4o image generation</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr/> <h2 id="forward-pass-recap">Forward Pass Recap</h2> <p>First, let’s write down the forward pass for a single input vector \(x\) (a row from \(X\)) of dimension \(N\):</p> \[y = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} * \gamma + \beta\] <p>We can break down the forward pass into the following steps:</p> <ol> <li><strong>Mean:</strong> \(\mu = \frac{1}{N} \sum_j x_j\)</li> <li><strong>Variance:</strong> \(\sigma^2 = \frac{1}{N} \sum_j (x_j - \mu)^2\)</li> <li><strong>Inverse Standard Deviation (rstd):</strong> \(\text{rstd} = \frac{1}{\sqrt{\sigma^2 + \epsilon}}\)</li> <li><strong>Normalized Input (\(\hat{x}\)):</strong> \(\begin{equation} \hat{x}_j = (x_j - \mu) \cdot \text{rstd} \label{eq:1} \end{equation}\)</li> <li><strong>Output (\(y\)):</strong> \(\begin{equation} y_j = \hat{x}_j \cdot \gamma_j + \beta_j \label{eq:2} \end{equation}\)</li> </ol> <p>In general for ML applications, while doing the backward pass, we are given \(\frac{dL}{dy_j}\) (denoted as \(dy_j\)) which is the gradient of the loss \(L\) with respect to the output \(y_j\).</p> <p>We want to find <strong>\(\frac{dL}{dx_j}\) (denoted \(dx_j\)), \(\frac{dL}{d\gamma_j}\) (denoted \(d\gamma_j\)), and \(\frac{dL}{d\beta_j}\) (denoted \(d\beta_j\))</strong>.</p> <h2 id="gradients-fracdldgamma_j-and-fracdldbeta_j">Gradients \(\frac{dL}{d\gamma_j}\) and \(\frac{dL}{d\beta_j}\)</h2> <p>These are the simplest. \(\gamma_j\) and \(\beta_j\) only affect \(y_j\) directly in the final step.</p> <p>Let’s define <strong>\(\delta_{kj}\) as 1 if \(k=j\), and \(0\) otherwise</strong>.</p> \[\frac{\partial y_k}{\partial \gamma_j} = \hat{x}_j \delta_{kj}\] \[\frac{\partial y_k}{\partial \beta_j} = \delta_{kj}\] <p>Using the chain rule:</p> \[\frac{dL}{d\gamma_j} = \sum_k \left( \frac{dL}{dy_k} \frac{\partial y_k}{\partial \gamma_j} \right) = \frac{dL}{dy_j} \frac{\partial y_j}{\partial \gamma_j} = dy_j \cdot \hat{x}_j\] \[\frac{dL}{d\beta_j} = \sum_k \left( \frac{dL}{dy_k} \frac{\partial y_k}{\partial \beta_j} \right) = \frac{dL}{dy_j} \frac{\partial y_j}{\partial \beta_j} = dy_j \cdot 1\] <p>If we consider the whole batch (multiple rows), \(\gamma_j\) and \(\beta_j\) are shared. So, the gradients are summed over all rows \(i\):</p> \[\frac{dL}{d\gamma_j} = \sum_i (dy_{ij} \cdot \hat{x}_{ij})\] \[\frac{dL}{d\beta_j} = \sum_i dy_{ij}\] <h2 id="gradient-fracdldhatx_j">Gradient \(\frac{dL}{d\hat{x}_j}\)</h2> <p>From Equation \eqref{eq:2}:</p> \[\frac{\partial y_k}{\partial \hat{x}_j} = \gamma_j \delta_{kj}\] <p>So,</p> \[\begin{equation} \frac{dL}{d\hat{x}_j} = \sum_k \left( \frac{dL}{dy_k} \frac{\partial y_k}{\partial \hat{x}_j} \right) = \frac{dL}{dy_j} \frac{\partial y_j}{\partial \hat{x}_j} = dy_j \cdot \gamma_j \label{eq:3} \end{equation}\] <h2 id="gradient-fracdldx_j-the-core-part">Gradient \(\frac{dL}{dx_j}\) (The Core Part)</h2> <p>This is the most complex part because \(x_j\) affects all \(\hat{x}_k\) in the same row through \(\mu\) and \(\text{rstd}\).</p> <p>We need \(\frac{dL}{dx_j} = \sum_k \left( \frac{dL}{d\hat{x}_k} \frac{\partial \hat{x}_k}{\partial x_j} \right)\).</p> <p>We already have \(\frac{dL}{d\hat{x}_k} = dy_k \cdot \gamma_k\) (from Equation \eqref{eq:3}). Let’s call this \(d\hat{x}'_k\).</p> <p>Now we need \(\frac{\partial \hat{x}_k}{\partial x_j}\). Recall (from Equation \eqref{eq:1}), \(\hat{x}_k = (x_k - \mu) \cdot \text{rstd}\).</p> <p>Using the product rule:</p> <p>\(\frac{\partial \hat{x}_k}{\partial x_j} = \frac{\partial (x_k - \mu)}{\partial x_j} \cdot \text{rstd} + (x_k - \mu) \cdot \frac{\partial \text{rstd}}{\partial x_j}\).</p> <p>Let’s find the intermediate derivatives:</p> \[\frac{\partial \mu}{\partial x_j} = \frac{1}{N}\] \[\frac{\partial (x_k - \mu)}{\partial x_j} = \frac{\partial x_k}{\partial x_j} - \frac{\partial \mu}{\partial x_j} = \delta_{kj} - \frac{1}{N}\] <p>Next, \(\frac{\partial \text{rstd}}{\partial x_j}\):</p> \[\text{rstd} = (\sigma^2 + \epsilon)^{-\frac{1}{2}}\] \[\frac{\partial \text{rstd}}{\partial x_j} = -\frac{1}{2} (\sigma^2 + \epsilon)^{-\frac{3}{2}} \frac{\partial \sigma^2}{\partial x_j}\] \[\begin{equation} \frac{\partial \text{rstd}}{\partial x_j} = -\frac{1}{2} \text{rstd}^3 \frac{\partial \sigma^2}{\partial x_j} \label{eq:4} \end{equation}\] <p>Now, \(\frac{\partial \sigma^2}{\partial x_j}\):</p> \[\sigma^2 = \frac{1}{N} \sum_p (x_p - \mu)^2\] \[\frac{\partial \sigma^2}{\partial x_j} = \frac{1}{N} \sum_p \left[ 2 (x_p - \mu) \cdot \frac{\partial (x_p - \mu)}{\partial x_j} \right]\] \[\frac{\partial \sigma^2}{\partial x_j} = \frac{2}{N} \sum_p \left[ (x_p - \mu) \cdot \left(\delta_{pj} - \frac{1}{N}\right) \right]\] \[\frac{\partial \sigma^2}{\partial x_j} = \frac{2}{N} \left[ (x_j - \mu)\left(1 - \frac{1}{N}\right) + \sum_{p \neq j} (x_p - \mu)\left(-\frac{1}{N}\right) \right]\] \[\frac{\partial \sigma^2}{\partial x_j} = \frac{2}{N} \left[ (x_j - \mu) - \frac{1}{N}(x_j - \mu) - \frac{1}{N}\sum_{p \neq j} (x_p - \mu) \right]\] \[\frac{\partial \sigma^2}{\partial x_j} = \frac{2}{N} \left[ (x_j - \mu) - \frac{1}{N}\sum_p (x_p - \mu) \right]\] <p>Since \(\sum_p (x_p - \mu) = 0\), the second term vanishes.</p> \[\begin{equation} \frac{\partial \sigma^2}{\partial x_j} = \frac{2}{N} (x_j - \mu) \label{eq:5} \end{equation}\] <p>Substitute back into Equation \eqref{eq:4}, \(\frac{\partial \text{rstd}}{\partial x_j}\):</p> \[\frac{\partial \text{rstd}}{\partial x_j} = \left(-\frac{1}{2}\right) \cdot \text{rstd}^3 \cdot \left(\frac{2}{N}\right) \cdot (x_j - \mu)\] \[\frac{\partial \text{rstd}}{\partial x_j} = -\frac{1}{N} \text{rstd}^3 (x_j - \mu)\] \[\frac{\partial \text{rstd}}{\partial x_j} = -\frac{1}{N} \text{rstd}^2 \cdot ((x_j - \mu) \cdot \text{rstd})\] \[\begin{equation} \frac{\partial \text{rstd}}{\partial x_j} = -\frac{1}{N} \text{rstd}^2 \hat{x}_j \label{eq:6} \end{equation}\] <p>Now assemble \(\frac{\partial \hat{x}_k}{\partial x_j}\):</p> \[\frac{\partial \hat{x}_k}{\partial x_j} = \left(\delta_{kj} - \frac{1}{N}\right) \cdot \text{rstd} + (x_k - \mu) \cdot \left(-\frac{1}{N} \text{rstd}^2 \hat{x}_j\right)\] \[\frac{\partial \hat{x}_k}{\partial x_j} = \left(\delta_{kj} - \frac{1}{N}\right) \cdot \text{rstd} - \frac{1}{N} \cdot ((x_k - \mu) \cdot \text{rstd}) \cdot \text{rstd} \cdot \hat{x}_j\] \[\frac{\partial \hat{x}_k}{\partial x_j} = \left(\delta_{kj} - \frac{1}{N}\right) \cdot \text{rstd} - \frac{1}{N} \hat{x}_k \cdot \text{rstd} \cdot \hat{x}_j\] \[\begin{equation} \frac{\partial \hat{x}_k}{\partial x_j} = \frac{\text{rstd}}{N} (N \delta_{kj} - 1 - \hat{x}_k \hat{x}_j) \label{eq:7} \end{equation}\] <p>Finally, using Equation \eqref{eq:7},</p> \[\frac{dL}{dx_j} = \sum_k \left( d\hat{x}'_k \cdot \frac{\partial \hat{x}_k}{\partial x_j} \right)\] \[\frac{dL}{dx_j} = \sum_k \left[ d\hat{x}'_k \cdot \frac{\text{rstd}}{N} (N \delta_{kj} - 1 - \hat{x}_k \hat{x}_j) \right]\] \[\frac{dL}{dx_j} = \frac{\text{rstd}}{N} \sum_k \left[ d\hat{x}'_k (N \delta_{kj} - 1 - \hat{x}_k \hat{x}_j) \right]\] \[\frac{dL}{dx_j} = \frac{\text{rstd}}{N} \left[ (d\hat{x}'_j (N - 1 - \hat{x}_j \hat{x}_j)) + \sum_{k \neq j} d\hat{x}'_k (-1 - \hat{x}_k \hat{x}_j) \right]\] \[\frac{dL}{dx_j} = \frac{\text{rstd}}{N} \left[ N d\hat{x}'_j - d\hat{x}'_j - d\hat{x}'_j \hat{x}_j^2 - \sum_{k \neq j} d\hat{x}'_k - \sum_{k \neq j} (d\hat{x}'_k \hat{x}_k \hat{x}_j) \right]\] \[\frac{dL}{dx_j} = \frac{\text{rstd}}{N} \left[ N d\hat{x}'_j - \left(\sum_k d\hat{x}'_k\right) - \hat{x}_j \left(\sum_k d\hat{x}'_k \hat{x}_k\right) \right]\] <p>The sum expansions are correct because \(d\hat{x}'_j \hat{x}_j^2\) is one term of \(\hat{x}_j (\sum_k d\hat{x}'_k \hat{x}_k)\) and \(d\hat{x}'_j\) is one term of \(\sum_k d\hat{x}'_k\).</p> <p>So, for a specific \(j\), from Equation \eqref{eq:8}:</p> \[\begin{equation} \frac{dL}{dx_j} = \text{rstd} \cdot \left[ d\hat{x}'_j - \frac{1}{N} \left(\sum_k d\hat{x}'_k\right) - \frac{\hat{x}_j}{N} \left(\sum_k d\hat{x}'_k \hat{x}_k\right) \right] \label{eq:8} \end{equation}\] <p>Now:</p> <ul> <li>\(d\hat{x}'_j = dy_j \cdot \gamma_j\) (from Equation \eqref{eq:3})</li> <li>Let \(c_2 = \frac{1}{N} \sum_k d\hat{x}'_k = \frac{1}{N} \sum_k (dy_k \cdot \gamma_k)\)</li> <li>Let \(c_1 = \frac{1}{N} \sum_k (d\hat{x}'_k \cdot \hat{x}_k) = \frac{1}{N} \sum_k ( (dy_k \cdot \gamma_k) \cdot \hat{x}_k )\)</li> </ul> <p>Substituting these back in Equation \eqref{eq:8}:</p> \[\frac{dL}{dx_j} = \text{rstd} \cdot \left[ (dy_j \cdot \gamma_j) - c_2 - \hat{x}_j \cdot c_1 \right]\] \[\begin{equation} \frac{dL}{dx_j} = \text{rstd} \cdot \left[ (dy_j \cdot \gamma_j) - (\hat{x}_j \cdot c_1 + c_2) \right] \label{eq:9} \end{equation}\] <p>If we want to be more explicit with batch indexing (let \(i\) be the row/sequence index in the batch):</p> \[\begin{equation} \frac{dL}{dx_{ij}} = \text{rstd}_i \cdot \left[ (dy_{ij} \cdot \gamma_j) - c_{2_{i}} - \hat{x}_{ij} \cdot c_{1_{i}} \right] \label{eq:10} \end{equation}\] <hr/> <p>We now have the final gradients -</p> \[\boxed{ \begin{aligned} \frac{dL}{d\gamma_j} &amp;= \sum_i (dy_{ij} \cdot \hat{x}_{ij}) \\ \frac{dL}{d\beta_j} &amp;= \sum_i dy_{ij} \\ \frac{dL}{dx_{ij}} &amp;= \text{rstd}_i \cdot \left[ (dy_{ij} \cdot \gamma_j) - c_{2_{i}} - \hat{x}_{ij} \cdot c_{1_{i}} \right] \end{aligned} }\] <p>where,</p> \[\boxed{ \begin{aligned} c_1 = \frac{1}{N} \sum_k (d\hat{x}'_k \cdot \hat{x}_k) &amp;= \frac{1}{N} \sum_k ( (dy_k \cdot \gamma_k) \cdot \hat{x}_k ) \\ c_2 = \frac{1}{N} \sum_k d\hat{x}'_k &amp;= \frac{1}{N} \sum_k (dy_k \cdot \gamma_k) \\ d\hat{x}'_j &amp;= dy_j \cdot \gamma_j \end{aligned} }\] <hr/> <p>Hope this was helpful!</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="ML"/><category term="ml"/><category term="math"/><summary type="html"><![CDATA[Understanding the math behind Layer Normalization and deriving the gradients for the backward pass.]]></summary></entry><entry><title type="html">Notes from GTC’25: CUDA Techniques to Maximize Compute and Instruction Throughput</title><link href="https://shreyansh26.github.io/post/2025-04-04_gtc25-maximize-compute-instruction-throughput/" rel="alternate" type="text/html" title="Notes from GTC’25: CUDA Techniques to Maximize Compute and Instruction Throughput"/><published>2025-04-04T00:00:00+00:00</published><updated>2025-04-04T00:00:00+00:00</updated><id>https://shreyansh26.github.io/post/gtc25-maximizing-compute-instruction-throughput</id><content type="html" xml:base="https://shreyansh26.github.io/post/2025-04-04_gtc25-maximize-compute-instruction-throughput/"><![CDATA[<hr/> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/cover.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>You can watch the talk here - <a href="https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1727709629316001myds">link</a></p> <hr/> <h1 id="gpu-basics">GPU Basics</h1> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/hopper_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>PCIe is used for communication between the CPU and the GPU. <br/> NVLink is used for communication between the GPUs.</p> <p>Each SM of Hopper architecture has -</p> <ul> <li>4 sub-partitions</li> <li>128 FP32 units</li> <li>64 FP64 units</li> <li>64 INT32 units</li> <li>4 mixed-precision Tensor Cores</li> <li>16 special function units</li> <li>4 warp schedulers</li> <li>32 load/store units</li> <li>64K 32-bit registers</li> <li>256KB unified L1 cache and shared memory (however on checking CUDA device properties, I found that the shared memory is 228 KB)</li> <li>Tensor Memory Accelerator (TMA)</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/hopper_sm_small.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>In Hopper, in addition to blocks and grids, there is an optional level in the thread hierarchy called - <strong>Thread Block Clusters</strong>. Thread blocks in a cluster are guaranteed to be concurrently scheduled and enable efficient cooperation and data sharing for threads across multiple SMs.</p> <p>GPUs follow SIMT (Single Instruction, Multiple Threads) execution.</p> <ul> <li>Each thread has its own program counter.</li> <li>SIMT = SIMD + Program Counters</li> </ul> <p>Since Volta, each thread has its own program counter.</p> <h1 id="warp-divergence">Warp Divergence</h1> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/simt.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Metrics to look at (in NCU) to detect divergence:</p> <ul> <li>Average Predicated-On Threads Executed <ul> <li>At this instruction, how converged is my warp on average?</li> </ul> </li> <li>Divergent Branches <ul> <li>Number of times branch target differed</li> </ul> </li> <li>(Soon) Derivative Average Predicated-On Threads Executed <ul> <li>E.g. if in a piece of code - At to level, it diverges slightly (but only once) and stays diverged; and in lower level code, there is frequent divergence and re-convergence although less severely. Then, <ul> <li>Derivative metric has higher value for the top level divergence.</li> <li>Divergent Branch metric has higher value for the lower level code.</li> </ul> </li> </ul> </li> </ul> <h2 id="tips-for-reducing-warp-divergence">Tips for reducing warp divergence</h2> <p><strong>Causes and solutions</strong></p> <ul> <li>If per thread work is different - Queue and bin/sort the work</li> <li>If per thread work is discovered at different times - Queue the work</li> <li> <p>If per thread work ends at different times - Split into multiple kernels.</p> </li> <li>Implement conceptual divergence via varying data, instead of varying control flow.</li> <li>Consider algorithmic / higher order changes to reduce divergence.</li> </ul> <h3 id="work-queueing-in-shared-memory">Work queueing in shared memory</h3> <p>There are workloads where an expensive computational calculation has a lightweight check to guard against it. A naive implementation may suffer from high divergence as not all threads will have work that passes the check.</p> <p>Solution:</p> <ul> <li>When a threads finds a place to deep dive, add it to a queue and move on.</li> <li>Occasionally, all threads work simultaneously to clear the queue.</li> <li>Note: Threads that are finished scouting will then be used to help clear the queue.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/queueing.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/processing.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="conceptual-divergence">Conceptual Divergence</h3> <p>Simple example:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">x</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span>
<span class="k">if</span> <span class="p">(</span><span class="n">isA</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">valA</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">else</span> <span class="nf">if</span> <span class="p">(</span><span class="n">isB</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">valB</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>The above code has a divergence during assignment of value to <code class="language-plaintext highlighter-rouge">x</code>.</p> <p>Conversion to conceptual divergence:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">isA</span><span class="p">)</span> <span class="o">*</span> <span class="n">valA</span> <span class="o">+</span> <span class="p">(</span><span class="n">isB</span><span class="p">)</span> <span class="o">*</span> <span class="n">valB</span><span class="p">;</span>
</code></pre></div></div> <p>In this case, the result would be the same but we avoid the divergence by treating the boolean as scalar factors.</p> <h1 id="warp-scheduling-and-kernel-profiling">Warp scheduling and Kernel profiling</h1> <p>In Hopper,</p> <ul> <li>4 warp schedulers per SM</li> <li>Each scheduler manages a pool of 16 warps</li> <li>In each clock cycle, each scheduler can issue an instruction for 1 warp.</li> </ul> <p><strong>Warp States</strong></p> <ul> <li>Unused</li> <li>Active - Warp is resident on processor <ul> <li>Stalled - Warp is waiting for previous instructions to complete; for input data of next instruction to be produced</li> <li>Eligible - All data, etc. the warp needs to execute the next instruction is available</li> <li>Selected - Eligible and selected by the scheduler to issue instruction in the cycle</li> </ul> </li> </ul> <h2 id="warp-scheduler-statistics">Warp scheduler statistics</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/warp_scheduler_stats.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>For a kernel launched with 32 threads and one block, performing addition of a scalar to each element of an array - assuming the addition takes 8 cycles, the warp statistics are -</p> <ul> <li>warps_active = 8/8 = 1 (per scheduler)</li> <li>warps_stalled = 7/8</li> <li>warps_eligible = 1/8</li> <li>warps_selected = 1/8</li> </ul> <p>Context switching between warps is free from software perspective.</p> <ul> <li>Context is always resident on processor</li> <li>Switch is implemented in hardware</li> </ul> <h1 id="kernel-profiling">Kernel Profiling</h1> <p><strong>Compute Bound vs. Memory Bound vs. Latency Bound vs. Compute and Memory Bound</strong></p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/boundedness.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>What to look for in NCU for each of these boundedness conditions -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/ncu_boundedness.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="latency-hiding--increasing-instruction-throughput">Latency hiding / Increasing instruction throughput</h2> <p>Most time is spent waiting for instructions to finish, and hardware resources are underutilized.</p> <p>Need more instructions in flight at once to hide instruction latency and increase hardware utilization.</p> <h3 id="types-of-stalls-warp-stalls">Types of stalls (warp stalls)</h3> <ul> <li>Wait - Waiting for an instruction of compile-time-known latency</li> <li>Scoreboard - Waiting for an instruction of runtime-determined latency <ul> <li>Long Scoreboard - typically associated with global memory</li> <li>Short Scoreboard - typically associated with shared memory</li> </ul> </li> <li>Throttle - Waiting for the queue of a hardware resource to have free space</li> <li>Branch resolving - Waiting for branch / PC bookkeeping</li> <li>Barrier - Waiting for other threads to synchronize</li> </ul> <p>Prefetching / Software / register pipelining is one way to hide latency.</p> <h3 id="barriers">Barriers</h3> <p>Barriers are a location in the code for threads to stop and wait for each other before moving on.</p> <p><code class="language-plaintext highlighter-rouge">__syncthreads()</code> syncs entire thread block. Required to be called by all the threads in the block. It cannot be called within conditionals unless they evaluate identically across thread block. Otherwise it has undefined behavior.</p> <p>Cooperative Groups Sync - Syncs entire group defined by the user. Permitted to be called by only some threads and in divergent branches.</p> <h3 id="increasing-in-flight-instructions">Increasing in-flight instructions</h3> <ul> <li>Instruction Level Parallelism (ILP)</li> <li>Improve Occupancy - thread level parallelism <ul> <li>Determines how many warps can run concurrently given HW resource constraints</li> <li>More concurrently active warps = more in-flight instructions</li> </ul> </li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/lb_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/lb_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/lb_3.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Essentially - more in-flight instructions, then more in-flight bytes.</p> <h3 id="summary">Summary</h3> <p>If SM or Memory System resources are busy - don’t worry about stalls or unused issue slots. Issuing more frequently won’t help. Resources are already busy.</p> <p>Otherwise, you are latency bound. Provide HW with more concurrent work. Try to -</p> <ul> <li>Issue more frequently</li> <li>Stall less frequently</li> <li>Busy yourself with something else during the stall</li> <li>Decrease duration of stall (use lower latency instructions)</li> </ul> <h2 id="occupancy">Occupancy</h2> \[\text{Occupancy} = \frac{\text{Achievable # Active Warps per SM}}{\text{Device # Active Warps per SM}}\] <p>Achievable occupancy of a CUDA kernel will be limited by at least one of several factors -</p> <ul> <li>SM resource assignment (shared memory, register partitioning; block size)</li> <li>Hardware factors - max blocks per SM, max warps per SM, etc.</li> </ul> <h3 id="occupancy-limiters---registers">Occupancy Limiters - Registers</h3> <p>To get report of register usage, compile with <code class="language-plaintext highlighter-rouge">--ptxas-options=-v</code> flag.</p> <p>Maximum number of registers per thread can be set manually -</p> <ul> <li>At compile time using <code class="language-plaintext highlighter-rouge">--maxregcount</code> flag of nvcc (per-file basis)</li> <li>At runtime using <code class="language-plaintext highlighter-rouge">__launch_bounds__</code> or <code class="language-plaintext highlighter-rouge">__maxnreg__</code> qualifiers (per-kernel basis)</li> <li>Hopper has 64k (65536) registers per SM. These are allocated in fixed-size chunks of 256 registers.</li> </ul> <p>Example - If a kernel uses 63 registers per thread</p> <ul> <li>Registers per warp = 63 * 32 = 2016</li> <li>Registers allocated per warp = 2048 (rounded up to nearest multiple of 256)</li> <li>Achievable active warps per SM = 65536 / 2048 = 32</li> <li>Occupancy = 32 / 64 = 50%</li> </ul> <p><strong>Hopper supports up to 64 warps per SM</strong></p> <p>If compiler needs more registers for a kernel than is allowed by the device/specified, then it spills to <strong>local memory</strong>.</p> <p>Local memory is a thread-private storage space located in device memory and cached in L1 and L2. Local memory is at same level as global memory and hence slower.</p> <p>In NCU, “Live Registers” metric can show hot-spots of high register usage.</p> <p><strong>Tips for reducing register pressure</strong></p> <ul> <li><code class="language-plaintext highlighter-rouge">__forceinline</code> to avoid function call overheads and the ABI</li> <li>Tune loop unrolling - excessive unrolling can lead to excessive register usage</li> <li>Avoid 64-bit types wherever possible as they use two registers</li> <li>Check buffers in register if they can be moved to some other memory space (e.g. shared memory)</li> <li>Assign less work to individual threads</li> <li>Doing kernel fusion can also lead to increased register pressure</li> </ul> <h3 id="occupancy-limiters---thread-block-size">Occupancy Limiters - Thread Block size</h3> <p>Thread block size is a multiple of warp size (32). Even if you request fewer threads, hardware will round it up to the nearest multiple of 32.</p> <p>Each thread block can have a maximum size of 1024.</p> <p>Each SM in Hopper can have up to 64 warps, 32 blocks and 2048 threads.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/improve_occupancy.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="reducing-instruction-count">Reducing Instruction Count</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/reducing_instruction_count.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>Focus on all levels of the problem - source tweaks, algorithm changes, etc.</li> <li>Perform “inexpensive prechecks” to see if you can avoid expensive operations.</li> <li>Algebraic optimizations</li> <li>Operating in a different numeric space</li> <li>Use cccl for high performance primitives. Don’t reinvent the wheel.</li> <li>Vectorized instructions (memory operations, DPX, f32x2 on Blackwell)</li> </ul> <p>E.g. for an instruction bound kernel -</p> <ul> <li>Making it float4 made it 128 bit loads</li> <li>Increase in shared memory traffic and decrease in instructions</li> <li>And since instruction bound -&gt; better performance</li> </ul> <h3 id="math-optimizations">Math Optimizations</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/math_ops.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Use the precision that is required. If lower precision is acceptable, then use it.</p> <p>Beware of the implicit cast to double. Use the <code class="language-plaintext highlighter-rouge">.f</code> suffix on the numeric literals to avoid it.</p> <p>Make use of the fast math optimizations - <code class="language-plaintext highlighter-rouge">--use-fast-math</code></p> <ul> <li>Single Precision Intrinsics - <code class="language-plaintext highlighter-rouge">__cosf()</code>, <code class="language-plaintext highlighter-rouge">__expf()</code>, <code class="language-plaintext highlighter-rouge">__fsqrt_*__()</code>, etc.</li> <li>Single precision trigonometric math API functions may use some double precision instructions and local memory.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/math_fast.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="algebraic-optimizations">Algebraic Optimizations</h3> <p><strong>Static Considerations</strong></p> <ul> <li>Move divisors to the other side of comparison operators (division is expensive)</li> <li>If you have division by a run time constant, compute inverse on host and pass to kernel to multiply.</li> <li>Use template parameters for any variable known at compute time or with a limited range of values. Runtime compilation can take this even further.</li> </ul> <p><strong>Runtime Considerations</strong></p> <p>If possible, provide the compiler with hints which the user knows about e.g., the possible range of values produced by an expression and figure out if any optimizations are possible.</p> <p><strong>Interesting Example</strong> - Use signed integers rather than unsigned integers as loop counters. Reason - unsigned int overflows are defined behavior and the compiling needs to account for this resulting in possible extra instructions. Since int overflows are undefined behavior, the compiler has more flexibility to generate faster code.</p> <h3 id="operating-in-a-different-numeric-space">Operating in a different numeric space</h3> <ul> <li>Use log probabilities for improved accuracy and performance</li> <li>Comparing squared distances rather than distances to avoid <code class="language-plaintext highlighter-rouge">sqrt</code></li> </ul> <h3 id="optimizing-polynomial-evaluation">Optimizing polynomial evaluation</h3> <ul> <li>Use Horner’s method for polynomial evaluation.</li> <li>Take care of precision using the <code class="language-plaintext highlighter-rouge">.f</code> suffix on numeric literals.</li> <li>Use <code class="language-plaintext highlighter-rouge">__fma()</code> for polynomial evaluation.</li> <li>Use <code class="language-plaintext highlighter-rouge">__fmaf()</code> for single precision polynomial evaluation.</li> <li>Or use <code class="language-plaintext highlighter-rouge">fmad=True</code> in the compiler flags.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/horner.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/fma.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/estrin.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h1 id="tensor-cores-overview">Tensor Cores Overview</h1> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/tc1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>A, B, C will be distributed among the registers of the warp</p> <h2 id="history-of-tensor-cores">History of Tensor Cores</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/tc2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="tensor-core-providers">Tensor Core Providers</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_compute_throughput/tc3.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr/> <p>Hope this was helpful!</p> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="MLSys"/><category term="cuda"/><category term="mlsys"/><summary type="html"><![CDATA[My notes from the talk on maximizing compute and instruction throughput at NVIDIA GTC 2025.]]></summary></entry><entry><title type="html">Notes from GTC’25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency - Part 1</title><link href="https://shreyansh26.github.io/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/" rel="alternate" type="text/html" title="Notes from GTC’25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency - Part 1"/><published>2025-03-23T00:00:00+00:00</published><updated>2025-03-23T00:00:00+00:00</updated><id>https://shreyansh26.github.io/post/gtc25-maximizing-memory-throughput-part1</id><content type="html" xml:base="https://shreyansh26.github.io/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/"><![CDATA[<hr/> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/cover.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>You can watch the talk here - <a href="https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1727709012449001X6PZ">link</a></p> <p>The talk had two major sections - maximizing memory throughput, and memory models and hiding latency. For clarity of thought and understanding, I will split my notes into two parts.</p> <p>Refer to part 2 of the notes <a href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-2/">here</a>.</p> <hr/> <h1 id="maximizing-memory-throughput">Maximizing Memory Throughput</h1> <hr/> <p>As GPU generations are progressing -</p> <ul> <li>Bandwidth is increasing rapidly</li> <li># SMs is increasing slowly</li> </ul> <p>This implies, <strong>more bandwidth per SM is available to saturate</strong>.</p> <p><strong>Little’s Law</strong> - The mean number if units in a system is equal to the (mean arrival rate) * (mean residency time) <br/> <strong>Little’s Law for GPU memory</strong> - <code class="language-plaintext highlighter-rouge">bytes-in-flight = bandwidth * mean latency</code> * Bandwidth and Mean latency are determined by hardware. Bytes-in-flight can be controlled in software. * More bytes-in-flight are required to saturate DRAM bandwidth increases with every generation. Mainly due to bandwidth increase. It went 2x from Hopper to Blackwell.</p> \[\text{estimated bytes-in-flight / SM} = \text{(# bytes / load)} \times \text{(# loads / thread)} \times \text{(# threads / block)} \times \text{(# blocks / SM)}\] <p>bytes-in-flight per SM required to saturate bandwidth -</p> <ul> <li>Hopper - \(&gt; 32\) KiB</li> <li>Blackwell - \(&gt; 40\) KiB</li> </ul> <h1 id="how-to-increase-bytes-in-flight">How to increase Bytes-in-Flight?</h1> <ul> <li><strong>Instruction Level Parallelism (ILP) - More independent memory operations within a thread</strong></li> <li><strong>Data Level Parallelism (DLP) - Vectorized memory operations within a thread</strong></li> <li><strong>Asynchronous Data Copies</strong></li> </ul> <h2 id="increasing-ilp">Increasing ILP</h2> <p><strong>Loop Unrolling</strong> <br/> Sometimes manual unrolling may be needed</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/pragma_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/pragma_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="increasing-dlp">Increasing DLP</h2> <p><strong>Using vectorized loads</strong></p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/dlp_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/dlp_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Hopper has a 128 byte cache line for both L1 and L2 caches.</p> <p>Vectorized global and shared memory accesses require aligned data. Can be 64 or 128 bit width (float2, float4).</p> <p>Implicit casting to vector pointers can also be used.</p> <h2 id="increasing-ilp-and-dlp-increase-register-pressure">Increasing ILP and DLP increase register pressure</h2> <p>The previous techniques mentioned above increase the byte-in-flight at the cost of increased register usage.</p> <ul> <li>Bytes-in-flight need to be backed by registers.</li> <li>May lead to register spills to local memory.</li> </ul> <p>Newer generation of GPUs need higher levels of ILP and DLP =&gt; more registers to saturate the memory bandwidth.</p> <p>Therefore, not many registers are left for computation.</p> <ul> <li>For low compute intensity kernels, this may not be a problem.</li> <li>For high compute intensity kernels, this may lead to low occupancy and register spilling to local memory.</li> </ul> <h1 id="asynchronous-data-copies">Asynchronous Data Copies</h1> <p>Asynchronous data copies are a way to skip the registers and go directly to shared memory.</p> <ul> <li>Free up more registers for computation.</li> <li>Reduce L1 traffic</li> <li>Reduce MIO pressure (less instructions)</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/async_copy.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>This helps us to overlap compute and data loads.</p> <ul> <li>Data prefetching - distance of \(n\)</li> <li>Producer-Consumer pattern</li> </ul> <p>For the <code class="language-plaintext highlighter-rouge">GMem -&gt; SMem -&gt; Computation in SMem -&gt; Write results to GMem</code> pattern, async copies can give large benefits when the kernel is iterative. i.e. if the prefetching can be done for future iterations.</p> <ul> <li>Especially for low occupancy compute-heavy kernels.</li> </ul> <p>Mechanisms for async copies -</p> <ul> <li>LDGSTS in Ampere+ (Async Copy)</li> <li>TMA in Hopper+ (Bulk Async Copy)</li> </ul> <h2 id="ldgsts">LDGSTS</h2> <p>It is an async version of <code class="language-plaintext highlighter-rouge">smem[sidx] = gmem[gidx]</code>.</p> <ul> <li>Supports copying of 4, 8, 16 bytes at a time.</li> <li>Two modes - <ul> <li>L1 Bypass - the accesses invalidate / bypass the L1 cache <ul> <li>size of datatype and alignment should be 16 bytes</li> </ul> </li> <li>L1 Access - the accesses go through L1 <ul> <li>size of datatype and alignment should be 4 or 8 bytes</li> </ul> </li> </ul> </li> <li>Compiler prefers L1 Bypass mode if requirements are met.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ldgsts_1.png" alt="APIs for LDGSTS"/> <figcaption>APIs for LDGSTS</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ldgsts_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><code class="language-plaintext highlighter-rouge">__pipeline_commit</code> associates a barrier with previous memcopies. <br/> <code class="language-plaintext highlighter-rouge">__pipeline_wait_prior</code>’s argument indicates the number of data transfers to wait for except the last \(n\). If \(n = 0\), it waits for all.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ldgsts_3.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Needs an explicit barrier. Initialized with block size as all threads are going to arrive at the block. <br/> Libcudacxx takes the barrier as input as well in <code class="language-plaintext highlighter-rouge">memcpy_async</code>. <br/> Also, the libcudacxx API, can handle greater than 16 bytes under the hood.</p> <h3 id="data-prefetching-with-ldgsts">Data Prefetching with LDGSTS</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ldgsts_prefetching_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><code class="language-plaintext highlighter-rouge">acquire</code> and <code class="language-plaintext highlighter-rouge">commit</code> are called in the converged code - not in the divergent code.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ldgsts_prefetching_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>There are two stages here and we wait for all except the last fetch i.e the previous iteration’s fetch (hence the \(1\) in <code class="language-plaintext highlighter-rouge">__pipeline_wait_prior()</code>).</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ldgsts_prefetching_mts_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The prefetching distance can be increased beyond 1. The \(\text{NUM_STAGES}\) is the number of stages in the pipeline.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ldgsts_prefetching_mts_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Always wait for the oldest stage.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ldgsts_prefetching_mts_4_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>If we can copy 16 bytes at a time, i.e. float4, then we can use the L1 Bypass mode. <br/> Less threads will be doing 16 byte loads. But all of them will be involved in the computation.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ldgsts_prefetching_mts_4_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Because of producer-consumer, we need to synchronize before start of computation because a different set of threads will be doing the fetch.</p> <p>Coming back to our equation for bytes-in-flight per SM,</p> \[\text{estimated bytes-in-flight / SM} = \text{(# bytes / load)} \times \text{(# loads / thread)} \times \text{(# threads / block)} \times \text{(# blocks / SM)}\] <p>\(\text{estimated bytes-in-flight / SM}\) is fixed - \(&gt; 32\) KiB for Hopper and \(&gt; 40\) KiB for Blackwell. <br/> \(\text{(# bytes / load)}\) is fixed <br/> Similarly, \(\text{(# threads / block)}\) and \(\text{(# blocks / SM)}\) are fixed - based on occupancy.</p> <p>So, we need to control \(\text{(# loads / thread)}\) is tunable and is dependent on the number of stages. For the above example, it is (2 * number of stages).</p> <h2 id="tma-tensor-memory-accelerator">TMA (Tensor Memory Accelerator)</h2> <p>TMA is an efficient data transfer mechanism for bulk copies. There are two programming models -</p> <ul> <li>Bulk async copy of one-dimensional contiguous arrays (TMA 1D)</li> <li>Bulk async copy of multi-dimensional arrays (TMA 2D)</li> </ul> <p>The programming model is <strong><em>warp uniform</em></strong>.</p> <ul> <li>Unlike LDGSTS, it is more efficient to call TMA from a single thread per warp.</li> <li>If more threads per warp are active, the compiler will generate a peeling loop to execute each TMA operation sequentially.</li> </ul> <h3 id="tma-1d-ublkcp">TMA 1D (UBLKCP)</h3> <p>Uses shared memory barriers. <br/> Alignment requirements -</p> <ul> <li>Source and destination pointers should be 16 byte aligned.</li> <li>Copy size should be a multiple of 16 bytes.</li> </ul> <p>API libcudacxx <code class="language-plaintext highlighter-rouge">&lt;cuda/ptx&gt;</code></p> <ul> <li><code class="language-plaintext highlighter-rouge">cuda::memcpy_async()</code> combined with <code class="language-plaintext highlighter-rouge">cuda::barrier</code></li> <li><code class="language-plaintext highlighter-rouge">cuda::device::memcpy_async_tx()</code> combined with <code class="language-plaintext highlighter-rouge">cuda::barrier</code></li> <li><code class="language-plaintext highlighter-rouge">cuda::ptx</code> can be used for finer-grain barrier synchronization using PTX.</li> </ul> <p>Also enabled in <code class="language-plaintext highlighter-rouge">thrust::transform</code> in CCCL. Thrust will internally auto-tune the bytes-in-flight.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ublkcp.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/tma1d_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>As shown above, only one thread launches the copy. <br/> Each thread receives a token which it uses to wait on the barrier. <br/> <code class="language-plaintext highlighter-rouge">memcpy_async</code> has a fallback mechanism to handle the case when the copy size and/or alignment is not a multiple of 16 bytes.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/tma1d_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>However in <code class="language-plaintext highlighter-rouge">memcpy_async_tx</code> there is an undefined behavior if the copy size and/or alignment is not a multiple of 16 bytes. <br/> So, generally it is safer to use <code class="language-plaintext highlighter-rouge">memcpy_async</code>.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/tma1d_ptx_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>PTX gives finer control over synchronization. <br/> Using the PTX API, we can only have one thread arrive at the barrier and inform the barrier how many bytes it is expected to transfer. <br/> Waiting here is faster as we are essentially checking if data has arrived or not and not if all threads have completed. This is faster than <code class="language-plaintext highlighter-rouge">barrier.wait()</code>.</p> <p><strong>Interestingly</strong>, when we launch TMA using only one thread, compiler does not know that this is true and still generates a peeling loop.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/tma1d_ptx_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>For this, the <code class="language-plaintext highlighter-rouge">invoke_one</code> function from cooperative groups API is used.</p> <h3 id="using-async-copies-for-batched-computation">Using Async copies for batched computation</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/tma_async_batch_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Thrust gives a zero-effort way to do this as well using <code class="language-plaintext highlighter-rouge">thrust::transform</code>.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/tma_async_batch_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="data-prefetching-with-tma-1d">Data Prefetching with TMA 1D</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/tma_prefetching_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Can’t use pipeline construct and need to have two shared memory barriers (for two stages).</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/tma_prefetching_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="optimization-guidelines-mindmap">Optimization Guidelines Mindmap</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/async_copy_summary.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/optimization_flowchart.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>If registers are available (and we are not limited by register pressure), we should do unrolling and vectorization.</p> <p>If registers are not available, we can use shared memory.</p> <p>Size of chunk of data can determine LDGSTS or TMA.</p> <ul> <li>More than 2KiB -&gt; TMA</li> <li>Less than 1KiB -&gt; LDGSTS</li> <li>Between 1KiB and 2KiB -&gt; Benchmark both and decide.</li> </ul> <hr/> <p>Hope this was helpful!</p> <p>Refer to part 2 of the notes <a href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-2/">here</a>.</p> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="MLSys"/><category term="cuda"/><category term="mlsys"/><summary type="html"><![CDATA[First part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.]]></summary></entry><entry><title type="html">Notes from GTC’25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency - Part 2</title><link href="https://shreyansh26.github.io/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-2/" rel="alternate" type="text/html" title="Notes from GTC’25: CUDA Techniques to Maximize Memory Bandwidth and Hide Latency - Part 2"/><published>2025-03-23T00:00:00+00:00</published><updated>2025-03-23T00:00:00+00:00</updated><id>https://shreyansh26.github.io/post/gtc25-maximizing-memory-throughput-part2</id><content type="html" xml:base="https://shreyansh26.github.io/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-2/"><![CDATA[<hr/> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/cover.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>You can watch the talk here - <a href="https://register.nvidia.com/flow/nvidia/gtcs25/vap/page/vsessioncatalog/session/1727709012449001X6PZ">link</a></p> <p>Part 1 of the talk focused on maximizing memory throughput. The notes can be found <a href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/">here</a>.</p> <p>These are the notes for the second part of the talk which focused on memory models and hiding latency.</p> <hr/> <h1 id="memory-model">Memory Model</h1> <hr/> <p>Memory model is a way to understand how memory is accessed and used in a program. It is a contract between the user and the compiler/hardware/language.</p> <h2 id="single-threaded">Single-threaded</h2> <p>Standard memory model. <br/> Stores are visible to the thread that stored them. <br/> Loads and stores to the same address remain in order - they cannot overtake each other in the memory subsystem.</p> <p>Important concept - <strong>same-address ordering</strong>. <br/> Same-address ordering does not hold always. E.g. when using constant caches. The constant caches have a link to the L2 cache but not to the L1 cache. Hence, these caches are not coherent.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/constant_cache.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>So, constant cached values can cause issues. You can do the store - which would go through L1 to the L2 and update it. However, during load, the constant cache is used and it returns the old value.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/constant_cache_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="memory-ordering">Memory Ordering</h2> <p>Memory order specifies how memory accesses, including regular (non-atomic) accesses, are to be ordered around an atomic operation.</p> <p>Four important memory orders in multi-threaded memory model:</p> <ol> <li>Sequentially consistent</li> <li>Acquire</li> <li>Release</li> <li>Relaxed</li> </ol> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/memory_ordering.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="multi-threaded">Multi-threaded</h2> <h3 id="cuda-c-scope">CUDA C++ Scope</h3> <ul> <li>Thread - <code class="language-plaintext highlighter-rouge">cuda::thread_scope_thread</code> - Only local thread can observe this thread’s loads and stores</li> <li>Thread Block - <code class="language-plaintext highlighter-rouge">cuda::thread_scope_block</code> - Only threads in the same block can observe this thread’s loads and stores</li> <li>GPU Device - <code class="language-plaintext highlighter-rouge">cuda::thread_scope_device</code> - All threads in the GPU can observe this thread’s loads and stores</li> <li>System - <code class="language-plaintext highlighter-rouge">cuda::thread_scope_system</code> - All threads in the system (CPU, other GPUs, other nodes) can observe this thread’s loads and stores</li> </ul> <h3 id="cuda-ptx-scope">CUDA PTX Scope</h3> <ul> <li>Thread Block - <code class="language-plaintext highlighter-rouge">.cta</code> - Only threads in the same block can observe this thread’s loads and stores</li> <li>GPU Device - <code class="language-plaintext highlighter-rouge">.gpu</code> - All threads in the GPU can observe this thread’s loads and stores</li> <li>System - <code class="language-plaintext highlighter-rouge">.sys</code> - All threads in the system (CPU, other GPUs, other nodes) can observe this thread’s loads and stores</li> </ul> <h3 id="thread-scope---block">Thread scope - Block</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ts_block.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Threads in the same block execute on same SM. <br/> Data only has to be consistent in L1. All threads in the block see the same data. <br/> Release and acquire semantics are quite fast. Because data does not have to be flushed very far. We don’t have to invalidate many caches.</p> <h3 id="thread-scope---cluster">Thread scope - Cluster</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ts_cluster.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Many threads working across multiple SMs working together. <br/> Data has to go through L2. <br/> <strong>In release, we would have to flush to L2 and in acquire, we would have to make sure that L1 is invalidated.</strong></p> <h3 id="thread-scope---gpu">Thread scope - GPU</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ts_system.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Many threads working across multiple SMs of a GPU working together. <br/> Synchronization is as difficult as cluster. <br/> <strong>In release, we would have to flush to L2 and in acquire, we would have to make sure that L1 is invalidated.</strong></p> <h3 id="thread-scope---system">Thread scope - System</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/ts_system.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Many threads working across multiple GPUs working together. <br/> <strong>In release, we would have to make sure that all the stores made it to the relevant caches across GPUs and nodes.</strong> <br/> <strong>Acquire is still cheap, all L1s need to be invalidated.</strong></p> <h3 id="data-transfer-examples">Data transfer examples</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/relaxed_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/relaxed_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Using thread scope <code class="language-plaintext highlighter-rouge">block</code> when working with same block.</p> <p>Using thread scope <code class="language-plaintext highlighter-rouge">device</code> when working with different thread blocks.</p> <p>But, for a not so relaxed example, where there are two variables we need to work with, simply using <code class="language-plaintext highlighter-rouge">device</code> scope is not enough.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/not_relaxed_3.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>We need to use a release-acquire pattern.</strong></p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/not_relaxed_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="relaxed-vs-release-acquire">Relaxed vs Release-Acquire</h3> <p><strong>Relaxed</strong></p> <ul> <li>Faster - A single store or load to or from the cache at the point of coherency.</li> <li>Does not provide ordering w.r.t other reads and writes.</li> <li>Useful if two threads want to exchange one value.</li> </ul> <p><strong>Release-Acquire</strong></p> <ul> <li>Slower - Requires flushing to point of coherency and / or invalidating caches.</li> <li>Provides ordering w.r.t other reads and writes.</li> <li>Useful if multiple threads want to exchange multiple values.</li> </ul> <p>For larger chunks of data, release-acquire is preferred.</p> <h2 id="async-thread---ampere">Async thread - Ampere</h2> <p><strong>PTX instruction <code class="language-plaintext highlighter-rouge">st.async</code></strong></p> <ul> <li>Stores a value to Distributed Shared Memory of another block in the cluster/</li> <li>Once the store is complete, it updates a shared memory barrier in the shared memory of the other block.</li> </ul> <p><strong>However, a subsequent load or store can race ahead, violating the same-address ordering.</strong></p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/store_async.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="async-proxy---hopper">Async proxy - Hopper</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/async_proxy.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Proxies represent situations where there are multiple different paths from a single thread to a single physical memory location, with no coherence across paths.</p> <p><strong>Generic Proxy</strong> - All normal loads and stores go through the generic proxy. <br/> <strong>Async Proxy</strong> - A different path that is used by TMA units, tensor cores and several other instructions.</p> <p>Between a generic proxy load/store and an async proxy load/store, there is <strong>no same-address ordering</strong>. Even less than earlier (async threads).</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/async_proxy_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The normal store can overtake the TMA load.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/async_proxy_code_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Here, the generic proxy store to shared memory will be most likely overtaken by async proxy load from shared memory. <br/> This will store stale values to global memory.</p> <h3 id="async-proxy-fence">Async Proxy Fence</h3> <p><strong>Solution is to use an async proxy fence</strong> -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/async_proxy_2_fence.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The fence traces the store to shared memory, and makes sure that the store is complete. Once it is complete, the fence comes back, notifies the thread and only then will the TMA load be allowed to proceed.</p> <p><strong>Implicit Fencing</strong></p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/async_implicit_fence.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Here we start waiting on the barrier after the copy async bulk is issued. Barrier waiting request goes to to the shared memory until the load is finished. Only when all the required updates to the shared memory are done (stores), the barrier is updated.</p> <h2 id="async-thread-and-async-proxy-instructions">Async thread and Async proxy instructions</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/async_thread_proxy_instructions.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li><code class="language-plaintext highlighter-rouge">st.async</code> and <code class="language-plaintext highlighter-rouge">red.async</code> are in Hopper but still async thread only</li> <li><code class="language-plaintext highlighter-rouge">cp.async</code> - Ampere</li> <li>If you have a normal load and store before - obeys same-address ordering</li> <li>But normal load and store after - it will not obey</li> <li>Async proxy fence is still needed to ensure correct ordering</li> </ul> <h1 id="low-latency-cluster-synchronization">Low-Latency Cluster Synchronization</h1> <hr/> <p><strong>Key points</strong></p> <ul> <li>The point of coherency for a cluster is L2 - thread blocks can be in different SMs</li> <li>Any release-acquire pattern with cluster scope requires a round trip to L2 which is expensive</li> <li>To reduce latency - avoid these round trips</li> </ul> <h2 id="thread-synchronization-in-a-cluster">Thread synchronization in a cluster</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/low_latency_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Arrive has to be executed by all threads in a cluster but wait doesn’t need to be. <br/> The arrive can have different memory model orderings.</p> <ul> <li>Release - Requires flushing to L2 but gives synchronization of data</li> <li>Relaxed - Only execution synchronization but no data synchronization</li> </ul> <h3 id="barrier-initialization---simple-way">Barrier Initialization - Simple way</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/low_latency_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Initializing a shared memory barrier and making it visible to all threads in the cluster. <br/> A cluster sync is done to make the barrier visible to all threads. <br/> Nothing needs to be flushed to L2 =&gt; this is more expensive than it has to be.</p> <h3 id="barrier-initialization---fast-way">Barrier Initialization - Fast way</h3> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/low_latency_3.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Instead of <code class="language-plaintext highlighter-rouge">cluster::sync</code>, we use a <strong>relaxed arrive</strong> which does not flush anything to L2, but ensures execution synchronization. <br/> But to ensure correctness, we do a release fence of just the mbarrier init. <br/> Additionally there is a release-acquire pattern and they have to be scope clusters. <br/> <code class="language-plaintext highlighter-rouge">fence_mbarrier_init</code>, <code class="language-plaintext highlighter-rouge">arrive</code> and <code class="language-plaintext highlighter-rouge">wait</code> are all fairly cheap.</p> <p>For kernels which are short, this type of optimization can help a lot. However, for long kernels, this won’t help much.</p> <h2 id="data-communication-in-a-cluster">Data communication in a cluster</h2> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/data_comm_1.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/data_comm_2.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>Arrival should be relaxed and scope_cluster. If it were a release, then it would have a flush to L2.</li> <li>Wait from other cluster should be acquire (in a loop) so that it can form a release-acquire pattern with <code class="language-plaintext highlighter-rouge">st_async</code>. As <code class="language-plaintext highlighter-rouge">st_async</code> just releases the 4 bytes it has stored and that’s what we acquire in the <code class="language-plaintext highlighter-rouge">mbarrier_try_wait</code> which is also a scope cluster and you wait on the local barrier which is cheap.</li> <li>FInally, we need to make sure the other thread in the cluster got our value before we send another. This can be relaxed as we just need to ensure execution synchronization.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gtc_memory_bandwidth/data_comm_3.png" alt="Source: Slides from the talk"/> <figcaption>Source: Slides from the talk</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>But again, this helps only for short kernels. For long kernels, this won’t help much. We can fo go for the simple code.</p> <hr/> <p>Hope this was helpful!</p> <p>Notes for part 1 on maximizing memory bandwidth can be found <a href="/post/2025-03-23_gtc25-maximize-memory-bandwidth-part-1/">here</a>.</p> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="MLSys"/><category term="cuda"/><category term="mlsys"/><summary type="html"><![CDATA[Second part of my notes from the talk on maximizing memory bandwidth at NVIDIA GTC 2025.]]></summary></entry><entry><title type="html">Faster Cross-Encoder Inference: Unleashing torch.compile for speed</title><link href="https://shreyansh26.github.io/post/2025-03-02_cross-encoder-inference-torch-compile/" rel="alternate" type="text/html" title="Faster Cross-Encoder Inference: Unleashing torch.compile for speed"/><published>2025-03-02T00:00:00+00:00</published><updated>2025-03-02T00:00:00+00:00</updated><id>https://shreyansh26.github.io/post/cross-encoder-inference-optimization</id><content type="html" xml:base="https://shreyansh26.github.io/post/2025-03-02_cross-encoder-inference-torch-compile/"><![CDATA[<hr/> <p><strong>Code</strong> - <a href="https://github.com/shreyansh26/Accelerating-Cross-Encoder-Inference">Github repo</a></p> <hr/> <p>When deploying large ML models in production, optimization becomes crucial for maintaining both performance and cost-effectiveness. In this post, I’ll share my experience optimizing the inference of a cross-encoder (reranker) model using torch.compile and a custom batching strategy. We’ll explore how combining torch.compile with careful input handling can significantly improve inference speed.</p> <h2 id="the-setup-cross-encoder-neural-reranker-model">The Setup: Cross-Encoder (Neural Reranker) Model</h2> <p>For this experiment, I used the Jina reranker model (<code class="language-plaintext highlighter-rouge">jinaai/jina-reranker-v2-base-multilingual</code>), which is designed for scoring the similarity between text pairs. Such type of models are used in a lot of applications like information retrieval, semantic search, recommender systems, etc. The model takes pairs of text as input and outputs similarity scores. Here’s what makes this use case interesting:</p> <ol> <li>Variable input lengths (here we assume each text contains 2-15 sentences)</li> <li>Batch processing</li> </ol> <p>While running inference at scale, even the smallest of optimizations can make a huge difference.</p> <blockquote> <p><strong>Note</strong> - The optimizations and the techniques described in this post are not silver bullets for model inference optimization. Models may have different architectures and inference algorithms which can completely change how they can be optimized. However, the general principles described in this post would definitely hold.</p> </blockquote> <h2 id="understanding-torchcompile-and-the-inductor-backend">Understanding torch.compile and the Inductor Backend</h2> <p>PyTorch 2.0 (and onwards) comes with <code class="language-plaintext highlighter-rouge">torch.compile</code>. Although there are a <ins><a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">bunch</a></ins> <ins><a href="https://pytorch.org/docs/stable/torch.compiler.html">of resources</a></ins> to understand how it works, in short, torch.compile JIT (just in time) compiles your model and makes your Pytorch code run faster by using optimizations like operation fusion, graph capture, custom triton kernels, etc.</p> <p>There are various choices of backends for torch.compile. I used the <code class="language-plaintext highlighter-rouge">inductor</code> backend in my experiments as it is also the most advanced Pytorch-native backend at the moment. Let’s understand how it works:</p> <h3 id="how-inductor-works">How Inductor Works</h3> <p>At its core, Inductor optimizes your PyTorch model through several key steps:</p> <ol> <li><strong>Graph Capture</strong>: (TorchDynamo) When you first run your compiled model, Inductor captures the computational graph of your operations.</li> <li><strong>Operation Fusion</strong>: (TorchDynamo) Multiple operations are combined where possible to reduce memory transfers.</li> <li><strong>Hardware-Specific Optimization</strong>: (TorchInductor) The backend generates optimized kernels specifically for your GPU.</li> </ol> <p>Here’s how we set up our compiled model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_compile</span> <span class="o">=</span> <span class="nc">DynamicCrossEncoder</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">jinaai/jina-reranker-v2-base-multilingual</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="sh">"</span><span class="s">cuda</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">config_args</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">use_flash_attn</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span><span class="p">}</span>
<span class="p">)</span>

<span class="n">model_compile</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">forward</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">model_compile</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">forward</span><span class="p">,</span> 
    <span class="n">backend</span><span class="o">=</span><span class="sh">"</span><span class="s">inductor</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">max-autotune</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">dynamic</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
</code></pre></div></div> <p>The key parameters we’re using:</p> <ul> <li><code class="language-plaintext highlighter-rouge">backend="inductor"</code></li> <li><code class="language-plaintext highlighter-rouge">mode="max-autotune"</code>: Enables aggressive optimization</li> <li><code class="language-plaintext highlighter-rouge">dynamic=True</code>: Handles our variable input sizes</li> </ul> <p>If you’re curious as to why we set <code class="language-plaintext highlighter-rouge">use_flash_attn = False</code>, I discuss it in a <a href="#but-why-set-use_flash_attn--false">later section</a> after describing the optimizations and results.</p> <h2 id="smart-batching-with-length-buckets">Smart Batching with Length Buckets</h2> <p>Having static shapes is ideal for torch.compile. If there are a variations in the sizes of the variables, then TorchDynamo will have to trace all such variations. Keeping the number of size variations minimum while still giving enough flexibility will be our goal.</p> <p>One way to do it is, depending on the lengths of our sentences in the dataset, we can decide to keep a static sequence length for the model by specifying the <code class="language-plaintext highlighter-rouge">max_length</code> parameter while initializing the cross encoder. This length could be the maximum sequence length or a high enough length that covers most sequences (the ones longer would be truncated), The main issue with this approach is that for sequence lengths much smaller than the fixed length (which could be a significant portion of the dataset), we would be wasting a lot of compute on the padding tokens.</p> <p>In our experiment, we tackle this by creating sequence-length buckets for padding. Instead of padding all sequences to the maximum length in the batch, we pad to the nearest multiple of 16. Obviously this is not perfect, but in my experience of using cross encoders, I find that a max-length of 512 is enough for most practical use cases where a reranker works effectively. In case we do need longer sequence lengths, I would recommend increasing the bucket size from 16 to 32 or even higher based on the maximum length we need.</p> <p>Here’s our implementation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">BUCKETS</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">528</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">smart_batching_collate_text_only</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">texts</span> <span class="o">=</span> <span class="p">[[</span><span class="n">text</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">field</span><span class="p">]</span> <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)]</span>
    <span class="n">tokenized</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">tokenizer</span><span class="p">(</span>
        <span class="o">*</span><span class="n">texts</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="sh">"</span><span class="s">longest_first</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">return_tensors</span><span class="o">=</span><span class="sh">"</span><span class="s">pt</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">max_length</span>
    <span class="p">)</span>
    <span class="n">tokenized</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tokenized</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>

    <span class="c1"># Pad to nearest bucket
</span>    <span class="n">cur_length</span> <span class="o">=</span> <span class="n">tokenized</span><span class="p">[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">].</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">bucket_length</span> <span class="o">=</span> <span class="nf">next</span><span class="p">((</span><span class="n">b</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">BUCKETS</span> <span class="k">if</span> <span class="n">b</span> <span class="o">&gt;=</span> <span class="n">cur_length</span><span class="p">),</span> <span class="n">cur_length</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">bucket_length</span> <span class="o">&gt;</span> <span class="n">cur_length</span><span class="p">:</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">bucket_length</span> <span class="o">-</span> <span class="n">cur_length</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">tokenized</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">pad_value</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="n">tokenized</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">diff</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="n">pad_value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tokenized</span>
</code></pre></div></div> <p>This bucketing approach helps in two ways:</p> <ol> <li>Reduces wasted computation on padding tokens</li> <li>Helps the compiled model optimize for specific input sizes</li> </ol> <h2 id="input-sorting-for-better-efficiency">Input Sorting for Better Efficiency</h2> <p>To further improve performance, we implemented input sorting. This groups similarly-sized inputs together, making our bucket-based padding more effective:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">on_sorted_inputs</span><span class="p">:</span>
    <span class="c1"># Sort by max length of each pair
</span>    <span class="n">lengths</span> <span class="o">=</span> <span class="p">[(</span><span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span> <span class="o">+</span> <span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span> <span class="n">i</span><span class="p">)</span> 
                <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">sentence_pairs</span><span class="p">)]</span>
    <span class="n">sorted_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>
    <span class="n">sentence_pairs_sorted</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence_pairs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sorted_indices</span><span class="p">]</span>
</code></pre></div></div> <h2 id="but-why-set-use_flash_attn--false">But why set <code class="language-plaintext highlighter-rouge">use_flash_attn = False</code>?</h2> <p>While Flash Attention is generally faster than vanilla attention implementations, there are several technical reasons why I disabled it when using torch.compile for this particular optimization:</p> <h3 id="1-variable-sequence-lengths-complicate-tracing">1. Variable sequence lengths complicate tracing</h3> <p>Flash Attention operates through highly optimized CUDA kernels that are already compiled for performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In FlashSelfAttention, from mha.py - showing Flash Attention's compiled nature
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/mha.py
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">qkv</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">cu_seqlens</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">max_seqlen</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1"># ...
</span>    <span class="k">if</span> <span class="n">unpadded</span><span class="p">:</span>
        <span class="c1"># Using pre-compiled CUDA kernel
</span>        <span class="k">return</span> <span class="nf">flash_attn_varlen_qkvpacked_func</span><span class="p">(</span>
            <span class="n">qkv</span><span class="p">,</span>
            <span class="n">cu_seqlens</span><span class="p">,</span>
            <span class="n">max_seqlen</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">drop</span><span class="p">.</span><span class="n">p</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="n">softmax_scale</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">softmax_scale</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
            <span class="c1"># ...
</span>        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Using pre-compiled CUDA kernel
</span>        <span class="k">return</span> <span class="nf">flash_attn_qkvpacked_func</span><span class="p">(</span>
            <span class="n">qkv</span><span class="p">,</span>
            <span class="n">self</span><span class="p">.</span><span class="n">drop</span><span class="p">.</span><span class="n">p</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="n">softmax_scale</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">softmax_scale</span><span class="p">,</span>
            <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
            <span class="c1"># ...
</span>        <span class="p">)</span>
</code></pre></div></div> <p>The goal of our bucketing strategy was to have a consistent and a small number of tensor shapes for efficient compilation. However, when using <code class="language-plaintext highlighter-rouge">flash_attn_varlen_qkvpacked_func</code> the unpadding mechanism in the original Flash Attention implementation leads to dynamic tensor shapes that are difficult to trace:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From xlm_padding.py, and called in modeling_xlm_roberta.py
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/xlm_padding.py
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/modeling_xlm_roberta.py
</span><span class="k">def</span> <span class="nf">unpad_input</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Convert padded sequences to packed format for efficiency
    </span><span class="sh">"""</span>
    <span class="n">seqlens_in_batch</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">nonzero</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">as_tuple</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="nf">flatten</span><span class="p">()</span>
    <span class="n">max_seqlen_in_batch</span> <span class="o">=</span> <span class="n">seqlens_in_batch</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
    <span class="n">cu_seqlens</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">seqlens_in_batch</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="nf">return </span><span class="p">(</span>
        <span class="nf">index_first_axis</span><span class="p">(</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="sh">"</span><span class="s">b s ... -&gt; (b s) ...</span><span class="sh">"</span><span class="p">),</span> <span class="n">indices</span><span class="p">),</span>
        <span class="n">indices</span><span class="p">,</span>
        <span class="n">cu_seqlens</span><span class="p">,</span>
        <span class="n">max_seqlen_in_batch</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></div> <p>This operation creates tensors with sizes dependent on the input data, which conflicts with our bucketing strategy where we want to pad to the nearest multiple of 16. This dynamic sizing makes it challenging for torch.compile to effectively trace and optimize the model.</p> <h3 id="2-attention-mask-handling-limitations">2. Attention mask handling limitations</h3> <p>The alternative in the code was to use <code class="language-plaintext highlighter-rouge">flash_attn_qkvpacked_func</code> which doesn’t offer the flexibility we needed for custom attention masking as it expects qkv matrices together and internally handles causal or non-causal masking.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In FlashSelfAttention, from mha.py
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/mha.py
</span><span class="k">return</span> <span class="nf">flash_attn_qkvpacked_func</span><span class="p">(</span>
    <span class="n">qkv</span><span class="p">,</span>
    <span class="n">self</span><span class="p">.</span><span class="n">drop</span><span class="p">.</span><span class="n">p</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">softmax_scale</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">softmax_scale</span><span class="p">,</span>
    <span class="n">causal</span><span class="o">=</span><span class="n">causal</span><span class="p">,</span>
    <span class="n">alibi_slopes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">window_size</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">window_size</span><span class="p">,</span>
    <span class="n">deterministic</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">deterministic</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>While there is a regular <code class="language-plaintext highlighter-rouge">flash_attn_func</code> that might have worked, integrating our attention mask to mask padding tokens was not straightforward.</p> <h2 id="the-hybrid-approach">The Hybrid Approach</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In SelfAttention, from mha.py
# https://huggingface.co/jinaai/jina-reranker-v2-base-multilingual/blob/main/mha.py
</span><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">qkv</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Implements the multihead softmax attention.
        Arguments
        ---------
            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D)
            causal: if passed, will override self.causal
            key_padding_mask: boolean mask to apply to the attention weights. True means to keep,
                False means to mask out. (B, S)
        </span><span class="sh">"""</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seqlen</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">qkv</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">causal</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">causal</span> <span class="k">if</span> <span class="n">causal</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">causal</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">qkv</span><span class="p">.</span><span class="nf">unbind</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">softmax_scale</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">softmax_scale</span> <span class="ow">or</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bthd,bshd-&gt;bhts</span><span class="sh">"</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">*</span> <span class="n">softmax_scale</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">(</span>
                <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">),</span> <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="p">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="p">.</span><span class="n">device</span>
            <span class="p">)</span>
            <span class="n">padding_mask</span><span class="p">.</span><span class="nf">masked_fill_</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="sh">"</span><span class="s">b s -&gt; b 1 1 s</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">causal</span><span class="p">:</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">triu</span><span class="p">(</span>
                <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">seqlen</span><span class="p">,</span> <span class="n">seqlen</span><span class="p">),</span> <span class="o">-</span><span class="mf">10000.0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">scores</span><span class="p">.</span><span class="n">device</span><span class="p">),</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">causal_mask</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">scores</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">v</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">attention_drop</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">attention</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bhts,bshd-&gt;bthd</span><span class="sh">"</span><span class="p">,</span> <span class="n">attention_drop</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p>The standard PyTorch attention implementation (without Flash Attention) allowed torch.compile to see through the entire computation graph and apply optimizations like operation fusion and kernel generation tailored to our specific inputs.</p> <p>By disabling Flash Attention but keeping our bucketing and sorting strategies, we created a middle ground that allowed torch.compile to shine. This approach:</p> <ol> <li>Gives torch.compile more visibility into the computation graph</li> <li>Maintains consistent tensor shapes through our bucketing strategy</li> <li>Allows handling of attention mask quite simply</li> </ol> <p>The results showed this hybrid approach outperformed the baseline (Flash Attention) implementation. Even without input sorting, the torch.compile version was faster or about the same as the baseline (Flash Attention) + input sorting version. </p> <h2 id="benchmarking">Benchmarking</h2> <p>Our benchmarking system provides reliable measurements through proper warm-up and synchronization:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">benchmark</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">print_scores</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_runs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">on_sorted_inputs</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>  
    <span class="n">sentence_pairs_warmup</span> <span class="o">=</span> <span class="nf">load_and_sample_sentences</span><span class="p">(</span><span class="n">num_pairs</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">base_seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">sentence_pairs</span> <span class="o">=</span> <span class="nf">load_and_sample_sentences</span><span class="p">(</span><span class="n">num_pairs</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">base_seed</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
        <span class="c1"># Warmup
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Warming up...</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
            <span class="n">sentence_pairs_warmup</span> <span class="o">=</span> <span class="nf">load_and_sample_sentences</span><span class="p">(</span><span class="n">num_pairs</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">base_seed</span><span class="o">=</span><span class="n">seed</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
            <span class="n">_</span> <span class="o">=</span> <span class="nf">inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentence_pairs_warmup</span><span class="p">)</span>

        <span class="c1"># Multiple benchmark runs
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Benchmarking...</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">times</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_runs</span><span class="p">):</span>
            <span class="n">sentence_pairs</span> <span class="o">=</span> <span class="nf">load_and_sample_sentences</span><span class="p">(</span><span class="n">num_pairs</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">base_seed</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">seed</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">on_sorted_inputs</span><span class="p">:</span>
                <span class="c1"># Apply sorting if enabled
</span>                <span class="n">lengths</span> <span class="o">=</span> <span class="p">[(</span><span class="nf">max</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="nf">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]))),</span> <span class="n">i</span><span class="p">)</span> 
                          <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">sentence_pairs</span><span class="p">)]</span>
                <span class="n">sorted_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)]</span>
                <span class="n">sentence_pairs_sorted</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence_pairs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sorted_indices</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sentence_pairs_sorted</span> <span class="o">=</span> <span class="n">sentence_pairs</span>
                <span class="n">sorted_indices</span> <span class="o">=</span> <span class="bp">None</span>

            <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
            <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
            
            <span class="n">scores</span> <span class="o">=</span> <span class="nf">inference</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sentence_pairs_sorted</span><span class="p">)</span>
            
            <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
            <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
            <span class="n">times</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>
</code></pre></div></div> <h2 id="results">Results</h2> <p>Here are our key findings:</p> <table> <thead> <tr> <th style="text-align: left">Configuration</th> <th style="text-align: left">Mean Time (s)</th> <th style="text-align: left">Std Dev (s)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Baseline (Without Flash Attention) + Unsorted Inputs</td> <td style="text-align: left">0.3566</td> <td style="text-align: left">0.0101</td> </tr> <tr> <td style="text-align: left">Baseline (Without Flash Attention) + Sorted Inputs</td> <td style="text-align: left">0.3245</td> <td style="text-align: left">0.0623</td> </tr> <tr> <td style="text-align: left">Baseline (Flash Attention) + Unsorted Inputs</td> <td style="text-align: left">0.2961</td> <td style="text-align: left">0.0089</td> </tr> <tr> <td style="text-align: left">Baseline (Flash Attention) + Sorted Inputs</td> <td style="text-align: left">0.2658</td> <td style="text-align: left">0.0119</td> </tr> <tr> <td style="text-align: left">torch.compile + Unsorted Inputs</td> <td style="text-align: left">0.2595</td> <td style="text-align: left">0.0077</td> </tr> <tr> <td style="text-align: left"><strong>torch.compile + Sorted Inputs</strong></td> <td style="text-align: left"><strong>0.2089</strong></td> <td style="text-align: left"><strong>0.0196</strong></td> </tr> </tbody> </table> <p><br/></p> <h3 id="key-observations">Key observations:</h3> <ol> <li>torch.compile provides upto ~1.3x speedup over the base model</li> <li>Input sorting improves performance by upto 1.25x</li> <li>The combination of torch.compile and sorted inputs gives us the best performance</li> </ol> <h2 id="best-practices-and-learnings">Best Practices and Learnings</h2> <p>Through this optimization process, we discovered several important practices:</p> <ol> <li> <p><strong>Proper Warm-up</strong>: Always run warm-up iterations before benchmarking to ensure the compiled model has optimized its execution path and seen all variations of sizes so that there are no recompilations during the actual benchmarking.</p> </li> <li> <p><strong>Accurate Timing</strong>: Use proper CUDA synchronization for accurate measurements:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
<span class="c1"># ... inference ...
</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">perf_counter</span><span class="p">()</span>
</code></pre></div> </div> </li> </ol> <h2 id="conclusion">Conclusion</h2> <p>By combining torch.compile with smart batching and input sorting, we achieved a significant speedup in our neural reranker inference. The key takeaway is that optimization often requires a multi-faceted approach - compiler optimizations alone might not give you the best results, but when combined with domain-specific optimizations like bucket-based padding and input sorting, the improvements can be substantial.</p> <p>For those looking to optimize their own models, I recommend:</p> <ol> <li>Start with torch.compile as it’s relatively easy to implement</li> <li>Add bucket-based padding if you have variable-length inputs</li> <li>Consider input sorting if your batch sizes are large enough to benefit from it</li> <li>Always measure and profile your specific use case, as the benefits of each optimization can vary depending on your model and data</li> </ol> <p>The complete code for this optimization project is available in the snippets above. Feel free to adapt these techniques for your own use case!</p> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="MLSys"/><category term="inference-optimization"/><category term="efficiency"/><category term="mlsys"/><summary type="html"><![CDATA[A quick writeup on accelerating a Jina Cross-Encoder using torch.compile]]></summary></entry><entry><title type="html">Paper Summary #13 - Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process</title><link href="https://shreyansh26.github.io/post/2024-09-21_physics-of-lms-2-1-grade-school-math-and-the-hidden-reasoning-process/" rel="alternate" type="text/html" title="Paper Summary #13 - Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process"/><published>2024-09-21T00:00:00+00:00</published><updated>2024-09-21T00:00:00+00:00</updated><id>https://shreyansh26.github.io/post/physics-of-lms-21</id><content type="html" xml:base="https://shreyansh26.github.io/post/2024-09-21_physics-of-lms-2-1-grade-school-math-and-the-hidden-reasoning-process/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2407.20311">Arxiv Link</a><br/> <strong>Video</strong>: <a href="https://www.youtube.com/watch?v=bpp6Dz8N2zY">YouTube Link</a> <br/> <strong>Annotated Paper</strong>: <a href="https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/LLMs/Physics%20of%20Large%20Language%20Models/Part%202.1%2C%20Grade-School%20Math%20and%20the%20Hidden%20Reasoning%20Process.pdf">Github repo</a></p> <hr/> <h2 id="key-questions">Key Questions</h2> <ol> <li>Can language models truly develop reasoning skills, or do they simply memorize templates?</li> <li>What is the model’s hidden (mental) reasoning process?</li> <li>Do models solve math questions using skills similar to or different from humans?</li> <li>Do models trained solely on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems?</li> <li>What mental process causes models to make reasoning mistakes?</li> <li>How large or deep must a model be to effectively solve GSM8K-level math questions?</li> </ol> <h2 id="idea">Idea</h2> <ul> <li>To study reasoning and intelligence, a pre-trained model can be fine-tuned on existing math problem datasets like GSM8K.</li> <li>However, there are concerns that GSM8K or similar data may be leaked in the pre-train data for LLMs.</li> <li>Hence when answering mathematical questions, it is hard to tell whether these models are actually performing reasoning (i.e. questions 1-3 above) or has just memorized problem templates during training.</li> <li>Using existing math datasets like GSM8K for pre-training is insufficient due to the small size of these datasets.</li> <li>Also, the idea of using GPT4 to augment similar problems like GSM8K wasn’t chosen because of the potential bias of the augmented data towards a small number of solution templates.</li> <li>Hence the authors created their own pre-training dataset of math problems, with a much larger and diverse set of grade-school math problems to test a GPT2-like language model.</li> </ul> <h2 id="dataset-generation">Dataset Generation</h2> <ul> <li>This section in the paper is super cool but also quiet dense.</li> <li>It is a great source of learning on how to create synthetic datasets. Won’t go into much detail in the notes, the paper is the best source to understand the complexity.</li> <li>A standard grade-school math problem in GSM8K looks like this - <ul> <li> <blockquote> <p>Betty is saving money for a new wallet which costs 100. Betty has only half of the money she needs. Her parents decided to give her 15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?</p> </blockquote> </li> </ul> </li> <li>This problem has multiple parameters whose values are connected with various equalities, such as “Betty’s current money = 0.5 × cost of the wallet” and “money given by grandparents = 2 × money given by parents.”</li> <li>Motivated by this, the authors build a GSM8K-like math dataset through a synthetic generation pipeline that captures the dependencies of parameters.</li> <li>The main focus was on the “logical reasoning” aspect of the problems which involves understanding the dependency of parameters in the problem statement, such as direct, instance, and implicit dependencies.</li> <li><strong>Dependency Types</strong>: <ul> <li><strong>Direct Dependency</strong>: Parameters directly depend on others (e.g., \(A = 5 \times (X + Y)\)).</li> <li><strong>Instance Dependency</strong>: Parameters depend on instances of categories (e.g., total number of chairs = chairs per classroom × number of classrooms).</li> <li><strong>Implicit Dependency</strong>: Parameters involve abstract concepts that need to be inferred (e.g., identifying fruits from a list of items).</li> </ul> </li> <li><strong>Problem Structure</strong>: Problems are generated using a hierarchical categorization of items, where each category can include multiple layers and items. This structure adds complexity by requiring models to learn concepts implicitly.</li> <li><strong>Dependency Graph</strong>: A directed acyclic graph (DAG) is used to represent dependencies among parameters, including both direct and implicit dependencies. The graph guides the generation of math problems by linking parameters in specific ways.</li> <li><strong>Problem Generation</strong>: The math problems are articulated by translating the dependency graphs into English sentences. The order of these sentences is randomized to increase the difficulty, and a question is posed to test the model’s understanding.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/structure_graph.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>Additional care was taken to reduce the difficulty of the problem statements <ul> <li>Ensuring clarity in expression, to reduce the difficulty arising from common-sense</li> <li>For arithmetic, all integers and arithmetic were mod23</li> </ul> </li> <li>A sample problem (corresponding to the figure above) looks like this - <ul> <li> <blockquote> <p>The number of each Riverview High’s Film Studio equals 5 times as much as the sum of each Film Studio’s Backpack and each Dance Studio’s School Daypack. The number of each Film Studio’s School Daypack equals 12 more than the sum of each Film Studio’s Messenger Backpack and each Central High’s Film Studio. The number of each Central High’s Film Studio equals the sum of each Dance Studio’s School Daypack and each Film Studio’s Messenger Backpack. The number of each Riverview High’s Dance Studio equals the sum of each Film Studio’s Backpack, each Film Studio’s Messenger Backpack, each Film Studio’s School Daypack and each Central High’s Backpack. The number of each Dance Studio’s School Daypack equals 17. The number of each Film Studio’s Messenger Backpack equals 13. How many Backpack does Central High have?</p> </blockquote> </li> </ul> </li> </ul> <h3 id="solution-construction">Solution Construction</h3> <ul> <li>The solution generation is done in the Chain of Thought reasoning format.</li> <li>The methodology is given below - <ul> <li>Let <strong>solution</strong> be a sequence of sentences describing the <strong>necessary</strong> steps towards solving the given problem, where the sentences follow any topological order - known as CoT</li> <li>For each parameter <strong>necessary</strong> towards answering the final question, it is assigned a random letter among the 52 choices (a..z or A..Z), and a sentence is used to describe its computation.</li> <li>Sample solution corresponding to the above problem -</li> <li> <blockquote> <p>Define Dance Studio’s School Daypack as p; so p = 17. Define Film Studio’s Messenger Backpack as W; so W = 13. Define Central High’s Film Studio as B; so B = p + W = 17 + 13 = 7. Define Film Studio’s School Daypack as g; R = W + B = 13 + 7 = 20; so g = 12 + R = 12 + 20 = 9. Define Film Studio’s Backpack as w; so w = g + W = 9 + 13 = 22. Define Central High’s Backpack as c; so c = B * w = 7 * 22 = 16. <i>Answer: 16</i>.</p> </blockquote> </li> </ul> </li> <li>Important points during solution construction - <ul> <li>The solution only contain parameters necessary towards calculating the final query parameter</li> <li>The solution follows the correct logical order: i.e. all the parameters used in the calculation must have appeared and been computed beforehand.</li> <li>Computations are broken into binary ops: \(g = 12 + 13 + 7\) is broken into g = 12+R and R = 13+7 in the above solution. The number of semicolons “;” equals the number of operations. This reduces the arithmetic complexity of the solution.</li> </ul> </li> </ul> <h3 id="difficulty-control">Difficulty Control</h3> <ul> <li>Two parameters are used to control the difficulty of the problem - <ul> <li>\(\textrm{ip}\) is the number of instance parameters</li> <li>\(\textrm{op}\) is the number of solution operations</li> </ul> </li> <li>The data difficulty is an increasing function over them</li> <li>The authors call the dataset iGSM</li> <li>We use \(\textrm{iGSM}^{\textrm{op} \leq op,\textrm{ip} \leq ip}\) to denote the data generated with constraint \(\textrm{op} \leq op\) and \(\textrm{ip} \leq ip\), and use \(\textrm{iGSM}^{\textrm{op}=op,\textrm{ip} \leq ip}\) to denote those restricting to \(\textrm{op}=op\).</li> </ul> <h3 id="train-and-test-datasets">Train and Test Datasets</h3> <ul> <li>\(\textrm{iGSM-med}\) uses \(\textrm{ip} \leq 20\) <ul> <li>Train data is essentially \(\textrm{iGSM}^{\textrm{op} \leq 15,\textrm{ip} \leq 20}\) (referred as \(\textrm{iGSM-med}^{\textrm{op} \leq 15}\))</li> <li>Evaluation is done on both \(\textrm{iGSM-med}^{\textrm{op} \leq 15}\) and out-of-distribution (OOD) on \(\textrm{iGSM-med}^{\textrm{op} \leq op}\) where \(op \in \{20, 21, 23, 23\}\)</li> <li>Another set of evaluation is also done on \(\textrm{iGSM-med}^{\textrm{op} = op,\textrm{reask}}\). Here \(\textrm{reask}\) denotes first generating a problem from \(\textrm{iGSM-med}^{\textrm{op} = op}\) and then resampling a query parameter.</li> <li>Due to the topological nature of the data/solution generation process, \(\textrm{reask}\) greatly changes the data distribution and the number of operations needed. It provides an excellent OOD sample for evaluation.</li> </ul> </li> <li>\(\textrm{iGSM-hard}\) uses \(\textrm{ip} \leq 28\) <ul> <li>Train data is essentially \(\textrm{iGSM}^{\textrm{op} \leq 21,\textrm{ip} \leq 28}\) (referred as \(\textrm{iGSM-hard}^{\textrm{op} \leq 21}\))</li> <li>Evaluation is done on both \(\textrm{iGSM-hard}^{\textrm{op} \leq 21}\) and out-of-distribution (OOD) on \(\textrm{iGSM-hard}^{\textrm{op} \leq op}\) where \(op \in \{28, 29, 30, 31, 32\}\)</li> <li>Another set of evaluation is also done on \(\textrm{iGSM-hard}^{\textrm{op} = op,\textrm{reask}}\).</li> </ul> </li> <li>Key points about the dataset <ul> <li>Ignoring unused parameters, numerics, sentence orderings, English words, a-z and A-Z letter choices, \(\textrm{iGSM}^{\textrm{op} = 15}\) still has at least 7 billion solution templates, and \(\textrm{iGSM-hard}^{\textrm{op} = 21}\) has at least 90 trillion solution templates.</li> <li>The OOD evaluation is guaranteed to not have data contamination as training is done only on \(\textrm{op} \leq 21\) but evaluation is done on \(\textrm{op} \geq 28\)</li> <li>Training is done on data whose hash value of solution template is \(&lt;17\) (mod 23), and but tested with those \(\geq 17\). This ensures no template-level overlap between training and testing.</li> </ul> </li> </ul> <h2 id="model">Model</h2> <ul> <li>A GPT2 model but with its positional embedding replaced with rotary embeddings (RoPE)</li> <li>It is still called GPT2 in the paper</li> <li>The authors mostly stick to the 12-layer, 12-head, 768-dim GPT2 (a.k.a. GPT2-small, 124M) for experiments but also explore larger models for some experiments</li> <li>The context length is 768 / 1024 for pretraining on \(\textrm{iGSM-med}\) / \(\textrm{iGSM-hard}\) and 2048 for evaluation.</li> </ul> <h2 id="key-result---models-behavior-process">Key Result - Model’s Behavior process</h2> <ul> <li><strong>TL;DR</strong> - <ul> <li>The authors demonstrate that the GPT2 model, pretrained on the synthetic dataset, not only achieves 99% accuracy in solving math problems from the same distribution but also out-of-distribution generalizes, such as to those of longer reasoning lengths than any seen during training.</li> <li>Note that the model has never seen any training example of the same length as in test time.</li> <li>This signifies that the model can truly learn some reasoning skill instead of memorizing solution templates.</li> <li>Crucially, the model can learn to generate shortest solutions, almost always avoiding unnecessary computations.</li> <li>This suggests that the model formulates a plan before it generates, in order to avoid computing any quantities that are not needed towards solving the underlying math problem.</li> </ul> </li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/test_acc_3.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_4.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h2 id="key-result---models-mental-process">Key Result - Model’s Mental Process</h2> <ul> <li><strong>TL;DR</strong> <ul> <li>The authors examine the model’s internal states through probing, introducing six probing tasks to elucidate how the model solves math problems.</li> <li>For instance, they discover that the model (mentally!) preprocesses the full set of necessary parameters before it starts any generation. Likewise, humans also do this preprocess although we write this down on scratch pads.</li> <li>The model also learns unnecessary, yet important skills after pretraining, such as all-pair dependency.</li> <li>Before any question is asked, it already (mentally!) computes with good accuracy which parameters depend on which, even though some are not needed for solving the math problem.</li> <li>Note that computing all-pair dependency is a skill not needed to fit all the solutions in the training data. This is the first evidence that a language model can learn useful skills beyond those necessary to fit its pretraining data.</li> </ul> </li> <li>The authors use linear probing to study the following tasks which align with human problem-solving strategies - <ul> <li>\(\textrm{nece(A)}\): If the parameter \(A\) is necessary for computing the answer.</li> <li>\(\textrm{dep(A, B)}\): if parameter \(A\) (recursively) depends on parameter B given the problem statement.</li> <li>\(\textrm{known(A)}\): if parameter \(A\) has already been computed.</li> <li>\(\textrm{value(A)}\): the value of parameter \(A\) (a number between 0-22, or 23 if \(\textrm{known(A) = false}\)).</li> <li>\(\textrm{can_next(A)}\): if \(A\) can be computed in the next solution sentence (namely, its predecessors have all been calculated). Note that \(A\) might not be necessary to answer the question.</li> <li>\(\textrm{nece_next(A)}\): if parameter A satisfies both \(\textrm{can_next(A)}\) and \(\textrm{nece(A)}\).</li> </ul> </li> <li>For a model to generate the shortest solutions, it must identify \(\textrm{nece(A)}\) for all \(A\)’s in its mental process.</li> <li>Whether \(\textrm{nece(A)}\) is \(\textrm{true}\), directly corresponds to whether there is a solution sentence to compute \(A\).</li> <li>Other similar probing tasks and what they imply are shown in the figure below -</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_5.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="variable-probing-a--nearly-linear-probing-method">V(ariable)-Probing: A Nearly-Linear Probing Method</h3> <ul> <li>As illustrated in the figure above, the authors conduct probing <ul> <li>At the end of the problem description for the \(\textrm{dep}\) task</li> <li>At the end of the question description for the \(\textrm{nece}\) task</li> <li>For all other tasks, they are probed at the end of every solution sentence</li> </ul> </li> <li>In Linear Probing a trainable linear classifier is introduced over the hidden states and a lightweight finetuning is performed for the task.</li> <li>V-Probing however has certain differences.</li> <li><strong>Motivation</strong>: V-Probing is introduced to handle more complex properties in math problems that involve one or two conditional variables (A and B) described in plain English.</li> <li><strong>Probing Setup</strong>: The math problems are truncated to the probing position, and special tokens <code class="language-plaintext highlighter-rouge">[START]</code> and <code class="language-plaintext highlighter-rouge">[END]</code> are placed around the descriptions of A (or A, B). The probing is done from the <code class="language-plaintext highlighter-rouge">[END]</code> token position to check if the property is linearly encoded in the model’s last layer.</li> <li><strong>Enhancement Over Linear Probing</strong>: Unlike standard linear probing, V-Probing introduces a small, trainable rank-8 linear update to the input embedding layer to account for input changes.</li> <li><strong>Training Process</strong>: The pretrained language model is frozen, and both the linear classifier and the rank-8 update are fine-tuned to probe for the desired property, making the process more adaptable to complex input structures.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_13.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="probing-results">Probing Results</h3> <ul> <li>The probing accuracies are high for all the tasks, compared to majority guess and random-model probing - except for the very hard OOD cases (i.e., for large op where the model’s generation accuracies fall down to 80% anyways.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_7.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li><strong>Model solves math problems like humans</strong> <ul> <li>When generating solutions, the model not only remembers which parameters have been computed and which have not (\(\textrm{value, known}\)) but <strong>also knows which parameters can be computed next</strong> (\(\textrm{can_next, nece_next}\)). These abilities <strong>ensure that the model can solve the given math problem step by step</strong>, similar to human problem-solving skill.</li> <li>By the end of the problem description, the model already knows the full list of necessary parameters (\(\textrm{nece}\)). This indicates that the model has learned to plan ahead, identifying necessary parameters before starting to generate the solution. This aligns with human behavior, except that the model plans mentally while humans typically write this down.</li> </ul> </li> <li><strong>Model learns beyond human reasoning skills</strong> <ul> <li>The model learns \(\textrm{dep(A, B)}\) and \(\textrm{can_next(A)}\), even for parameters A not necessary for answering the question, as shown in the figure above.</li> <li>This differs from human problem-solving, where we typically use backward reasoning from the question to identify necessary parameters, often overlooking unnecessary ones.</li> <li>In contrast, language models can pre-compute the all-pair dependency graph \(\textrm{dep(A, B)}\) mentally even before a question is asked. This “level-2” reasoning is very different from human behavior or mental processes.</li> <li>This enables the model to sort relationships among the things it hears, a skill that can be useful for future tasks (via instruction fine-tuning).</li> <li>This may be the first evidence of a language model acquiring skills beyond those needed for learning its pretrain data.</li> </ul> </li> </ul> <h2 id="key-results---explain-models-mistakes">Key Results - Explain Model’s Mistakes</h2> <ul> <li>The authors tried to answer the following questions - <ul> <li>When does the model answer correctly but include unnecessary parameters?</li> <li>What causes incorrect answers?</li> </ul> </li> <li>The aim is to determine if such erroneous behaviour of the model aligns with errors in the model’s mental process (via probing)</li> <li>Earlier, it was shown that the model rarely produces solutions longer than necessary, so the authors looked at the OOD \(\textrm{reask}\) data for evaluation.</li> <li>On this data, pretrained models produce an average of ~0.5 unnecessary parameters per solution even for \(\textrm{op} = 32\) (figure 4).</li> <li>The authors examined if these unnecessary parameters \(A\) were incorrectly predicted as \(\textrm{nece = true}\) in the probing task.</li> <li>The following figure reveals that this is often indeed the case, thus language models produce solutions with unnecessary steps due to errors in their mental planning phase.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_8.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>In the second part of the figure, the author’s findings show that the model’s errors mainly stem from incorrectly predicting \(\textrm{nece_next(A)}\) or \(\textrm{can_next(A)}\) as true in its internal states when such \(A\)’s are not ready for computation.</li> <li>Essentially, <ul> <li>Many reasoning mistakes made by the language model are systematic, stemming from errors in its mental process, not merely random from the generation process.</li> <li>Some of the model’s mistakes can be discovered by probing its inner states even before the model opens its mouth (i.e., before it says the first solution step).</li> </ul> </li> </ul> <h2 id="key-results---depth-vs-reasoning-length">Key Results - Depth vs. Reasoning Length</h2> <ul> <li>The authors find that - <strong>Language model depth is crucial for mathematical reasoning.</strong></li> <li>They experimented with model of various depths and various hidden sizes.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_9.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>From the figure above, it can be seen that a 4-layer transformer, even with 1920 hidden dimensions, underperforms on the math datasets in the paper.</li> <li>Conversely, deeper but smaller models, such as a 20-layer 576-dim, perform very well.</li> <li>There is a clear correlation between the model depth and performance.</li> </ul> <h3 id="but-how-does-model-depth-influence-math-problem-solving-skills">But how does model depth influence math problem solving skills?</h3> <ul> <li>Using the \(\textrm{nece}\) probing task, the authors focus on the necessary parameters at distance \(t\) from the query parameter, for \(t \in \{1,2,...,8\}\).</li> <li>Though these parameters all have \(\textrm{nece = true}\), but the model can be probed to see how correct they are at predicting \(\textrm{nece(A)}\) at different hidden layers.</li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/physics_of_lms_21/fig_10.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <ul> <li>The above figure shows a correlation between the model’s layer hierarchy, reasoning accuracy and mental reasoning depth.</li> <li>Shallower layers excel at predicting nece(A) for parameters \(A\) closer to the query, whereas deeper layers are more accurate and can predict \(\textrm{nece(A)}\) for parameters further from the query.</li> <li>This suggests that the **model employs layer-by-layer reasoning during the planning phase to recursively identify all parameters the query depends on. **</li> <li>So, for larger \(t\), the model may require and benefit from deeper models (assuming all other hyperparameters remain constant).</li> <li>To note - <ul> <li>If the “backward thinking process” is added as CoT to the data, then deep mental thinking is no longer required, reducing the language model’s depth requirement. However, in practice, many such “thinking processes” may not be included in standard math solutions or languages in general.</li> <li>The above claim does not imply that “a \(t\)-step mental thinking requires a depth-\(t\) transformer”. It is plausible for a single transformer layer (containing many sub-layers) to implement \(t &gt; 1\) mental thinking steps, though possibly with reduced accuracy as \(t\) increases. There is no exact correlation as it depends on the data distribution.</li> </ul> </li> </ul> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="LLMs"/><category term="transformer"/><category term="reasoning"/><category term="paper-summaries"/><summary type="html"><![CDATA[My notes from the Physics of Language Models series of papers.]]></summary></entry><entry><title type="html">Paper Summary #12 - Image Recaptioning in DALL-E 3</title><link href="https://shreyansh26.github.io/post/2024-02-18_dalle3_image_recaptioner/" rel="alternate" type="text/html" title="Paper Summary #12 - Image Recaptioning in DALL-E 3"/><published>2024-02-18T08:00:00+00:00</published><updated>2024-02-18T08:00:00+00:00</updated><id>https://shreyansh26.github.io/post/image-recaptioning-dalle3</id><content type="html" xml:base="https://shreyansh26.github.io/post/2024-02-18_dalle3_image_recaptioner/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Technical Paper</strong>: <a href="https://cdn.openai.com/papers/dall-e-3.pdf">Improving Image Generation with Better Captions</a></p> <p>OpenAI’s Sora is built upon the image captioning model which was described in quite some detail in the DALL-E 3 technical report.</p> <hr/> <p>In general, in text-image datasets, the captions omit background details or common sense relationships, e.g. sink in a kitchen or stop signs along the road. They also omit the position and count of objects in the picture, color and size of the objects and any text present in the image.</p> <p>OpenAI trained a captioner model to solve this.</p> <p>An image captioner is similar to a language model that predicts the next token conditioned on the image and the past generated tokens. Since images are composed of many thousands of pixel values, conditioning on all of this information is very complex and inefficient. The solution DALL-E 3 used was to use CLIP’s compressed representational space to condition upon. OpenAI jointly pre-trained a captioner with a CLIP and a language modeling objective using this formulation on the text and image pairs.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/lm.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>However, this still doesn’t solve the reluctance of the captioning model to provide details, as mentioned above. The fine-tuning of this captioner to be more descriptive and helpful for the text-to-image task is done in two phases.</p> <p>First, they built a small dataset of captions that describe only the main subject of the image. The captioner is fine-tuned on this dataset. Now the model is more biased towards describing the main subject of the image.</p> <p>Next, they created a dataset of long, highly-descriptive captions describing the contents of each image in the fine-tuning dataset. In addition to the main subject, these captions describe the surroundings, background, text found in the image, styles, coloration, etc. as well. The captioner is then fine-tuned on this dataset to generate descriptive synthetic captions.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/synthetic_captions.png" alt="Examples of alt-text accompanying selected images scraped from the internet, short synthetic captions (SSC), and descriptive synthetic captions (DSC)."/> <figcaption>Examples of alt-text accompanying selected images scraped from the internet, short synthetic captions (SSC), and descriptive synthetic captions (DSC).</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>There is a still a problem though…</strong></p> <p>Text-to-Image diffusion models have a tendency to overfit to the text distribution. Using synthetic captions can lead to such issues since the captioner model can have many modal behaviours that won’t be apparent but can bias the text-to-image model when trained on the synthetic captions.<br/> Examples of where this might occur is in letter casing, where punctuation appears in the caption (e.g. does it always end with a period?), how long the captions are, or stylistic tendencies such as starting all captions with the words “a” or “an”.</p> <p>OpenAI overcame this issue by regularizing the text inputs to a distribution that is similar to the humans. The ground-truth captions provide this already since they were drawn from a distribution of human-written text. The regularization can be introduced during training the model by randomly selecting either the ground truth or synthetic caption with a fixed percent chance. DALLE-3 used 95% synthetic captions and 5% ground truth captions.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/perf.png" alt="CLIP scores for text-to-image models trained on different caption types. Left is evaluation results with ground truth captions on our evaluation dataset. Right uses the descriptive synthetic captions from the same dataset"/> <figcaption>CLIP scores for text-to-image models trained on different caption types. Left is evaluation results with ground truth captions on our evaluation dataset. Right uses the descriptive synthetic captions from the same dataset</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/synthetic_ratio.png" alt="CLIP scores for text-to-image models trained on various blending ratios of descriptive synthetic captions and ground-truth captions. Evaluation performed using ground truth captions."/> <figcaption>CLIP scores for text-to-image models trained on various blending ratios of descriptive synthetic captions and ground-truth captions. Evaluation performed using ground truth captions.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The above figure shows that using very high percentage of synthetic captions maximizes the performance of the models. But increasing the synthetic caption ratio implies biasing the model to the distribution of long, highly-descriptive captions emitted by the captioning model. OpenAI used GPT-4 to upsamples any caption into a highly descriptive one.</p> <p>Here are some examples -</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/gpt_upsample.png" alt="Effect of using 'upsampled' drawbench captions to create samples with DALL-E 3. Original drawbench captions on top, upsampled captions on bottom. Images are best of 4 for each caption."/> <figcaption>Effect of using 'upsampled' drawbench captions to create samples with DALL-E 3. Original drawbench captions on top, upsampled captions on bottom. Images are best of 4 for each caption.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Below is the prompt OpenAI used to “upsample” the captions.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/dalle3_image_recaptioner/upsample_prompt.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="Computer Vision"/><category term="image-captioning"/><category term="generative-ai"/><summary type="html"><![CDATA[The image recaptioning technique used in DALL-E 3 was extended to videos in Sora.]]></summary></entry><entry><title type="html">Paper Summary #11 - Sora</title><link href="https://shreyansh26.github.io/post/2024-02-18_sora_openai/" rel="alternate" type="text/html" title="Paper Summary #11 - Sora"/><published>2024-02-18T07:00:00+00:00</published><updated>2024-02-18T07:00:00+00:00</updated><id>https://shreyansh26.github.io/post/sora-openai</id><content type="html" xml:base="https://shreyansh26.github.io/post/2024-02-18_sora_openai/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sora_openai/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Technical Paper</strong>: <a href="https://openai.com/sora">Sora - Creating video from text</a><br/> <strong>Blog</strong>: <a href="https://openai.com/research/video-generation-models-as-world-simulators">Video generation models as world simulators</a></p> <p>These are just short notes / excerpts from the technical paper for quick lookup.</p> <hr/> <p>Sora is quite a breakthrough. It is able to understand and simulate the physical world, generating up to 60s long high-definition videos while maintaining the quality, scene continuation and following the user’s prompt.</p> <p>Key papers Sora is built upon -</p> <ul> <li><a href="https://arxiv.org/abs/2212.09748">Diffusion Transformer (DiT)</a></li> <li><a href="https://arxiv.org/abs/2112.10752">Latent Diffusion Models</a></li> <li><a href="https://cdn.openai.com/papers/dall-e-3.pdf">DALL-E 3 Image Recaptioning</a></li> </ul> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sora_openai/diffusion.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Sora (being a diffusion transformer) uses the idea from ViT of using patches. The videos are converted to patches by first first compressing videos into a lower-dimensional latent space (as in the Latent Diffusion Models paper) and then decomposing the representation into spacetime patches.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sora_openai/patches1.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The network takes raw video as input and outputs a latent representation that is compressed temporally and spatially. The video training and generation is done within this latent space. A decoder model is also trained that maps generated latents back to pixel space.</p> <p>A sequence of spacetime patches is extracted from the compressed video. The patch-based representation enables Sora to train on videos and images of variable resolutions, durations and aspect ratios. During inference time, an appropriately-sized grid (for the random patches) can be selected to control the size of the generated videos.</p> <p>Hence, while past approaches resized, cropped or trimmed videos to a standard size, Sora can sample widescreen 1920x1080p videos, vertical 1080x1920 videos and everything inbetween. Training on the native aspect ratios was in fact helpful for the model to improve composition and framing. This is similar to what Adept did in <a href="https://www.adept.ai/blog/fuyu-8b">Fuyu-8B</a>.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/sora_openai/fuyu.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <h3 id="how-does-it-adhere-to-text-so-well">How does it adhere to text so well?</h3> <p>OpenAI trained a very descriptive captioner model which they used to generate captions for all the videos in their training set (probably a finetuned GPT-4V ?). Training on highly descriptive video captions improves text fidelity as well as the overall quality of videos.</p> <p>They also use query expansion - using GPT to turn short user prompts into longer detailed captions that are sent to the video model.</p> <p>More details about the image captioner in the next blog post.</p> <h3 id="what-are-some-emergent-capabilities-that-are-exhibited-with-scale">What are some emergent capabilities that are exhibited with scale?</h3> <ul> <li>3D consistency - Sora videos can shift and rotate elements based on the camera motion.</li> <li>Long-range coherence and object permanence</li> <li>Interacting with the world - Though not perfect, Sora can sometimes remember the state of the world after an action was taken.</li> <li>Simulating digital worlds - Sora can generate Minecraft videos, wherein it can simultaneously control the player with a basic policy while also rendering the world and its dynamics in high fidelity.</li> </ul> <hr/> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="Computer Vision"/><category term="diffusion"/><category term="image-generation"/><category term="video-generation"/><category term="generative-ai"/><summary type="html"><![CDATA[OpenAI announced a ground-breaking text-to-video diffusion model capable of generating high-definition videos up to 60 seconds long.]]></summary></entry><entry><title type="html">Paper Summary #10 - Gemini 1.5 Pro</title><link href="https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/" rel="alternate" type="text/html" title="Paper Summary #10 - Gemini 1.5 Pro"/><published>2024-02-18T06:00:00+00:00</published><updated>2024-02-18T06:00:00+00:00</updated><id>https://shreyansh26.github.io/post/gemini15-pro</id><content type="html" xml:base="https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/"><![CDATA[<div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/featured.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p><strong>Technical Paper</strong>: <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</a><br/> <strong>Blog</strong>: <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024">Our next-generation model: Gemini 1.5</a></p> <p>These are just short notes / excerpts from the technical paper for quick lookup. Actual implementational details are anyways missing in the technical paper.</p> <hr/> <p>Gemini 1.5 Pro is a sparse MoE Transformer-based model (seems like a trend these days, after GPT-4’s rumors and Mixtral).</p> <p>It supports upto 10M context multimodal context length which is about a day of 22 hours of audio recordings, more than ten times the entirety of the 1440 page book “War and Peace”, the entire Flax codebase (41,070 lines of code), or three hours of video at 1 frame-per-second.</p> <p>Near-perfect retrieval across complete context length. In all modalities, i.e., text, video and audio, it achieves 100% accurate needle-in-a-haystack recall for 530k tokens, 99.7% accurate recall for 1M tokens and 99.2% for 10M contexts.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/niah1.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/text_niah.png" alt="Text Haystack. This figure compares Gemini 1.5 Pro with GPT-4 Turbo for the text needle-in-a-haystack task. Green cells indicate the model successfully retrieved the secret number, gray cells indicate API errors, and red cells indicate that the model response did not contain the secret number. The top row shows results for Gemini 1.5 Pro, from 1k to 1M tokens (top left), and from 1M to 10M tokens (top right). The bottom row shows results on GPT-4 Turbo up to the maximum supported context length of 128k tokens. The results are color-coded to indicate: green for successful retrievals and red for unsuccessful ones."/> <figcaption>Text Haystack. This figure compares Gemini 1.5 Pro with GPT-4 Turbo for the text needle-in-a-haystack task. Green cells indicate the model successfully retrieved the secret number, gray cells indicate API errors, and red cells indicate that the model response did not contain the secret number. The top row shows results for Gemini 1.5 Pro, from 1k to 1M tokens (top left), and from 1M to 10M tokens (top right). The bottom row shows results on GPT-4 Turbo up to the maximum supported context length of 128k tokens. The results are color-coded to indicate: green for successful retrievals and red for unsuccessful ones.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/video_niah.png" alt="Video Haystack. This figure compares Gemini 1.5 Pro with GPT-4V for the video needle-in-a-haystack task, where the models are given video clips of different lengths up to three hours of video and are asked to retrieve a secret word embedded as text at different points within the clip. All video clips are sampled at one frame-per-second (1 fps). The first pair of 10 × 50 haystack plots on the left compare Gemini 1.5 Pro with GPT-4V on the first hour of the documentary. The x-axis represents the video duration which ranges from 1.2 minutes to 1 hour, and the y-axis represents the depth, namely the relative offset of the needle (e.g., the top left cell represents providing the model with the first 1.2 minutes and randomly sampling a frame in the first ten percent of that trimmed video where the needle is inserted). A green cell indicates that the model successfully retrieved the needle, whereas a gray cell indicates an API error. Whereas the GPT-4V API supports video lengths only up to around the first 3 minutes of the full hour, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages up to an hour of video, as shown by the all-green plot. Finally, the 10 × 10 grid on the right shows Gemini 1.5 Pro’s perfect retrieval capabilities across three full hours of video, constructed by concatenating two copies of the documentary back-to-back."/> <figcaption>Video Haystack. This figure compares Gemini 1.5 Pro with GPT-4V for the video needle-in-a-haystack task, where the models are given video clips of different lengths up to three hours of video and are asked to retrieve a secret word embedded as text at different points within the clip. All video clips are sampled at one frame-per-second (1 fps). The first pair of 10 × 50 haystack plots on the left compare Gemini 1.5 Pro with GPT-4V on the first hour of the documentary. The x-axis represents the video duration which ranges from 1.2 minutes to 1 hour, and the y-axis represents the depth, namely the relative offset of the needle (e.g., the top left cell represents providing the model with the first 1.2 minutes and randomly sampling a frame in the first ten percent of that trimmed video where the needle is inserted). A green cell indicates that the model successfully retrieved the needle, whereas a gray cell indicates an API error. Whereas the GPT-4V API supports video lengths only up to around the first 3 minutes of the full hour, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages up to an hour of video, as shown by the all-green plot. Finally, the 10 × 10 grid on the right shows Gemini 1.5 Pro’s perfect retrieval capabilities across three full hours of video, constructed by concatenating two copies of the documentary back-to-back.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/audio_niah.png" alt="Audio Haystack. This figure presents the audio version of the needle-in-a-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo. In this setting, the needle is a short segment of audio that is inserted within a very large audio segment (of up to 22 hours) containing concatenated audio clips. The task is to retrieve the 'secret keyword' which is revealed in the needle. Red indicates that the model did not identify the keyword, whereas green indicates that the model identified the keyword correctly"/> <figcaption>Audio Haystack. This figure presents the audio version of the needle-in-a-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo. In this setting, the needle is a short segment of audio that is inserted within a very large audio segment (of up to 22 hours) containing concatenated audio clips. The task is to retrieve the 'secret keyword' which is revealed in the needle. Red indicates that the model did not identify the keyword, whereas green indicates that the model identified the keyword correctly</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>However, with about 100 needles (with the model requiring to retrieve them all), the performance drops to 70% recall up to 128K tokens, and &gt;60% recall up to 1M tokens. GPT-4 is 50% recall at 128k tokens (its maximum context).</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/multiple_niah.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Gemini 1.5 Pro beats RAG-based systems for all modalities. Although this may not be cost efficient right now unless there are multiple queries and the long context can be prefix-cached.</p> <p>Excellent use of context for learning new skills. It was able to learn to translate from/to English to/from Kalamang, an extremely low-resource language (spoken by less than 200 people in the world), with just a single set of linguistic documentation, a dictionary, and ≈ 400 parallel sentences. That’s it!</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/kalamang.png" alt="Given a reference grammar book and a bilingual wordlist (dictionary), Gemini 1.5 Pro is able to translate from English to Kalamang with similar quality to a human who learned from the same materials."/> <figcaption>Given a reference grammar book and a bilingual wordlist (dictionary), Gemini 1.5 Pro is able to translate from English to Kalamang with similar quality to a human who learned from the same materials.</figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/kalamang_results.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Gemini 1.5 Pro is exceeds Gemini 1.0 Ultra on text. However the latter is still better on vision and audio tasks, but the compute-efficiency and speed of shipping does suggest some ideas of distillation coming into the picture.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/results_comparison.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Figures 4 and 5 in the Technical report are super cool. In Figure 4, Gemini 1.5 is able to extract the exact scene with the page number from a textbook given a hand-drawn sketch as the prompt.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/fig4_paper.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>In Figure 5 they show that it is able to do the same in videos as well. The model returns a detailed description and timestamp of the scene.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/fig5_paper.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>The cumulative average negative log-likelihood (NLL) as a function of token position in long documents follows a power-law quite accurately up to 1M tokens for long-documents and about 2M tokens for code but deviates at the 10M scale.</p> <div class="outer"> <figure class="image"> <img src="/assets/img/posts_images/gemini15_pro/pplx.png" alt=""/> <figcaption></figcaption> <br/> </figure> </div> <style>.outer{display:block;text-align:center;max-width:100%}.image{display:inline-block;max-width:100%;margin:0 auto}.image img{display:block;width:100%;height:auto;max-width:100%}figure.embed,figure.embed-top,figure.overlay,figure.embed-over{display:inline-block;text-align:initial;vertical-align:top;position:relative;margin:.5em;font-size:.8em;background:white;overflow:hidden}figure.embed img,figure.embed-top img,figure.overlay img,figure.embed-over img{display:block;margin-left:auto;margin-right:auto}figure.embed figcaption,figure.embed-top figcaption,figure.overlay figcaption,figure.embed-over figcaption{width:100%;padding:.5em;color:rgba(50,50,50,1);background:rgba(200,200,200,0.825)}figcaption{display:block;font-size:80%}</style> <p>Though it <a href="https://youtu.be/wa0MT8OwHuk">seems like the inference speeds are quite slow</a> for long contexts at the moment, but the capabilities are absolutely insane!</p> <hr/> <p>An exciting future ahead!</p> <p> </p> <script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script> <div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div> <style>.example_a{color:#fff!important;text-transform:uppercase;text-decoration:none;background:#3f51b5;padding:20px;border-radius:5px;cursor:pointer;display:inline-block;border:0;transition:all .4s ease 0}.example_a:hover{background:#434343;letter-spacing:1px;-webkit-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);-moz-box-shadow:0 5px 40px -10px rgba(0,0,0,0.57);box-shadow:5px 40px -10px rgba(0,0,0,0.57);transition:all .4s ease 0}</style> <script type="text/javascript">function showMailingPopUp(){window.dojoRequire(["mojo/signup-forms/Loader"],function(o){o.start({baseUrl:"mc.us4.list-manage.com",uuid:"0b10ac14f50d7f4e7d11cf26a",lid:"667a1bb3da",uniqueMethods:!0})}),document.cookie="MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC"}document.getElementById("openpopup").onclick=function(){showMailingPopUp()};</script> <p> </p> <script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script> <p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>]]></content><author><name>Shreyansh Singh</name></author><category term="LLMs"/><category term="llm"/><category term="multimodal"/><category term="transformer"/><summary type="html"><![CDATA[Google DeepMind announced a multimodal LLM with support of up to 10M context length.]]></summary></entry></feed>